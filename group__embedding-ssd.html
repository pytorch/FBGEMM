<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "https://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en-US">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=11"/>
<meta name="generator" content="Doxygen 1.10.0"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>fbgemm_gpu: Embedding SSD Operators</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<script type="text/javascript" src="clipboard.js"></script>
<script type="text/javascript" src="cookie.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr id="projectrow">
  <td id="projectalign">
   <div id="projectname">fbgemm_gpu
   </div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.10.0 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
var searchBox = new SearchBox("searchBox", "search/",'.html');
/* @license-end */
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(function() { init_search(); });
});
/* @license-end */
</script>
<div id="main-nav"></div>
</div><!-- top -->
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<div id="MSearchResults">
<div class="SRPage">
<div id="SRIndex">
<div id="SRResults"></div>
<div class="SRStatus" id="Loading">Loading...</div>
<div class="SRStatus" id="Searching">Searching...</div>
<div class="SRStatus" id="NoMatches">No Matches</div>
</div>
</div>
</div>
</div>

<div class="header">
  <div class="summary">
<a href="#nested-classes">Classes</a> &#124;
<a href="#enum-members">Enumerations</a> &#124;
<a href="#func-members">Functions</a>  </div>
  <div class="headertitle"><div class="title">Embedding SSD Operators</div></div>
</div><!--header-->
<div class="contents">
<table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a id="nested-classes" name="nested-classes"></a>
Classes</h2></td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classl2__cache_1_1_cache_lib_cache.html">CacheLibCache</a></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classps_1_1_embedding_parameter_server.html">EmbeddingParameterServer</a></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classkv__db_1_1_cache_context.html">CacheContext</a></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">struct &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="structkv__db_1_1_queue_item.html">QueueItem</a></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classkv__db_1_1_embedding_k_v_d_b.html">EmbeddingKVDB</a></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">class &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="classssd_1_1_embedding_rocks_d_b.html">EmbeddingRocksDB</a></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table><table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a id="enum-members" name="enum-members"></a>
Enumerations</h2></td></tr>
<tr class="memitem:gaf1ce371b1e598c817791f840612cc1a3" id="r_gaf1ce371b1e598c817791f840612cc1a3"><td class="memItemLeft" align="right" valign="top">enum &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="#gaf1ce371b1e598c817791f840612cc1a3">RocksdbWriteMode</a> </td></tr>
<tr class="separator:gaf1ce371b1e598c817791f840612cc1a3"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table><table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a id="func-members" name="func-members"></a>
Functions</h2></td></tr>
<tr class="memitem:ga62f470c1f80d566177fd011b4f7181ac" id="r_ga62f470c1f80d566177fd011b4f7181ac"><td class="memItemLeft" align="right" valign="top">size_t&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="#ga62f470c1f80d566177fd011b4f7181ac">hash_shard</a> (int64_t id, size_t num_shards)</td></tr>
<tr class="separator:ga62f470c1f80d566177fd011b4f7181ac"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ga14b70f98db5378da5071b9049b7356db" id="r_ga14b70f98db5378da5071b9049b7356db"><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="#ga14b70f98db5378da5071b9049b7356db">cuda_callback_func</a> (cudaStream_t stream, cudaError_t status, void *functor)</td></tr>
<tr class="separator:ga14b70f98db5378da5071b9049b7356db"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:gaf16c3a72b210803c8ed97db57215ea49" id="r_gaf16c3a72b210803c8ed97db57215ea49"><td class="memItemLeft" align="right" valign="top">Tensor&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="#gaf16c3a72b210803c8ed97db57215ea49">masked_index_put_cuda</a> (Tensor self, Tensor indices, Tensor values, Tensor count, const bool use_pipeline, const int64_t preferred_sms)</td></tr>
<tr class="separator:gaf16c3a72b210803c8ed97db57215ea49"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ga8eda79bbff7e6020c078f6fb3b799cf6" id="r_ga8eda79bbff7e6020c078f6fb3b799cf6"><td class="memItemLeft" align="right" valign="top">Tensor&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="#ga8eda79bbff7e6020c078f6fb3b799cf6">masked_index_select_cuda</a> (Tensor self, Tensor indices, Tensor values, Tensor count, const bool use_pipeline, const int64_t preferred_sms)</td></tr>
<tr class="separator:ga8eda79bbff7e6020c078f6fb3b799cf6"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ga6c17fd18abf27a7776661565e7f1e011" id="r_ga6c17fd18abf27a7776661565e7f1e011"><td class="memItemLeft" align="right" valign="top">std::tuple&lt; Tensor, Tensor &gt;&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="#ga6c17fd18abf27a7776661565e7f1e011">ssd_generate_row_addrs_cuda</a> (const Tensor &amp;lxu_cache_locations, const Tensor &amp;assigned_cache_slots, const Tensor &amp;linear_index_inverse_indices, const Tensor &amp;unique_indices_count_cumsum, const Tensor &amp;cache_set_inverse_indices, const Tensor &amp;lxu_cache_weights, const Tensor &amp;inserted_ssd_weights, const Tensor &amp;unique_indices_length, const Tensor &amp;cache_set_sorted_unique_indices)</td></tr>
<tr class="separator:ga6c17fd18abf27a7776661565e7f1e011"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:gabac3e6cc0bdc3bd2306604a7eeabc88c" id="r_gabac3e6cc0bdc3bd2306604a7eeabc88c"><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="#gabac3e6cc0bdc3bd2306604a7eeabc88c">ssd_update_row_addrs_cuda</a> (const Tensor &amp;ssd_row_addrs_curr, const Tensor &amp;inserted_ssd_weights_curr_next_map, const Tensor &amp;lxu_cache_locations_curr, const Tensor &amp;linear_index_inverse_indices_curr, const Tensor &amp;unique_indices_count_cumsum_curr, const Tensor &amp;cache_set_inverse_indices_curr, const Tensor &amp;lxu_cache_weights, const Tensor &amp;inserted_ssd_weights_next, const Tensor &amp;unique_indices_length_curr)</td></tr>
<tr class="separator:gabac3e6cc0bdc3bd2306604a7eeabc88c"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ga3c1be516461fee114c157402e33cb0d9" id="r_ga3c1be516461fee114c157402e33cb0d9"><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="#ga3c1be516461fee114c157402e33cb0d9">compact_indices_cuda</a> (std::vector&lt; Tensor &gt; compact_indices, Tensor compact_count, std::vector&lt; Tensor &gt; indices, Tensor masks, Tensor count)</td></tr>
<tr class="separator:ga3c1be516461fee114c157402e33cb0d9"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table>
<a name="details" id="details"></a><h2 class="groupheader">Detailed Description</h2>
<h2 class="groupheader">Enumeration Type Documentation</h2>
<a id="gaf1ce371b1e598c817791f840612cc1a3" name="gaf1ce371b1e598c817791f840612cc1a3"></a>
<h2 class="memtitle"><span class="permalink"><a href="#gaf1ce371b1e598c817791f840612cc1a3">&#9670;&#160;</a></span>RocksdbWriteMode</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">enum <a class="el" href="#gaf1ce371b1e598c817791f840612cc1a3">RocksdbWriteMode</a></td>
        </tr>
      </table>
</div><div class="memdoc">

<p>rocksdb write mode </p>
<p>In SSD offloading there are 3 writes in each train iteration FWD_ROCKSDB_READ: cache lookup will move uncached data from rocksdb into L2 cache on fwd path</p>
<p>FWD_L1_EVICTION: L1 cache eviciton will evict data into L2 cache on fwd path</p>
<p>BWD_L1_CNFLCT_MISS_WRITE_BACK: L1 conflict miss will insert into L2 for embedding update on bwd path</p>
<p>All the L2 cache filling above will potentially trigger rocksdb write once L2 cache is full</p>
<p>Additionally we will do ssd io on L2 flush </p>

</div>
</div>
<h2 class="groupheader">Function Documentation</h2>
<a id="ga3c1be516461fee114c157402e33cb0d9" name="ga3c1be516461fee114c157402e33cb0d9"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ga3c1be516461fee114c157402e33cb0d9">&#9670;&#160;</a></span>compact_indices_cuda()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">void compact_indices_cuda </td>
          <td>(</td>
          <td class="paramtype">std::vector&lt; Tensor &gt;</td>          <td class="paramname"><span class="paramname"><em>compact_indices</em>, </span></td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Tensor</td>          <td class="paramname"><span class="paramname"><em>compact_count</em>, </span></td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">std::vector&lt; Tensor &gt;</td>          <td class="paramname"><span class="paramname"><em>indices</em>, </span></td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Tensor</td>          <td class="paramname"><span class="paramname"><em>masks</em>, </span></td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Tensor</td>          <td class="paramname"><span class="paramname"><em>count</em></span>&#160;)</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Compact the given list of indices. </p>
<p>This operator compact the given list of indices based on the given masks (a tensor that contains either 0 or 1). The operater removes the indices that their corresponding mask is 0. It only operates on <code>count</code> number of elements (not the full tensor).</p>
<p>Example:</p>
<div class="fragment"><div class="line">indices = [[0, 3, -1, 3, -1, -1, 7], [0, 2, 2, 3, -1, 9, 7]]</div>
<div class="line">masks = [1, 1, 0, 1, 0, 0, 1]</div>
<div class="line">count = 5</div>
<div class="line"> </div>
<div class="line"><span class="preprocessor"># x represents an arbitrary value</span></div>
<div class="line">compact_indices = [[0, 3, 3, x, x, x, x], [0, 2, 3, x, x, x, x]]</div>
<div class="line">compact_count = 3</div>
</div><!-- fragment --><dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">compact_indices</td><td>A list of compact indices (output indices). </td></tr>
    <tr><td class="paramname">compact_count</td><td>A tensor that contains the number of elements after being compacted </td></tr>
    <tr><td class="paramname">indices</td><td>An input list of indices to be compacted </td></tr>
    <tr><td class="paramname">masks</td><td>A tensor that contains 0 or 1 to indicate whether to remove/keep the element. 0 = remove the corresponding index. 1 = keep the corresponding index. @count count A tensor that contains the number of elements to be compacted </td></tr>
  </table>
  </dd>
</dl>

</div>
</div>
<a id="ga14b70f98db5378da5071b9049b7356db" name="ga14b70f98db5378da5071b9049b7356db"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ga14b70f98db5378da5071b9049b7356db">&#9670;&#160;</a></span>cuda_callback_func()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">void cuda_callback_func </td>
          <td>(</td>
          <td class="paramtype">cudaStream_t</td>          <td class="paramname"><span class="paramname"><em>stream</em>, </span></td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">cudaError_t</td>          <td class="paramname"><span class="paramname"><em>status</em>, </span></td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">void *</td>          <td class="paramname"><span class="paramname"><em>functor</em></span>&#160;)</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>A callback function for <code>cudaStreamAddCallback</code> </p>
<p>A common callback function for <code>cudaStreamAddCallback</code>, i.e., <code>cudaStreamCallback_t callback</code>. This function casts <code>functor</code> into a void function, invokes it and then delete it (the deletion occurs in another thread)</p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">stream</td><td>CUDA stream that <code>cudaStreamAddCallback</code> operates on </td></tr>
    <tr><td class="paramname">status</td><td>CUDA status </td></tr>
    <tr><td class="paramname">functor</td><td>A functor that will be called</td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>None </dd></dl>

</div>
</div>
<a id="ga62f470c1f80d566177fd011b4f7181ac" name="ga62f470c1f80d566177fd011b4f7181ac"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ga62f470c1f80d566177fd011b4f7181ac">&#9670;&#160;</a></span>hash_shard()</h2>

<div class="memitem">
<div class="memproto">
<table class="mlabels">
  <tr>
  <td class="mlabels-left">
      <table class="memname">
        <tr>
          <td class="memname">size_t hash_shard </td>
          <td>(</td>
          <td class="paramtype">int64_t</td>          <td class="paramname"><span class="paramname"><em>id</em>, </span></td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">size_t</td>          <td class="paramname"><span class="paramname"><em>num_shards</em></span>&#160;)</td>
        </tr>
      </table>
  </td>
  <td class="mlabels-right">
<span class="mlabels"><span class="mlabel">inline</span></span>  </td>
  </tr>
</table>
</div><div class="memdoc">

<p>hash function used for SSD L2 cache and rocksdb sharding algorithm </p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">id</td><td>sharding key </td></tr>
    <tr><td class="paramname">num_shards</td><td>sharding range</td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>shard id ranges from [0, num_shards) </dd></dl>

</div>
</div>
<a id="gaf16c3a72b210803c8ed97db57215ea49" name="gaf16c3a72b210803c8ed97db57215ea49"></a>
<h2 class="memtitle"><span class="permalink"><a href="#gaf16c3a72b210803c8ed97db57215ea49">&#9670;&#160;</a></span>masked_index_put_cuda()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">Tensor masked_index_put_cuda </td>
          <td>(</td>
          <td class="paramtype">Tensor</td>          <td class="paramname"><span class="paramname"><em>self</em>, </span></td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Tensor</td>          <td class="paramname"><span class="paramname"><em>indices</em>, </span></td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Tensor</td>          <td class="paramname"><span class="paramname"><em>values</em>, </span></td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Tensor</td>          <td class="paramname"><span class="paramname"><em>count</em>, </span></td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const bool</td>          <td class="paramname"><span class="paramname"><em>use_pipeline</em>, </span></td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const int64_t</td>          <td class="paramname"><span class="paramname"><em>preferred_sms</em></span>&#160;)</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Similar to <code>torch.Tensor.index_put</code> but ignore <code>indices &lt; 0</code> </p>
<p><code>masked_index_put_cuda</code> only supports 2D input <code>values</code>. It puts <code>count</code> rows in <code>values</code> into <code>self</code> using the row indices that are &gt;= 0 in <code>indices</code>.</p>
<div class="fragment"><div class="line"><span class="comment"># Equivalent PyTorch Python code</span></div>
<div class="line">indices = indices[:count]</div>
<div class="line">filter_ = indices &gt;= 0</div>
<div class="line">indices_ = indices[filter_]</div>
<div class="line">self[indices_] = values[filter_.nonzero().flatten()]</div>
</div><!-- fragment --><dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">self</td><td>The 2D output tensor (the tensor that is indexed) </td></tr>
    <tr><td class="paramname">indices</td><td>The 1D index tensor </td></tr>
    <tr><td class="paramname">values</td><td>The 2D input tensor </td></tr>
    <tr><td class="paramname">count</td><td>The tensor that contains the length of <code>indices</code> to process </td></tr>
    <tr><td class="paramname">use_pipeline</td><td>A flag that indicates that this kernel will overlap with other kernels. If it is true, then use a fraction of SMs to reduce resource competition </td></tr>
    <tr><td class="paramname">preferred_sms</td><td>The number of preferred SMs for the kernel to use when use_pipeline=true. This value is ignored when use_pipeline=false.</td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>The <code>self</code> tensor </dd></dl>

</div>
</div>
<a id="ga8eda79bbff7e6020c078f6fb3b799cf6" name="ga8eda79bbff7e6020c078f6fb3b799cf6"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ga8eda79bbff7e6020c078f6fb3b799cf6">&#9670;&#160;</a></span>masked_index_select_cuda()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">Tensor masked_index_select_cuda </td>
          <td>(</td>
          <td class="paramtype">Tensor</td>          <td class="paramname"><span class="paramname"><em>self</em>, </span></td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Tensor</td>          <td class="paramname"><span class="paramname"><em>indices</em>, </span></td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Tensor</td>          <td class="paramname"><span class="paramname"><em>values</em>, </span></td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Tensor</td>          <td class="paramname"><span class="paramname"><em>count</em>, </span></td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const bool</td>          <td class="paramname"><span class="paramname"><em>use_pipeline</em>, </span></td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const int64_t</td>          <td class="paramname"><span class="paramname"><em>preferred_sms</em></span>&#160;)</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Similar to <code>torch.index_select</code> but ignore <code>indices &lt; 0</code> </p>
<p><code>masked_index_select_cuda</code> only supports 2D input <code>values</code>. It puts <code>count</code> rows that are specified in <code>indices</code> (where <code>indices</code> &gt;= 0) from <code>values</code> into <code>self</code></p>
<div class="fragment"><div class="line"><span class="comment"># Equivalent PyTorch Python code</span></div>
<div class="line">indices = indices[:count]</div>
<div class="line">filter_ = indices &gt;= 0</div>
<div class="line">indices_ = indices[filter_]</div>
<div class="line">self[filter_.nonzero().flatten()] = values[indices_]</div>
</div><!-- fragment --><dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">self</td><td>The 2D output tensor </td></tr>
    <tr><td class="paramname">indices</td><td>The 1D index tensor </td></tr>
    <tr><td class="paramname">values</td><td>The 2D input tensor (the tensor that is indexed) </td></tr>
    <tr><td class="paramname">count</td><td>The tensor that contains the length of <code>indices</code> to process </td></tr>
    <tr><td class="paramname">use_pipeline</td><td>A flag that indicates that this kernel will overlap with other kernels. If it is true, then use a fraction of SMs to reduce resource competition </td></tr>
    <tr><td class="paramname">preferred_sms</td><td>The number of preferred SMs for the kernel to use when use_pipeline=true. This value is ignored when use_pipeline=false.</td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>The <code>self</code> tensor </dd></dl>

</div>
</div>
<a id="ga6c17fd18abf27a7776661565e7f1e011" name="ga6c17fd18abf27a7776661565e7f1e011"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ga6c17fd18abf27a7776661565e7f1e011">&#9670;&#160;</a></span>ssd_generate_row_addrs_cuda()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">std::tuple&lt; Tensor, Tensor &gt; ssd_generate_row_addrs_cuda </td>
          <td>(</td>
          <td class="paramtype">const Tensor &amp;</td>          <td class="paramname"><span class="paramname"><em>lxu_cache_locations</em>, </span></td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const Tensor &amp;</td>          <td class="paramname"><span class="paramname"><em>assigned_cache_slots</em>, </span></td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const Tensor &amp;</td>          <td class="paramname"><span class="paramname"><em>linear_index_inverse_indices</em>, </span></td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const Tensor &amp;</td>          <td class="paramname"><span class="paramname"><em>unique_indices_count_cumsum</em>, </span></td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const Tensor &amp;</td>          <td class="paramname"><span class="paramname"><em>cache_set_inverse_indices</em>, </span></td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const Tensor &amp;</td>          <td class="paramname"><span class="paramname"><em>lxu_cache_weights</em>, </span></td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const Tensor &amp;</td>          <td class="paramname"><span class="paramname"><em>inserted_ssd_weights</em>, </span></td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const Tensor &amp;</td>          <td class="paramname"><span class="paramname"><em>unique_indices_length</em>, </span></td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const Tensor &amp;</td>          <td class="paramname"><span class="paramname"><em>cache_set_sorted_unique_indices</em></span>&#160;)</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Generate memory addresses for SSD TBE data. </p>
<p>The data retrieved from SSD can be stored in either a scratch pad (HBM) or LXU cache (also HBM). <code>lxu_cache_locations</code> is used to specify the location of the data. If the location is -1, the data for the associated index is in the scratch pad; otherwise, it is in the cache. To enable TBE kernels to access the data conveniently, this operator generates memory addresses of the first byte for each index. When accessing data, a TBE kernel only needs to convert addresses into pointers.</p>
<p>Moreover, this operator also generate the list of post backward evicted indices which are basically the indices that their data is in the scratch pad.</p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">lxu_cache_locations</td><td>The tensor that contains cache slots where data is stored for the <em>full</em> list of indices. -1 is a sentinel value that indicates that data is not in cache. </td></tr>
    <tr><td class="paramname">assigned_cache_slots</td><td>The tensor that contains cache slots for the <em>unique</em> list of indices. -1 indicates that data is not in cache </td></tr>
    <tr><td class="paramname">linear_index_inverse_indices</td><td>The tensor that contains the original position of linear indices before being sorted </td></tr>
    <tr><td class="paramname">unique_indices_count_cumsum</td><td>The tensor that contains the the exclusive prefix sum results of the counts of unique indices </td></tr>
    <tr><td class="paramname">cache_set_inverse_indices</td><td>The tensor that contains the original positions of cache sets before being sorted </td></tr>
    <tr><td class="paramname">lxu_cache_weights</td><td>The LXU cache tensor </td></tr>
    <tr><td class="paramname">inserted_ssd_weights</td><td>The scratch pad tensor </td></tr>
    <tr><td class="paramname">unique_indices_length</td><td>The tensor that contains the number of unique indices (GPU tensor) </td></tr>
    <tr><td class="paramname">cache_set_sorted_unique_indices</td><td>The tensor that contains associated unique indices for the sorted unique cache sets</td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>A tuple of tensors (the SSD row address tensor and the post backward evicted index tensor) </dd></dl>

</div>
</div>
<a id="gabac3e6cc0bdc3bd2306604a7eeabc88c" name="gabac3e6cc0bdc3bd2306604a7eeabc88c"></a>
<h2 class="memtitle"><span class="permalink"><a href="#gabac3e6cc0bdc3bd2306604a7eeabc88c">&#9670;&#160;</a></span>ssd_update_row_addrs_cuda()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">void ssd_update_row_addrs_cuda </td>
          <td>(</td>
          <td class="paramtype">const Tensor &amp;</td>          <td class="paramname"><span class="paramname"><em>ssd_row_addrs_curr</em>, </span></td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const Tensor &amp;</td>          <td class="paramname"><span class="paramname"><em>inserted_ssd_weights_curr_next_map</em>, </span></td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const Tensor &amp;</td>          <td class="paramname"><span class="paramname"><em>lxu_cache_locations_curr</em>, </span></td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const Tensor &amp;</td>          <td class="paramname"><span class="paramname"><em>linear_index_inverse_indices_curr</em>, </span></td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const Tensor &amp;</td>          <td class="paramname"><span class="paramname"><em>unique_indices_count_cumsum_curr</em>, </span></td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const Tensor &amp;</td>          <td class="paramname"><span class="paramname"><em>cache_set_inverse_indices_curr</em>, </span></td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const Tensor &amp;</td>          <td class="paramname"><span class="paramname"><em>lxu_cache_weights</em>, </span></td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const Tensor &amp;</td>          <td class="paramname"><span class="paramname"><em>inserted_ssd_weights_next</em>, </span></td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const Tensor &amp;</td>          <td class="paramname"><span class="paramname"><em>unique_indices_length_curr</em></span>&#160;)</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>Update memory addresses for SSD TBE data. </p>
<p>When pipeline prefetching is enabled, data in a scratch pad of the current iteration can be moved to L1 or a scratch pad of the next iteration during the prefetch step. This operator updates the memory addresses of data that is relocated to the correct location.</p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">ssd_row_addrs_curr</td><td>The tensor that contains the row address of the current iteration </td></tr>
    <tr><td class="paramname">inserted_ssd_weights_curr_next_map</td><td>The tensor that contains mapping between the location of each index in the current iteration in the scratch pad of the next iteration. (-1 = the data has not been moved). inserted_ssd_weights_curr_next_map[i] is the location </td></tr>
    <tr><td class="paramname">lxu_cache_locations_curr</td><td>The tensor that contains cache slots where data is stored for the <em>full</em> list of indices for the current iteration. -1 is a sentinel value that indicates that data is not in cache. </td></tr>
    <tr><td class="paramname">linear_index_inverse_indices_curr</td><td>The tensor that contains the original position of linear indices before being sorted for the current iteration </td></tr>
    <tr><td class="paramname">unique_indices_count_cumsum_curr</td><td>The tensor that contains the the exclusive prefix sum results of the counts of unique indices for the current iteration </td></tr>
    <tr><td class="paramname">cache_set_inverse_indices_curr</td><td>The tensor that contains the original positions of cache sets before being sorted for the current iteration </td></tr>
    <tr><td class="paramname">lxu_cache_weights</td><td>The LXU cache tensor </td></tr>
    <tr><td class="paramname">inserted_ssd_weights_next</td><td>The scratch pad tensor for the next iteration </td></tr>
    <tr><td class="paramname">unique_indices_length_curr</td><td>The tensor that contains the number of unique indices (GPU tensor) for the current iteration</td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>None </dd></dl>

</div>
</div>
</div><!-- contents -->
<!-- start footer part -->
<hr class="footer"/><address class="footer"><small>
Generated by&#160;<a href="https://www.doxygen.org/index.html"><img class="footer" src="doxygen.svg" width="104" height="31" alt="doxygen"/></a> 1.10.0
</small></address>
</body>
</html>
