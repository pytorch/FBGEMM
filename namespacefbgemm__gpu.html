<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "https://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en-US">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=11"/>
<meta name="generator" content="Doxygen 1.11.0"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>fbgemm_gpu: fbgemm_gpu Namespace Reference</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<script type="text/javascript" src="clipboard.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="resize.js"></script>
<script type="text/javascript" src="cookie.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr id="projectrow">
  <td id="projectalign">
   <div id="projectname">fbgemm_gpu
   </div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.11.0 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
var searchBox = new SearchBox("searchBox", "search/",'.html');
/* @license-end */
</script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
$(function() { codefold.init(0); });
/* @license-end */
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
$(function() {
  initMenu('',true,false,'search.php','Search',false);
  $(function() { init_search(); });
});
/* @license-end */
</script>
<div id="main-nav"></div>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
$(function(){ initResizable(false); });
/* @license-end */
</script>
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<div id="MSearchResults">
<div class="SRPage">
<div id="SRIndex">
<div id="SRResults"></div>
<div class="SRStatus" id="Loading">Loading...</div>
<div class="SRStatus" id="Searching">Searching...</div>
<div class="SRStatus" id="NoMatches">No Matches</div>
</div>
</div>
</div>
</div>

</div><!-- top -->
<div id="doc-content">
<div class="header">
  <div class="summary">
<a href="#nested-classes">Classes</a> &#124;
<a href="#func-members">Functions</a>  </div>
  <div class="headertitle"><div class="title">fbgemm_gpu Namespace Reference</div></div>
</div><!--header-->
<div class="contents">
<table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a id="nested-classes" name="nested-classes"></a>
Classes</h2></td></tr>
<tr class="memitem:"><td class="memItemLeft" align="right" valign="top">struct &#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="structfbgemm__gpu_1_1_comparator.html">Comparator</a></td></tr>
<tr class="separator:"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table><table class="memberdecls">
<tr class="heading"><td colspan="2"><h2 class="groupheader"><a id="func-members" name="func-members"></a>
Functions</h2></td></tr>
<tr class="memitem:gab708b23762a11187eb6a32a36f0e34a3" id="r_gab708b23762a11187eb6a32a36f0e34a3"><td class="memItemLeft" align="right" valign="top">Tensor&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="group__cumem-utils.html#gab708b23762a11187eb6a32a36f0e34a3">new_managed_tensor</a> (const Tensor &amp;self, const std::vector&lt; std::int64_t &gt; &amp;sizes)</td></tr>
<tr class="separator:gab708b23762a11187eb6a32a36f0e34a3"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ga5351c6ec3de203476cf09df330455d91" id="r_ga5351c6ec3de203476cf09df330455d91"><td class="memItemLeft" align="right" valign="top">Tensor&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="group__cumem-utils.html#ga5351c6ec3de203476cf09df330455d91">new_managed_tensor_meta</a> (const Tensor &amp;self, const std::vector&lt; std::int64_t &gt; &amp;sizes)</td></tr>
<tr class="separator:ga5351c6ec3de203476cf09df330455d91"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ga5663643a8ac5de83063d0ff51bb9af17" id="r_ga5663643a8ac5de83063d0ff51bb9af17"><td class="memItemLeft" align="right" valign="top">Tensor&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="group__cumem-utils.html#ga5663643a8ac5de83063d0ff51bb9af17">new_host_mapped_tensor</a> (const Tensor &amp;self, const std::vector&lt; std::int64_t &gt; &amp;sizes)</td></tr>
<tr class="separator:ga5663643a8ac5de83063d0ff51bb9af17"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ga6f8847537ea9ed13fc7e2e378bc79b1f" id="r_ga6f8847537ea9ed13fc7e2e378bc79b1f"><td class="memItemLeft" align="right" valign="top">Tensor&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="group__cumem-utils.html#ga6f8847537ea9ed13fc7e2e378bc79b1f">new_unified_tensor</a> (const Tensor &amp;self, const std::vector&lt; std::int64_t &gt; &amp;sizes, bool is_host_mapped)</td></tr>
<tr class="separator:ga6f8847537ea9ed13fc7e2e378bc79b1f"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ga8cd15c298228b5cd628ee588761dc67d" id="r_ga8cd15c298228b5cd628ee588761dc67d"><td class="memItemLeft" align="right" valign="top">Tensor&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="group__cumem-utils.html#ga8cd15c298228b5cd628ee588761dc67d">new_unified_tensor_meta</a> (const Tensor &amp;self, const std::vector&lt; std::int64_t &gt; &amp;sizes, bool is_host_mapped)</td></tr>
<tr class="separator:ga8cd15c298228b5cd628ee588761dc67d"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:gad5e0d2307667c3db5e73f0c0eec15df5" id="r_gad5e0d2307667c3db5e73f0c0eec15df5"><td class="memItemLeft" align="right" valign="top">Tensor&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="group__cumem-utils.html#gad5e0d2307667c3db5e73f0c0eec15df5">new_vanilla_managed_tensor</a> (const Tensor &amp;self, const std::vector&lt; std::int64_t &gt; &amp;sizes)</td></tr>
<tr class="separator:gad5e0d2307667c3db5e73f0c0eec15df5"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ga05bf2c435c434904ca454c6992861cb6" id="r_ga05bf2c435c434904ca454c6992861cb6"><td class="memItemLeft" align="right" valign="top">bool&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="group__cumem-utils.html#ga05bf2c435c434904ca454c6992861cb6">uvm_storage</a> (const Tensor &amp;self)</td></tr>
<tr class="separator:ga05bf2c435c434904ca454c6992861cb6"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:gacba28ed334d071e79c1ead1792391e9d" id="r_gacba28ed334d071e79c1ead1792391e9d"><td class="memItemLeft" align="right" valign="top">bool&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="group__cumem-utils.html#gacba28ed334d071e79c1ead1792391e9d">is_uvm_tensor</a> (const Tensor &amp;self)</td></tr>
<tr class="separator:gacba28ed334d071e79c1ead1792391e9d"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:gab5a3dab831988b1ce368ccc545b75b48" id="r_gab5a3dab831988b1ce368ccc545b75b48"><td class="memItemLeft" align="right" valign="top">Tensor&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="group__cumem-utils.html#gab5a3dab831988b1ce368ccc545b75b48">uvm_to_cpu</a> (const Tensor &amp;self)</td></tr>
<tr class="separator:gab5a3dab831988b1ce368ccc545b75b48"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:gaebfedcf8e6017a6d4f6fb16b52c4c04e" id="r_gaebfedcf8e6017a6d4f6fb16b52c4c04e"><td class="memItemLeft" align="right" valign="top">Tensor&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="group__cumem-utils.html#gaebfedcf8e6017a6d4f6fb16b52c4c04e">uvm_to_device</a> (const Tensor &amp;self, const Tensor &amp;prototype)</td></tr>
<tr class="separator:gaebfedcf8e6017a6d4f6fb16b52c4c04e"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:gae8c724e90d31245756fc4b0d975f9370" id="r_gae8c724e90d31245756fc4b0d975f9370"><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="group__cumem-utils.html#gae8c724e90d31245756fc4b0d975f9370">uvm_cuda_mem_advise</a> (const Tensor &amp;self, int64_t cuda_memory_advise)</td></tr>
<tr class="separator:gae8c724e90d31245756fc4b0d975f9370"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ga6bc25db5211698851a7664c945aeffa5" id="r_ga6bc25db5211698851a7664c945aeffa5"><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="group__cumem-utils.html#ga6bc25db5211698851a7664c945aeffa5">uvm_cuda_mem_prefetch_async</a> (const Tensor &amp;self, std::optional&lt; Tensor &gt; device_t)</td></tr>
<tr class="separator:ga6bc25db5211698851a7664c945aeffa5"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ga01301ad686f7570c21e81c122d2c7af8" id="r_ga01301ad686f7570c21e81c122d2c7af8"><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="group__cumem-utils.html#ga01301ad686f7570c21e81c122d2c7af8">uvm_mem_advice_dont_fork</a> (const Tensor &amp;self)</td></tr>
<tr class="separator:ga01301ad686f7570c21e81c122d2c7af8"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ga161495e682d9eac3701dca87469930db" id="r_ga161495e682d9eac3701dca87469930db"><td class="memItemLeft" align="right" valign="top">Tensor&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="group__cumem-utils.html#ga161495e682d9eac3701dca87469930db">uvm_to_cpu_clone</a> (const Tensor &amp;self)</td></tr>
<tr class="separator:ga161495e682d9eac3701dca87469930db"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a358cc74c17a25856e51a4bf73a8247ec" id="r_a358cc74c17a25856e51a4bf73a8247ec"><td class="memItemLeft" align="right" valign="top">void&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="#a358cc74c17a25856e51a4bf73a8247ec">embedding_inplace_update_cuda</a> (Tensor dev_weights, Tensor uvm_weights, Tensor weights_placements, Tensor weights_offsets, Tensor weights_tys, Tensor D_offsets, Tensor update_weights, Tensor update_table_idx, Tensor update_row_idx, Tensor update_offsets, const int64_t row_alignment, std::optional&lt; Tensor &gt; lxu_cache_weights=std::nullopt, std::optional&lt; Tensor &gt; lxu_cache_locations=std::nullopt)</td></tr>
<tr class="separator:a358cc74c17a25856e51a4bf73a8247ec"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:adda552b8784184a2f17aa997e10869f9" id="r_adda552b8784184a2f17aa997e10869f9"><td class="memItemLeft" align="right" valign="top">Tensor&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="#adda552b8784184a2f17aa997e10869f9">pruned_array_lookup_from_row_idx_cuda</a> (const Tensor &amp;update_row_indices, const Tensor &amp;update_table_indices, const Tensor &amp;index_remappings, const Tensor &amp;index_remappings_offsets)</td></tr>
<tr class="separator:adda552b8784184a2f17aa997e10869f9"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a0b0cc504f8b357b0ca086e2c78d9b54c" id="r_a0b0cc504f8b357b0ca086e2c78d9b54c"><td class="memItemLeft" align="right" valign="top">std::tuple&lt; std::vector&lt; int32_t &gt;, std::vector&lt; int32_t &gt;, std::vector&lt; int32_t &gt; &gt;&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="#a0b0cc504f8b357b0ca086e2c78d9b54c">kt_regroup_arguments_impl</a> (const std::vector&lt; std::vector&lt; std::string &gt; &gt; &amp;keys, const std::vector&lt; std::vector&lt; int64_t &gt; &gt; &amp;lengths, const std::vector&lt; std::vector&lt; std::string &gt; &gt; &amp;groups)</td></tr>
<tr class="separator:a0b0cc504f8b357b0ca086e2c78d9b54c"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:gab7344d63216dd37673733b26954aaec4" id="r_gab7344d63216dd37673733b26954aaec4"><td class="memItemLeft" align="right" valign="top">at::Tensor&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="group__sparse-data-cuda.html#gab7344d63216dd37673733b26954aaec4">expand_into_jagged_permute_cuda</a> (const at::Tensor &amp;permute, const at::Tensor &amp;input_offsets, const at::Tensor &amp;output_offsets, int64_t output_size)</td></tr>
<tr class="separator:gab7344d63216dd37673733b26954aaec4"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:gaef2a0a8c27e3b8b2d72be5c95ba7539e" id="r_gaef2a0a8c27e3b8b2d72be5c95ba7539e"><td class="memItemLeft" align="right" valign="top">std::tuple&lt; at::Tensor, at::Tensor &gt;&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="group__sparse-data-cpu.html#gaef2a0a8c27e3b8b2d72be5c95ba7539e">generic_histogram_binning_calibration_by_feature_cpu</a> (const at::Tensor &amp;logit, const at::Tensor &amp;segment_value, const at::Tensor &amp;segment_lengths, int64_t num_segments, const at::Tensor &amp;bin_num_examples, const at::Tensor &amp;bin_num_positives, const at::Tensor &amp;bin_boundaries, double positive_weight, int64_t bin_ctr_in_use_after=0, double bin_ctr_weight_value=1.0)</td></tr>
<tr class="separator:gaef2a0a8c27e3b8b2d72be5c95ba7539e"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a4f4260f8147fe33d5c0c5611ae0f7e0f" id="r_a4f4260f8147fe33d5c0c5611ae0f7e0f"><td class="memTemplParams" colspan="2">template&lt;typename IndexType , typename OffsetType &gt; </td></tr>
<tr class="memitem:a4f4260f8147fe33d5c0c5611ae0f7e0f"><td class="memTemplItemLeft" align="right" valign="top">void&#160;</td><td class="memTemplItemRight" valign="bottom"><a class="el" href="#a4f4260f8147fe33d5c0c5611ae0f7e0f">report_embedding_error</a> (int t, int B, int b_begin, int b_end, const OffsetType *offsets_data, const IndexType *indices_data, int64_t hash_size, bool allow_minus_one=false)</td></tr>
<tr class="separator:a4f4260f8147fe33d5c0c5611ae0f7e0f"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ae86238f4ca864fb4ea41318ece747ab4" id="r_ae86238f4ca864fb4ea41318ece747ab4"><td class="memTemplParams" colspan="2">template&lt;typename scalar_t , int ITEMS_PER_THREAD, int NUM_THREADS_PER_BLOCK&gt; </td></tr>
<tr class="memitem:ae86238f4ca864fb4ea41318ece747ab4"><td class="memTemplItemLeft" align="right" valign="top">__inline__ __device__ void&#160;</td><td class="memTemplItemRight" valign="bottom"><a class="el" href="#ae86238f4ca864fb4ea41318ece747ab4">inclusive_sum_scan_kernel</a> (scalar_t(&amp;arr)[ITEMS_PER_THREAD], typename cub::BlockScan&lt; scalar_t, NUM_THREADS_PER_BLOCK &gt;::TempStorage &amp;temp_storage, int *block_flags, volatile scalar_t *block_sums, scalar_t *block_prev, const int num_entries_per_block, const int block_id, const bool is_multi_block, const int signal)</td></tr>
<tr class="separator:ae86238f4ca864fb4ea41318ece747ab4"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ace574b23fe99327816839526e99298ac" id="r_ace574b23fe99327816839526e99298ac"><td class="memItemLeft" align="right" valign="top">at::Tensor&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="#ace574b23fe99327816839526e99298ac">per_tensor_quantize_i8</a> (at::Tensor X, double scale)</td></tr>
<tr class="separator:ace574b23fe99327816839526e99298ac"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:af01b4023830652f0cc3e99c87f7b4526" id="r_af01b4023830652f0cc3e99c87f7b4526"><td class="memItemLeft" align="right" valign="top">std::tuple&lt; Tensor, Tensor, Tensor &gt;&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="#af01b4023830652f0cc3e99c87f7b4526">padding_fused_tbe_input_combine_with_length_cpu</a> (const std::vector&lt; Tensor &gt; &amp;indices_list, const std::vector&lt; Tensor &gt; &amp;lengths_list, const std::vector&lt; Tensor &gt; &amp;per_sample_weights, int64_t batch_size)</td></tr>
<tr class="separator:af01b4023830652f0cc3e99c87f7b4526"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ad21c70bdd84772ee2b9b3950c87e9791" id="r_ad21c70bdd84772ee2b9b3950c87e9791"><td class="memTemplParams" colspan="2">template&lt;int NUM_JAGGED_DIM, typename index_t , typename scalar_t , typename F &gt; </td></tr>
<tr class="memitem:ad21c70bdd84772ee2b9b3950c87e9791"><td class="memTemplItemLeft" align="right" valign="top">__global__&#160;</td><td class="memTemplItemRight" valign="bottom"><a class="el" href="#ad21c70bdd84772ee2b9b3950c87e9791">__launch_bounds__</a> (kMaxThreads) void jagged_jagged_elementwise_dense_output_kernel_(const pta</td></tr>
<tr class="separator:ad21c70bdd84772ee2b9b3950c87e9791"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a53a6da74de342260dcb15c68e9bddfd6" id="r_a53a6da74de342260dcb15c68e9bddfd6"><td class="memItemLeft" align="right" valign="top">Tensor&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="#a53a6da74de342260dcb15c68e9bddfd6">jagged_index_add_2d_forward_cuda</a> (const Tensor &amp;values, const Tensor &amp;indices, const Tensor &amp;input_offsets, const Tensor &amp;output_offsets, const int64_t num_dense_input_rows, const int64_t num_output_rows)</td></tr>
<tr class="separator:a53a6da74de342260dcb15c68e9bddfd6"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:acb5a744fbd29c8a3a25621c2850686c1" id="r_acb5a744fbd29c8a3a25621c2850686c1"><td class="memItemLeft" align="right" valign="top">Tensor&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="#acb5a744fbd29c8a3a25621c2850686c1">jagged_index_select_2d_forward_cuda</a> (const Tensor &amp;values, const Tensor &amp;indices, const Tensor &amp;input_offsets, const Tensor &amp;output_offsets, const int64_t num_dense_output_rows)</td></tr>
<tr class="separator:acb5a744fbd29c8a3a25621c2850686c1"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:gaa797caaa08c70857433ae987d9cf30d7" id="r_gaa797caaa08c70857433ae987d9cf30d7"><td class="memItemLeft" align="right" valign="top">Tensor&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="group__jagged-tensor-ops-cpu.html#gaa797caaa08c70857433ae987d9cf30d7">jagged_dense_elementwise_add</a> (const Tensor &amp;x_values, const std::vector&lt; Tensor &gt; &amp;x_offsets, const Tensor &amp;y)</td></tr>
<tr class="separator:gaa797caaa08c70857433ae987d9cf30d7"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ga1290f40c3ba39837dd009c3006353d7c" id="r_ga1290f40c3ba39837dd009c3006353d7c"><td class="memItemLeft" align="right" valign="top">std::tuple&lt; Tensor, std::vector&lt; Tensor &gt; &gt;&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="group__jagged-tensor-ops-cpu.html#ga1290f40c3ba39837dd009c3006353d7c">jagged_dense_elementwise_add_jagged_output</a> (const Tensor &amp;x_values, const std::vector&lt; Tensor &gt; &amp;x_offsets, const Tensor &amp;y)</td></tr>
<tr class="separator:ga1290f40c3ba39837dd009c3006353d7c"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:aafb575930df07db6f09594c7e2d08650" id="r_aafb575930df07db6f09594c7e2d08650"><td class="memItemLeft" align="right" valign="top">std::vector&lt; Tensor &gt;&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="#aafb575930df07db6f09594c7e2d08650">jagged_index_select_2d</a> (const Tensor &amp;values, const Tensor &amp;lengths, const Tensor &amp;indices, const std::optional&lt; int64_t &gt; num_dense_output_rows)</td></tr>
<tr class="separator:aafb575930df07db6f09594c7e2d08650"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a71a54a14d90862afc8e5fe03e0c9ed8f" id="r_a71a54a14d90862afc8e5fe03e0c9ed8f"><td class="memItemLeft" align="right" valign="top">Tensor&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="#a71a54a14d90862afc8e5fe03e0c9ed8f">jagged_index_select_2d_forward_cpu</a> (const Tensor &amp;values, const Tensor &amp;indices, const Tensor &amp;input_offsets, const Tensor &amp;output_offsets, const int64_t num_dense_output_rows)</td></tr>
<tr class="separator:a71a54a14d90862afc8e5fe03e0c9ed8f"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:af80524a7d454f6db1c478808e8a659a6" id="r_af80524a7d454f6db1c478808e8a659a6"><td class="memItemLeft" align="right" valign="top">Tensor&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="#af80524a7d454f6db1c478808e8a659a6">jagged_index_add_2d_forward_cpu</a> (const Tensor &amp;values, const Tensor &amp;indices, const Tensor &amp;input_offsets, const Tensor &amp;output_offsets, const int64_t num_dense_input_rows, const int64_t num_output_rows)</td></tr>
<tr class="separator:af80524a7d454f6db1c478808e8a659a6"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a4e6521d00a6f81ad8ad7f7d38eef1aea" id="r_a4e6521d00a6f81ad8ad7f7d38eef1aea"><td class="memItemLeft" align="right" valign="top">Tensor&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="#a4e6521d00a6f81ad8ad7f7d38eef1aea">jagged_slice_forward_cpu</a> (const Tensor &amp;x_values, const Tensor &amp;x_lengths, const Tensor &amp;src_start, const Tensor &amp;output_lengths, const Tensor &amp;tgt_start, const int64_t num_output_rows, const int64_t slice_length, const bool fill_zeros)</td></tr>
<tr class="separator:a4e6521d00a6f81ad8ad7f7d38eef1aea"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:gad34ac20d2c9be5a6489c8e8befff7938" id="r_gad34ac20d2c9be5a6489c8e8befff7938"><td class="memItemLeft" align="right" valign="top">std::tuple&lt; Tensor, std::vector&lt; Tensor &gt; &gt;&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="group__jagged-tensor-ops-cuda.html#gad34ac20d2c9be5a6489c8e8befff7938">jagged_dense_elementwise_add_jagged_output_cuda</a> (const Tensor &amp;x_values, const std::vector&lt; Tensor &gt; &amp;x_offsets, const Tensor &amp;y)</td></tr>
<tr class="separator:gad34ac20d2c9be5a6489c8e8befff7938"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ac785a168bee4c98b8993d5cadf846267" id="r_ac785a168bee4c98b8993d5cadf846267"><td class="memItemLeft" align="right" valign="top">std::tuple&lt; Tensor, Tensor, Tensor, std::vector&lt; int64_t &gt; &gt;&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="#ac785a168bee4c98b8993d5cadf846267">kt_regroup_arguments_gpu</a> (const Tensor &amp;emb, const std::vector&lt; std::vector&lt; std::string &gt; &gt; &amp;keys, const std::vector&lt; std::vector&lt; int64_t &gt; &gt; &amp;lengths, const std::vector&lt; std::vector&lt; std::string &gt; &gt; &amp;groups)</td></tr>
<tr class="separator:ac785a168bee4c98b8993d5cadf846267"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ae85fe18941deb26c531c609da34d7c78" id="r_ae85fe18941deb26c531c609da34d7c78"><td class="memItemLeft" align="right" valign="top">std::vector&lt; Tensor &gt;&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="#ae85fe18941deb26c531c609da34d7c78">permute_multi_embedding_autograd</a> (const at::TensorList &amp;pooled_embs, const Tensor &amp;permutes, const Tensor &amp;in_shapes, const Tensor &amp;out_shapes, const c10::SymIntArrayRef out_lengths)</td></tr>
<tr class="separator:ae85fe18941deb26c531c609da34d7c78"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a8ee4a87df31fced027e10cd1bfb649ee" id="r_a8ee4a87df31fced027e10cd1bfb649ee"><td class="memItemLeft" align="right" valign="top">std::tuple&lt; Tensor, Tensor, Tensor, std::vector&lt; int64_t &gt; &gt;&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="#a8ee4a87df31fced027e10cd1bfb649ee">kt_regroup_arguments_cpu</a> (const Tensor &amp;, const std::vector&lt; std::vector&lt; std::string &gt; &gt; &amp;keys, const std::vector&lt; std::vector&lt; int64_t &gt; &gt; &amp;lengths, const std::vector&lt; std::vector&lt; std::string &gt; &gt; &amp;groups)</td></tr>
<tr class="separator:a8ee4a87df31fced027e10cd1bfb649ee"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ga42277d094806afc9f23d52430c510105" id="r_ga42277d094806afc9f23d52430c510105"><td class="memItemLeft" align="right" valign="top">at::Tensor&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="group__quantize-ops-cuda.html#ga42277d094806afc9f23d52430c510105">_float_to_bfloat16_gpu</a> (const at::Tensor &amp;input)</td></tr>
<tr class="separator:ga42277d094806afc9f23d52430c510105"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ga52c8ee305990222b63257a431024e835" id="r_ga52c8ee305990222b63257a431024e835"><td class="memItemLeft" align="right" valign="top">at::Tensor&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="group__quantize-ops-cuda.html#ga52c8ee305990222b63257a431024e835">_bfloat16_to_float_gpu</a> (const at::Tensor &amp;input)</td></tr>
<tr class="separator:ga52c8ee305990222b63257a431024e835"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:gaea7f718e5addaefa562e5a1f287c3de5" id="r_gaea7f718e5addaefa562e5a1f287c3de5"><td class="memItemLeft" align="right" valign="top">Tensor&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="group__quantize-ops-cuda.html#gaea7f718e5addaefa562e5a1f287c3de5">_float_to_FP8rowwise_gpu</a> (const Tensor &amp;input, const bool forward)</td></tr>
<tr class="separator:gaea7f718e5addaefa562e5a1f287c3de5"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:gaaf10540afa5cf8ca8805bb0e4c4e5492" id="r_gaaf10540afa5cf8ca8805bb0e4c4e5492"><td class="memItemLeft" align="right" valign="top">at::Tensor&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="group__quantize-ops-cuda.html#gaaf10540afa5cf8ca8805bb0e4c4e5492">_FP8rowwise_to_float_gpu</a> (const at::Tensor &amp;input, bool forward, const int64_t output_dtype)</td></tr>
<tr class="separator:gaaf10540afa5cf8ca8805bb0e4c4e5492"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ga50adb075977639eb15f0249751cb584d" id="r_ga50adb075977639eb15f0249751cb584d"><td class="memItemLeft" align="right" valign="top">Tensor&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="group__quantize-ops-cuda.html#ga50adb075977639eb15f0249751cb584d">_float_to_fused8bitrowwise_gpu</a> (const Tensor &amp;input)</td></tr>
<tr class="separator:ga50adb075977639eb15f0249751cb584d"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:gace77836954555ee52abf3270e89f831d" id="r_gace77836954555ee52abf3270e89f831d"><td class="memItemLeft" align="right" valign="top">Tensor&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="group__quantize-ops-cuda.html#gace77836954555ee52abf3270e89f831d">_half_to_fused8bitrowwise_gpu</a> (const Tensor &amp;input)</td></tr>
<tr class="separator:gace77836954555ee52abf3270e89f831d"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:gace06316bba9b336a20b9de76ca38943b" id="r_gace06316bba9b336a20b9de76ca38943b"><td class="memItemLeft" align="right" valign="top">Tensor&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="group__quantize-ops-cuda.html#gace06316bba9b336a20b9de76ca38943b">_single_or_half_precision_to_fused8bitrowwise_gpu</a> (const Tensor &amp;input)</td></tr>
<tr class="separator:gace06316bba9b336a20b9de76ca38943b"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ga4eaebf19f2a7330fc060b085e540db42" id="r_ga4eaebf19f2a7330fc060b085e540db42"><td class="memItemLeft" align="right" valign="top">at::Tensor&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="group__quantize-ops-cuda.html#ga4eaebf19f2a7330fc060b085e540db42">_fused8bitrowwise_to_float_gpu</a> (const at::Tensor &amp;input)</td></tr>
<tr class="separator:ga4eaebf19f2a7330fc060b085e540db42"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ga8193797bccd50ba869beefb87a14a9bd" id="r_ga8193797bccd50ba869beefb87a14a9bd"><td class="memItemLeft" align="right" valign="top">at::Tensor&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="group__quantize-ops-cuda.html#ga8193797bccd50ba869beefb87a14a9bd">_fused8bitrowwise_to_half_gpu</a> (const at::Tensor &amp;input)</td></tr>
<tr class="separator:ga8193797bccd50ba869beefb87a14a9bd"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ga29377cee5a9aca8c8ac1a38ce4a83ec1" id="r_ga29377cee5a9aca8c8ac1a38ce4a83ec1"><td class="memItemLeft" align="right" valign="top">at::Tensor&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="group__quantize-ops-cuda.html#ga29377cee5a9aca8c8ac1a38ce4a83ec1">_fused8bitrowwise_to_single_or_half_precision_gpu</a> (const at::Tensor &amp;input, const int64_t output_dtype, const bool scale_bias_last, const bool quant_padding_float_type)</td></tr>
<tr class="separator:ga29377cee5a9aca8c8ac1a38ce4a83ec1"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ga304ba1cfb80e721f8c15cfa1db25621c" id="r_ga304ba1cfb80e721f8c15cfa1db25621c"><td class="memItemLeft" align="right" valign="top">at::Tensor&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="group__quantize-ops-cuda.html#ga304ba1cfb80e721f8c15cfa1db25621c">_fused8bitrowwise_to_float_mixed_dim_gpu</a> (const at::Tensor &amp;input, const at::Tensor &amp;D_offsets, const int64_t output_dtype)</td></tr>
<tr class="separator:ga304ba1cfb80e721f8c15cfa1db25621c"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:gae55e28033798a9a0cbd93c70b119c3aa" id="r_gae55e28033798a9a0cbd93c70b119c3aa"><td class="memItemLeft" align="right" valign="top">Tensor&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="group__quantize-ops-cuda.html#gae55e28033798a9a0cbd93c70b119c3aa">_float_to_fusednbitrowwise_gpu</a> (const Tensor &amp;input, const int64_t bit_rate)</td></tr>
<tr class="separator:gae55e28033798a9a0cbd93c70b119c3aa"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ga399e3ece67cad23f6627ebf1f8127512" id="r_ga399e3ece67cad23f6627ebf1f8127512"><td class="memItemLeft" align="right" valign="top">at::Tensor&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="group__quantize-ops-cuda.html#ga399e3ece67cad23f6627ebf1f8127512">_half_to_fusednbitrowwise_gpu</a> (const at::Tensor &amp;input, const int64_t bit_rate)</td></tr>
<tr class="separator:ga399e3ece67cad23f6627ebf1f8127512"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:gaae960240a6e2884a30205ed3fa9d3111" id="r_gaae960240a6e2884a30205ed3fa9d3111"><td class="memItemLeft" align="right" valign="top">Tensor&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="group__quantize-ops-cuda.html#gaae960240a6e2884a30205ed3fa9d3111">_single_or_half_precision_to_fusednbitrowwise_gpu</a> (const Tensor &amp;input, const int64_t bit_rate)</td></tr>
<tr class="separator:gaae960240a6e2884a30205ed3fa9d3111"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ga802490706abaca9d86d780bbdc0922cc" id="r_ga802490706abaca9d86d780bbdc0922cc"><td class="memItemLeft" align="right" valign="top">at::Tensor&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="group__quantize-ops-cuda.html#ga802490706abaca9d86d780bbdc0922cc">_fusednbitrowwise_to_float_gpu</a> (const at::Tensor &amp;input, const int64_t bit_rate)</td></tr>
<tr class="separator:ga802490706abaca9d86d780bbdc0922cc"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ga756a59a6b84321e9b91af3e5f3334c31" id="r_ga756a59a6b84321e9b91af3e5f3334c31"><td class="memItemLeft" align="right" valign="top">at::Tensor&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="group__quantize-ops-cuda.html#ga756a59a6b84321e9b91af3e5f3334c31">_fusednbitrowwise_to_half_gpu</a> (const at::Tensor &amp;input, const int64_t bit_rate)</td></tr>
<tr class="separator:ga756a59a6b84321e9b91af3e5f3334c31"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ga5ee877a3c135bd5160991c77ed170b23" id="r_ga5ee877a3c135bd5160991c77ed170b23"><td class="memItemLeft" align="right" valign="top">at::Tensor&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="group__quantize-ops-cuda.html#ga5ee877a3c135bd5160991c77ed170b23">_fusednbitrowwise_to_single_or_half_precision_gpu</a> (const at::Tensor &amp;input, const int64_t bit_rate, const int64_t output_dtype)</td></tr>
<tr class="separator:ga5ee877a3c135bd5160991c77ed170b23"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ga4050ec86de055f06ad3a89c8b9cd24e9" id="r_ga4050ec86de055f06ad3a89c8b9cd24e9"><td class="memItemLeft" align="right" valign="top">at::Tensor&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="group__quantize-ops-cuda.html#ga4050ec86de055f06ad3a89c8b9cd24e9">_float_to_hfp8_gpu</a> (const at::Tensor &amp;input, const int64_t ebits, const int64_t exponent_bias, const double max_pos)</td></tr>
<tr class="separator:ga4050ec86de055f06ad3a89c8b9cd24e9"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ga1dd6b70398b542022b236f4c576609a2" id="r_ga1dd6b70398b542022b236f4c576609a2"><td class="memItemLeft" align="right" valign="top">at::Tensor&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="group__quantize-ops-cuda.html#ga1dd6b70398b542022b236f4c576609a2">_hfp8_to_float_gpu</a> (const at::Tensor &amp;input, const int64_t ebits, const int64_t exponent_bias)</td></tr>
<tr class="separator:ga1dd6b70398b542022b236f4c576609a2"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ga63e0eb01baaf7a680056549386bd17ec" id="r_ga63e0eb01baaf7a680056549386bd17ec"><td class="memItemLeft" align="right" valign="top">at::Tensor&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="group__quantize-ops-cuda.html#ga63e0eb01baaf7a680056549386bd17ec">_float_to_msfp_gpu</a> (const at::Tensor &amp;input, const int64_t bounding_box_size, const int64_t ebits, const int64_t mbits, const int64_t bias, const double min_pos, const double max_pos)</td></tr>
<tr class="separator:ga63e0eb01baaf7a680056549386bd17ec"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ga6bf8bd180a3ed7174965bf73d18c3f2e" id="r_ga6bf8bd180a3ed7174965bf73d18c3f2e"><td class="memItemLeft" align="right" valign="top">at::Tensor&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="group__quantize-ops-cuda.html#ga6bf8bd180a3ed7174965bf73d18c3f2e">_msfp_to_float_gpu</a> (const at::Tensor &amp;input, const int64_t ebits, const int64_t mbits, const int64_t bias)</td></tr>
<tr class="separator:ga6bf8bd180a3ed7174965bf73d18c3f2e"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ga211b61807b27bbdc041036557246c0ce" id="r_ga211b61807b27bbdc041036557246c0ce"><td class="memItemLeft" align="right" valign="top">Tensor&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="group__quantize-data-cpu.html#ga211b61807b27bbdc041036557246c0ce">fusednbitrowwise_sbfront_to_float_cpu</a> (const Tensor &amp;input, const int64_t bit_rate)</td></tr>
<tr class="separator:ga211b61807b27bbdc041036557246c0ce"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ga5f9b9bdd22dba5a1870ef9c0e0877087" id="r_ga5f9b9bdd22dba5a1870ef9c0e0877087"><td class="memItemLeft" align="right" valign="top">Tensor&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="group__quantize-ops-cuda.html#ga5f9b9bdd22dba5a1870ef9c0e0877087">_float_to_paddedFP8rowwise_gpu</a> (const Tensor &amp;input, const bool forward, const int64_t row_dim)</td></tr>
<tr class="separator:ga5f9b9bdd22dba5a1870ef9c0e0877087"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:ga6c6d6fc1318239ecfaf902349a988cd6" id="r_ga6c6d6fc1318239ecfaf902349a988cd6"><td class="memItemLeft" align="right" valign="top">at::Tensor&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="group__quantize-ops-cuda.html#ga6c6d6fc1318239ecfaf902349a988cd6">_paddedFP8rowwise_to_float_gpu</a> (const at::Tensor &amp;input, const bool forward, const int64_t row_dim, const int64_t output_last_dim, const int64_t output_dtype)</td></tr>
<tr class="separator:ga6c6d6fc1318239ecfaf902349a988cd6"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a96db75aa5b2617976c2937ab051b737e" id="r_a96db75aa5b2617976c2937ab051b737e"><td class="memItemLeft" align="right" valign="top">Tensor&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="#a96db75aa5b2617976c2937ab051b737e">batched_unary_embeddings_forward_cpu</a> (const Tensor &amp;weight, const Tensor &amp;table_offsets, const Tensor &amp;offsets, const Tensor &amp;indices)</td></tr>
<tr class="separator:a96db75aa5b2617976c2937ab051b737e"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a49cb5dd543cc63e932f458e1c79c0d00" id="r_a49cb5dd543cc63e932f458e1c79c0d00"><td class="memItemLeft" align="right" valign="top">Tensor&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="#a49cb5dd543cc63e932f458e1c79c0d00">pack_segments_forward_cpu</a> (const Tensor &amp;t_in, const Tensor &amp;lengths, const int64_t max_length)</td></tr>
<tr class="separator:a49cb5dd543cc63e932f458e1c79c0d00"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a51f0921a8e934c6c4d0fca5ebb5d8338" id="r_a51f0921a8e934c6c4d0fca5ebb5d8338"><td class="memItemLeft" align="right" valign="top">Tensor&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="#a51f0921a8e934c6c4d0fca5ebb5d8338">pack_segments_backward_cpu</a> (const Tensor &amp;data, const Tensor &amp;lengths, const int64_t total_length, const int64_t max_length)</td></tr>
<tr class="separator:a51f0921a8e934c6c4d0fca5ebb5d8338"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a24acd9b73d0192a8b79da788762c7806" id="r_a24acd9b73d0192a8b79da788762c7806"><td class="memItemLeft" align="right" valign="top">Tensor&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="#a24acd9b73d0192a8b79da788762c7806">pack_segments_backward_cuda</a> (const Tensor &amp;data, const Tensor &amp;lengths, int64_t total_length, int64_t max_length)</td></tr>
<tr class="separator:a24acd9b73d0192a8b79da788762c7806"><td class="memSeparator" colspan="2">&#160;</td></tr>
<tr class="memitem:a82c2bc63d382fd3881bcb6794b8d5e1d" id="r_a82c2bc63d382fd3881bcb6794b8d5e1d"><td class="memItemLeft" align="right" valign="top">Tensor&#160;</td><td class="memItemRight" valign="bottom"><a class="el" href="#a82c2bc63d382fd3881bcb6794b8d5e1d">pack_segments_forward_cuda</a> (const Tensor &amp;t_in, const Tensor &amp;lengths, const int64_t max_length)</td></tr>
<tr class="separator:a82c2bc63d382fd3881bcb6794b8d5e1d"><td class="memSeparator" colspan="2">&#160;</td></tr>
</table>
<a name="details" id="details"></a><h2 class="groupheader">Detailed Description</h2>
<div class="textblock"><p>This is a placeholder source file that is used to force compilation and generation of an .SO file. </p>
</div><h2 class="groupheader">Function Documentation</h2>
<a id="ad21c70bdd84772ee2b9b3950c87e9791" name="ad21c70bdd84772ee2b9b3950c87e9791"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ad21c70bdd84772ee2b9b3950c87e9791">&#9670;&#160;</a></span>__launch_bounds__()</h2>

<div class="memitem">
<div class="memproto">
<div class="memtemplate">
template&lt;int NUM_JAGGED_DIM, typename index_t , typename scalar_t , typename F &gt; </div>
      <table class="memname">
        <tr>
          <td class="memname">__global__ __launch_bounds__ </td>
          <td>(</td>
          <td class="paramtype">kMaxThreads</td>          <td class="paramname"><span class="paramname"><em></em></span></td><td>)</td>
          <td> const</td>
        </tr>
      </table>
</div><div class="memdoc">
<p>output = f(x, y) where x and y are jagged (and share x_offsets), and output is dense.</p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">padding_value</td><td>padding_value for the output, not for inputs </td></tr>
  </table>
  </dd>
</dl>

</div>
</div>
<a id="a96db75aa5b2617976c2937ab051b737e" name="a96db75aa5b2617976c2937ab051b737e"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a96db75aa5b2617976c2937ab051b737e">&#9670;&#160;</a></span>batched_unary_embeddings_forward_cpu()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">Tensor batched_unary_embeddings_forward_cpu </td>
          <td>(</td>
          <td class="paramtype">const Tensor &amp;</td>          <td class="paramname"><span class="paramname"><em>weight</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const Tensor &amp;</td>          <td class="paramname"><span class="paramname"><em>table_offsets</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const Tensor &amp;</td>          <td class="paramname"><span class="paramname"><em>offsets</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const Tensor &amp;</td>          <td class="paramname"><span class="paramname"><em>indices</em></span>&#160;)</td>
        </tr>
      </table>
</div><div class="memdoc">
<p>CPU version of batched_unary_embeddings forward pass.</p>
<p>Sums up <code>weight</code> embeddings according to <code>offsets</code> and <code>indices</code>. <code>table_offests</code> is a helper struct to quickly navigate through tables in <code>weight</code> &ndash; it is caller's responsibility to keep it in sync with <code>weight</code>. Visualization of op semantics: <a href="https://fburl.com/9a4uktmb">https://fburl.com/9a4uktmb</a></p>
<p>This version is only for numerical verification so not optimized for performance.</p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">weight</td><td>- Weight for the embeddings. </td></tr>
    <tr><td class="paramname">table_offsets</td><td>- Index offsets for each table entry in <code>weight</code>. </td></tr>
    <tr><td class="paramname">offsets</td><td>- Offsets for the starting point of each summation. </td></tr>
    <tr><td class="paramname">indices</td><td>- Indices for the embeddings to fetch (from <code>weight</code>). </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>The sumed embeddings. </dd></dl>

</div>
</div>
<a id="a358cc74c17a25856e51a4bf73a8247ec" name="a358cc74c17a25856e51a4bf73a8247ec"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a358cc74c17a25856e51a4bf73a8247ec">&#9670;&#160;</a></span>embedding_inplace_update_cuda()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">void embedding_inplace_update_cuda </td>
          <td>(</td>
          <td class="paramtype">Tensor</td>          <td class="paramname"><span class="paramname"><em>dev_weights</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Tensor</td>          <td class="paramname"><span class="paramname"><em>uvm_weights</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Tensor</td>          <td class="paramname"><span class="paramname"><em>weights_placements</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Tensor</td>          <td class="paramname"><span class="paramname"><em>weights_offsets</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Tensor</td>          <td class="paramname"><span class="paramname"><em>weights_tys</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Tensor</td>          <td class="paramname"><span class="paramname"><em>D_offsets</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Tensor</td>          <td class="paramname"><span class="paramname"><em>update_weights</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Tensor</td>          <td class="paramname"><span class="paramname"><em>update_table_idx</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Tensor</td>          <td class="paramname"><span class="paramname"><em>update_row_idx</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">Tensor</td>          <td class="paramname"><span class="paramname"><em>update_offsets</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const int64_t</td>          <td class="paramname"><span class="paramname"><em>row_alignment</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">std::optional&lt; Tensor &gt;</td>          <td class="paramname"><span class="paramname"><em>lxu_cache_weights</em></span><span class="paramdefsep"> = </span><span class="paramdefval">std::nullopt</span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">std::optional&lt; Tensor &gt;</td>          <td class="paramname"><span class="paramname"><em>lxu_cache_locations</em></span><span class="paramdefsep"> = </span><span class="paramdefval">std::nullopt</span>&#160;)</td>
        </tr>
      </table>
</div><div class="memdoc">
<p>Embedding tables inplace updates with absolute values (idempotent guarantee)</p>
<p>dev_weights: the loaded tables on device in TBE format uvm_weights: the loaded tables on UVM in TBE format weights_placements: placements for each table weights_offsets: physical offsets for each table weights_tys: weight types for each table D_offsets: table dimensions update_weights: new update weights tensor in TBE format update_table_idx: table indices for every new row update_row_idx: row indices for every new row update_offsets: offsets of new update weights row_alignment: alignment byte for embedding row lxu_cache_weights: the loaded cache weights lxu_cache_locations: the loaded cache location info</p>
<p>it's guaranteed from upper service level that each row of table will only receive one update at a time.</p>
<p>This function has embedding update parameters (update_weights, update_table_idx, updata_offsets) and delta embedding weights on the CUDA devices. </p>

</div>
</div>
<a id="ae86238f4ca864fb4ea41318ece747ab4" name="ae86238f4ca864fb4ea41318ece747ab4"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ae86238f4ca864fb4ea41318ece747ab4">&#9670;&#160;</a></span>inclusive_sum_scan_kernel()</h2>

<div class="memitem">
<div class="memproto">
<div class="memtemplate">
template&lt;typename scalar_t , int ITEMS_PER_THREAD, int NUM_THREADS_PER_BLOCK&gt; </div>
      <table class="memname">
        <tr>
          <td class="memname">__inline__ __device__ void inclusive_sum_scan_kernel </td>
          <td>(</td>
          <td class="paramtype">scalar_t(&amp;)</td>          <td class="paramname"><span class="paramname"><em>arr</em></span>[ITEMS_PER_THREAD], </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">typename cub::BlockScan&lt; scalar_t, NUM_THREADS_PER_BLOCK &gt;::TempStorage &amp;</td>          <td class="paramname"><span class="paramname"><em>temp_storage</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int *</td>          <td class="paramname"><span class="paramname"><em>block_flags</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">volatile scalar_t *</td>          <td class="paramname"><span class="paramname"><em>block_sums</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">scalar_t *</td>          <td class="paramname"><span class="paramname"><em>block_prev</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const int</td>          <td class="paramname"><span class="paramname"><em>num_entries_per_block</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const int</td>          <td class="paramname"><span class="paramname"><em>block_id</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const bool</td>          <td class="paramname"><span class="paramname"><em>is_multi_block</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const int</td>          <td class="paramname"><span class="paramname"><em>signal</em></span>&#160;)</td>
        </tr>
      </table>
</div><div class="memdoc">
<p>inclusive_sum_scan_kernel performs intra- and inter-thread block sum scan (i.e., prefix sum scan). We use cub::BlockScan to do inclusive sum within thread block and use a waterfall sync method to perform prefix sum across thread block.</p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">arr</td><td>an array of input values. Its length must be fixed to ITEMS_PER_THREAD </td></tr>
    <tr><td class="paramname">temp_storage</td><td>a shared memory struct for cub::BlockScan </td></tr>
    <tr><td class="paramname">block_flags</td><td>a global flag buffer for inter-block sync (must be initialized with zeros) </td></tr>
    <tr><td class="paramname">block_sums</td><td>a global sum buffer for inter-block sync </td></tr>
    <tr><td class="paramname">block_prev</td><td>a shared memory pointer for sharing sum from the previous block within a block </td></tr>
    <tr><td class="paramname">num_entries_per_block</td><td>a number of input entries for this block </td></tr>
    <tr><td class="paramname">block_id</td><td>a relative thread block ID (the first block that contains the first set of input entries has block_id = 0) </td></tr>
    <tr><td class="paramname">is_multi_block</td><td>a boolean to indicate if inter-block sum scan has to be performed </td></tr>
    <tr><td class="paramname">signal</td><td>If the value of block_flags of the previous block is equal to signal, it means that the previous block has written its sum to block_sums. We have thread blocks increment the value of block_flags by one after they write their sums to block_sums. We increment the flag instead of setting the flag to a single value to support multiple sequential inclusive_sum_scan_kernel calls (e.g., in the AUC kernel). signal is the order that inclusive_sum_scan_kernel is called. Since we intialize block_flags with zeros, the signal of the first call should be one. </td></tr>
  </table>
  </dd>
</dl>

</div>
</div>
<a id="af80524a7d454f6db1c478808e8a659a6" name="af80524a7d454f6db1c478808e8a659a6"></a>
<h2 class="memtitle"><span class="permalink"><a href="#af80524a7d454f6db1c478808e8a659a6">&#9670;&#160;</a></span>jagged_index_add_2d_forward_cpu()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">Tensor jagged_index_add_2d_forward_cpu </td>
          <td>(</td>
          <td class="paramtype">const Tensor &amp;</td>          <td class="paramname"><span class="paramname"><em>values</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const Tensor &amp;</td>          <td class="paramname"><span class="paramname"><em>indices</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const Tensor &amp;</td>          <td class="paramname"><span class="paramname"><em>input_offsets</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const Tensor &amp;</td>          <td class="paramname"><span class="paramname"><em>output_offsets</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const int64_t</td>          <td class="paramname"><span class="paramname"><em>num_dense_input_rows</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const int64_t</td>          <td class="paramname"><span class="paramname"><em>num_output_rows</em></span>&#160;)</td>
        </tr>
      </table>
</div><div class="memdoc">
<p>Add sequences from input jagged tensor to output jagged tensor based on indices specified in the indices tensor (this function invokes jagged_index_add_2d_kernel) </p><dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">values</td><td>2D dense value tensor of input jagged tensor </td></tr>
    <tr><td class="paramname">indices</td><td>1D tensor that contains indices to be added in output jagged tensor </td></tr>
    <tr><td class="paramname">input_offsets</td><td>1D tensor that contains offsets of input jagged tensor </td></tr>
    <tr><td class="paramname">output_offsets</td><td>1D tensor that contains offsets of output jagged tensor </td></tr>
    <tr><td class="paramname">num_dense_input_rows</td><td>The total number of rows in the 2D dense value tensor of input jagged tensor </td></tr>
    <tr><td class="paramname">num_output_rows</td><td>The number of sequences in jagged output tensor </td></tr>
  </table>
  </dd>
</dl>

</div>
</div>
<a id="a53a6da74de342260dcb15c68e9bddfd6" name="a53a6da74de342260dcb15c68e9bddfd6"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a53a6da74de342260dcb15c68e9bddfd6">&#9670;&#160;</a></span>jagged_index_add_2d_forward_cuda()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">Tensor jagged_index_add_2d_forward_cuda </td>
          <td>(</td>
          <td class="paramtype">const Tensor &amp;</td>          <td class="paramname"><span class="paramname"><em>values</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const Tensor &amp;</td>          <td class="paramname"><span class="paramname"><em>indices</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const Tensor &amp;</td>          <td class="paramname"><span class="paramname"><em>input_offsets</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const Tensor &amp;</td>          <td class="paramname"><span class="paramname"><em>output_offsets</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const int64_t</td>          <td class="paramname"><span class="paramname"><em>num_dense_input_rows</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const int64_t</td>          <td class="paramname"><span class="paramname"><em>num_output_rows</em></span>&#160;)</td>
        </tr>
      </table>
</div><div class="memdoc">
<p>Add sequences from input jagged tensor to output jagged tensor based on indices specified in the indices tensor (host function for dispatching jagged_index_add_2d_kernel to GPU) </p><dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">values</td><td>2D dense value tensor of input jagged tensor </td></tr>
    <tr><td class="paramname">indices</td><td>1D tensor that contains indices to be added in output jagged tensor </td></tr>
    <tr><td class="paramname">input_offsets</td><td>1D tensor that contains offsets of input jagged tensor </td></tr>
    <tr><td class="paramname">output_offsets</td><td>1D tensor that contains offsets of output jagged tensor </td></tr>
    <tr><td class="paramname">num_dense_input_rows</td><td>The total number of rows in the 2D dense value tensor of input jagged tensor </td></tr>
    <tr><td class="paramname">num_output_rows</td><td>The number of sequences in jagged output tensor </td></tr>
  </table>
  </dd>
</dl>

</div>
</div>
<a id="aafb575930df07db6f09594c7e2d08650" name="aafb575930df07db6f09594c7e2d08650"></a>
<h2 class="memtitle"><span class="permalink"><a href="#aafb575930df07db6f09594c7e2d08650">&#9670;&#160;</a></span>jagged_index_select_2d()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">std::vector&lt; Tensor &gt; jagged_index_select_2d </td>
          <td>(</td>
          <td class="paramtype">const Tensor &amp;</td>          <td class="paramname"><span class="paramname"><em>values</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const Tensor &amp;</td>          <td class="paramname"><span class="paramname"><em>lengths</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const Tensor &amp;</td>          <td class="paramname"><span class="paramname"><em>indices</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const std::optional&lt; int64_t &gt;</td>          <td class="paramname"><span class="paramname"><em>num_dense_output_rows</em></span>&#160;)</td>
        </tr>
      </table>
</div><div class="memdoc">
<p>Call the autograd function of jagged_index_select_2d</p>
<p>Forward: Copy sequences from input jagged tensor based on indices specified in the indices tensor to output jagged tensor</p>
<p>Backward: Add sequences from output gradient jagged tensor to input gradient jagged tensor based on indices specified in the indices tensor</p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">values</td><td>2D dense value of input jagged tensor </td></tr>
    <tr><td class="paramname">lengths</td><td>1D tensor that contains sequence lengths of input jagged tensor </td></tr>
    <tr><td class="paramname">indices</td><td>1D tensor that contains indices to be selected from input jagged tensor </td></tr>
  </table>
  </dd>
</dl>

</div>
</div>
<a id="a71a54a14d90862afc8e5fe03e0c9ed8f" name="a71a54a14d90862afc8e5fe03e0c9ed8f"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a71a54a14d90862afc8e5fe03e0c9ed8f">&#9670;&#160;</a></span>jagged_index_select_2d_forward_cpu()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">Tensor jagged_index_select_2d_forward_cpu </td>
          <td>(</td>
          <td class="paramtype">const Tensor &amp;</td>          <td class="paramname"><span class="paramname"><em>values</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const Tensor &amp;</td>          <td class="paramname"><span class="paramname"><em>indices</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const Tensor &amp;</td>          <td class="paramname"><span class="paramname"><em>input_offsets</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const Tensor &amp;</td>          <td class="paramname"><span class="paramname"><em>output_offsets</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const int64_t</td>          <td class="paramname"><span class="paramname"><em>num_dense_output_rows</em></span>&#160;)</td>
        </tr>
      </table>
</div><div class="memdoc">
<p>Copy sequences from input jagged tensor based on indices specified in the indices tensor to output jagged tensor (this function invokes jagged_index_select_2d_kernel) </p><dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">values</td><td>2D dense value tensor of input jagged tensor </td></tr>
    <tr><td class="paramname">indices</td><td>1D tensor that contains indices to be selected from input jagged tensor </td></tr>
    <tr><td class="paramname">input_offsets</td><td>1D tensor that contains offsets of input jagged tensor </td></tr>
    <tr><td class="paramname">output_offsets</td><td>1D tensor that contains offsets of output jagged tensor </td></tr>
    <tr><td class="paramname">num_dense_output_rows</td><td>The total number of rows in the 2D dense value tensor of output jagged tensor </td></tr>
  </table>
  </dd>
</dl>

</div>
</div>
<a id="acb5a744fbd29c8a3a25621c2850686c1" name="acb5a744fbd29c8a3a25621c2850686c1"></a>
<h2 class="memtitle"><span class="permalink"><a href="#acb5a744fbd29c8a3a25621c2850686c1">&#9670;&#160;</a></span>jagged_index_select_2d_forward_cuda()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">Tensor jagged_index_select_2d_forward_cuda </td>
          <td>(</td>
          <td class="paramtype">const Tensor &amp;</td>          <td class="paramname"><span class="paramname"><em>values</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const Tensor &amp;</td>          <td class="paramname"><span class="paramname"><em>indices</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const Tensor &amp;</td>          <td class="paramname"><span class="paramname"><em>input_offsets</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const Tensor &amp;</td>          <td class="paramname"><span class="paramname"><em>output_offsets</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const int64_t</td>          <td class="paramname"><span class="paramname"><em>num_dense_output_rows</em></span>&#160;)</td>
        </tr>
      </table>
</div><div class="memdoc">
<p>Copy sequences from input jagged tensor based on indices specified in the indices tensor to an output jagged tensor (host function for dispatching jagged_index_select_2d_kernel to GPU) </p><dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">values</td><td>2D dense value tensor of input jagged tensor </td></tr>
    <tr><td class="paramname">indices</td><td>1D tensor that contains indices to be selected from output jagged tensor </td></tr>
    <tr><td class="paramname">input_offsets</td><td>1D tensor that contains offsets of input jagged tensor </td></tr>
    <tr><td class="paramname">output_offsets</td><td>1D tensor that contains offsets of output jagged tensor </td></tr>
    <tr><td class="paramname">num_dense_output_rows</td><td>The total number of rows in the 2D dense value tensor of output jagged tensor </td></tr>
  </table>
  </dd>
</dl>

</div>
</div>
<a id="a4e6521d00a6f81ad8ad7f7d38eef1aea" name="a4e6521d00a6f81ad8ad7f7d38eef1aea"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a4e6521d00a6f81ad8ad7f7d38eef1aea">&#9670;&#160;</a></span>jagged_slice_forward_cpu()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">Tensor jagged_slice_forward_cpu </td>
          <td>(</td>
          <td class="paramtype">const Tensor &amp;</td>          <td class="paramname"><span class="paramname"><em>x_values</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const Tensor &amp;</td>          <td class="paramname"><span class="paramname"><em>x_lengths</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const Tensor &amp;</td>          <td class="paramname"><span class="paramname"><em>src_start</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const Tensor &amp;</td>          <td class="paramname"><span class="paramname"><em>output_lengths</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const Tensor &amp;</td>          <td class="paramname"><span class="paramname"><em>tgt_start</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const int64_t</td>          <td class="paramname"><span class="paramname"><em>num_output_rows</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const int64_t</td>          <td class="paramname"><span class="paramname"><em>slice_length</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const bool</td>          <td class="paramname"><span class="paramname"><em>fill_zeros</em></span>&#160;)</td>
        </tr>
      </table>
</div><div class="memdoc">
<p>Slice the jagged dim to max length from slice_length, from start point <code>start</code>. This is a jagged -&gt; jagged op </p><dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">x_values</td><td>- X values of shape B * J_DIM where J_DIM is jagged dim </td></tr>
    <tr><td class="paramname">x_lengths</td><td>- length along jagged dim </td></tr>
    <tr><td class="paramname">src_start</td><td>- start of slice operation from the src tensor </td></tr>
    <tr><td class="paramname">output_lengths</td><td>- length of jagged dim for output tensor </td></tr>
    <tr><td class="paramname">tgt_start</td><td>- position to start filling in sliced values from source </td></tr>
    <tr><td class="paramname">num_output_rows</td><td>- output dense dim </td></tr>
    <tr><td class="paramname">slice_length</td><td>- length of jagged dim to slice </td></tr>
    <tr><td class="paramname">fill_zeros</td><td>- option exists as an optimization, we can reuse the same code path for forward &amp; backward. For backward we need to fill zeros in output tensor but fwd we don't. </td></tr>
  </table>
  </dd>
</dl>

</div>
</div>
<a id="a8ee4a87df31fced027e10cd1bfb649ee" name="a8ee4a87df31fced027e10cd1bfb649ee"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a8ee4a87df31fced027e10cd1bfb649ee">&#9670;&#160;</a></span>kt_regroup_arguments_cpu()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">std::tuple&lt; Tensor, Tensor, Tensor, std::vector&lt; int64_t &gt; &gt; kt_regroup_arguments_cpu </td>
          <td>(</td>
          <td class="paramtype">const Tensor &amp;</td>          <td class="paramname"><span class="paramname"><em></em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const std::vector&lt; std::vector&lt; std::string &gt; &gt; &amp;</td>          <td class="paramname"><span class="paramname"><em>keys</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const std::vector&lt; std::vector&lt; int64_t &gt; &gt; &amp;</td>          <td class="paramname"><span class="paramname"><em>lengths</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const std::vector&lt; std::vector&lt; std::string &gt; &gt; &amp;</td>          <td class="paramname"><span class="paramname"><em>groups</em></span>&#160;)</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>generate the permutes arguments for permute_multi_embedding operator </p>
<p>This is a helper function for the permute_multi_embedding operator. It generates the required arguments for permute_multi_embedding operator. including permutes, in_shapes, out_shapes, and out_lengths.</p>
<p><b>Example:</b> </p><div class="fragment"><div class="line"><span class="comment"># input arguments</span></div>
<div class="line">keys = [[<span class="stringliteral">&quot;F1&quot;</span>, <span class="stringliteral">&quot;F2&quot;</span>], [<span class="stringliteral">&quot;F3&quot;</span>, <span class="stringliteral">&quot;F4&quot;</span>]]</div>
<div class="line">lengths = [[128, 128], [64, 32]]</div>
<div class="line">batch_size = 1024</div>
<div class="line">values = [torch.randn(batch_size, 256), torch.randn(batch_size, 96)]</div>
<div class="line"> </div>
<div class="line"><span class="comment"># target output KTs</span></div>
<div class="line">groups = [[<span class="stringliteral">&quot;F1&quot;</span>, <span class="stringliteral">&quot;F3&quot;</span>], [<span class="stringliteral">&quot;F2&quot;</span>, <span class="stringliteral">&quot;F4&quot;</span>]]</div>
<div class="line"> </div>
<div class="line"><span class="comment"># generate permutes</span></div>
<div class="line">permutes, in_shapes, out_shapes, out_lengths = kt_regroup_arguments(keys,</div>
<div class="line">lengths, groups)</div>
<div class="line"> </div>
<div class="line"><span class="comment"># permute and regroup</span></div>
<div class="line">permuted_values = permute_multi_embedding(values, permutes, in_shapes,</div>
<div class="line">out_shapes, lengths)</div>
</div><!-- fragment --><dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">emb</td><td>one of the tensors from KTs' values </td></tr>
    <tr><td class="paramname">keys</td><td>List[List[str]], each string represents a feature/key in a KT a list of keys represents a KT </td></tr>
    <tr><td class="paramname">lengths</td><td>List[List[int64_t]], each int represents the length of a feature/key in a KT, and a list of lengths represents a KT </td></tr>
    <tr><td class="paramname">groups</td><td>List[List[str]], each string represents a feature/key in an output KT a list of strings represents one output KT </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>tuple of permutes, in_shapes, out_shapes and output_lengths. See the inputs of permute_multi_embedding for more details. The output tensors should be contiguous, and on the same device as the input tensor.</dd></dl>
<dl class="section note"><dt>Note</dt><dd>This operator doesn't need autograd since it's purely about index.</dd></dl>
<dl class="section warning"><dt>Warning</dt><dd>the dispatcher should be able to dispatch meta function for this operator. </dd></dl>

</div>
</div>
<a id="ac785a168bee4c98b8993d5cadf846267" name="ac785a168bee4c98b8993d5cadf846267"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ac785a168bee4c98b8993d5cadf846267">&#9670;&#160;</a></span>kt_regroup_arguments_gpu()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">std::tuple&lt; Tensor, Tensor, Tensor, std::vector&lt; int64_t &gt; &gt; kt_regroup_arguments_gpu </td>
          <td>(</td>
          <td class="paramtype">const Tensor &amp;</td>          <td class="paramname"><span class="paramname"><em>emb</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const std::vector&lt; std::vector&lt; std::string &gt; &gt; &amp;</td>          <td class="paramname"><span class="paramname"><em>keys</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const std::vector&lt; std::vector&lt; int64_t &gt; &gt; &amp;</td>          <td class="paramname"><span class="paramname"><em>lengths</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const std::vector&lt; std::vector&lt; std::string &gt; &gt; &amp;</td>          <td class="paramname"><span class="paramname"><em>groups</em></span>&#160;)</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>generate the permutes arguments for permute_multi_embedding operator </p>
<p>This is a helper function for the permute_multi_embedding operator. It generates the required arguments for permute_multi_embedding operator. including permutes, in_shapes, out_shapes, and out_lengths. It also move the arguments (tensors) to the corresponding CUDA device.</p>
<p><b>Example:</b> </p><div class="fragment"><div class="line"><span class="comment"># input arguments</span></div>
<div class="line">keys = [[<span class="stringliteral">&quot;F1&quot;</span>, <span class="stringliteral">&quot;F2&quot;</span>], [<span class="stringliteral">&quot;F3&quot;</span>, <span class="stringliteral">&quot;F4&quot;</span>]]</div>
<div class="line">lengths = [[128, 128], [64, 32]]</div>
<div class="line">batch_size = 1024</div>
<div class="line">values = [torch.randn(batch_size, 256), torch.randn(batch_size, 96)]</div>
<div class="line"> </div>
<div class="line"><span class="comment"># target output KTs</span></div>
<div class="line">groups = [[<span class="stringliteral">&quot;F1&quot;</span>, <span class="stringliteral">&quot;F3&quot;</span>], [<span class="stringliteral">&quot;F2&quot;</span>, <span class="stringliteral">&quot;F4&quot;</span>]]</div>
<div class="line"> </div>
<div class="line"><span class="comment"># generate permutes</span></div>
<div class="line">permutes, in_shapes, out_shapes, out_lengths = kt_regroup_arguments(keys,</div>
<div class="line">lengths, groups)</div>
<div class="line"> </div>
<div class="line"><span class="comment"># permute and regroup</span></div>
<div class="line">permuted_values = permute_multi_embedding(values, permutes, in_shapes,</div>
<div class="line">out_shapes, lengths)</div>
</div><!-- fragment --><dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">emb</td><td>one of the tensors from KTs' values </td></tr>
    <tr><td class="paramname">keys</td><td>List[List[str]], each string represents a feature/key in a KT a list of keys represents a KT </td></tr>
    <tr><td class="paramname">lengths</td><td>List[List[int64_t]], each int represents the length of a feature/key in a KT, and a list of lengths represents a KT </td></tr>
    <tr><td class="paramname">groups</td><td>List[List[str]], each string represents a feature/key in an output KT a list of strings represents one output KT </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>tuple of permutes, in_shapes, out_shapes and output_lengths. See the inputs of permute_multi_embedding for more details. The output tensors should be contiguous, and on the same device as the input tensor.</dd></dl>
<dl class="section note"><dt>Note</dt><dd>This operator doesn't need autograd since it's purely about index.</dd></dl>
<dl class="section warning"><dt>Warning</dt><dd>this gpu/cuda version will move the output tensors to corresponding CUDA device. </dd></dl>

</div>
</div>
<a id="a0b0cc504f8b357b0ca086e2c78d9b54c" name="a0b0cc504f8b357b0ca086e2c78d9b54c"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a0b0cc504f8b357b0ca086e2c78d9b54c">&#9670;&#160;</a></span>kt_regroup_arguments_impl()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">std::tuple&lt; std::vector&lt; int32_t &gt;, std::vector&lt; int32_t &gt;, std::vector&lt; int32_t &gt; &gt; kt_regroup_arguments_impl </td>
          <td>(</td>
          <td class="paramtype">const std::vector&lt; std::vector&lt; std::string &gt; &gt; &amp;</td>          <td class="paramname"><span class="paramname"><em>keys</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const std::vector&lt; std::vector&lt; int64_t &gt; &gt; &amp;</td>          <td class="paramname"><span class="paramname"><em>lengths</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const std::vector&lt; std::vector&lt; std::string &gt; &gt; &amp;</td>          <td class="paramname"><span class="paramname"><em>groups</em></span>&#160;)</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>actual implementation of generating permutes arguments for the permute_multi_embedding operator </p>
<p>This is a helper function for the permute_multi_embedding operator. It generates the vector-form required arguments for permute_multi_embedding operator. including permutes, in_shapes, out_shapes, and out_lengths.</p>
<p><b>example</b> </p><div class="fragment"><div class="line"><span class="preprocessor"># each row represents a key (feature) permute move, which consists of the</span></div>
<div class="line">following parameters: # [input_tensor_idx, output_tensor_idx, input_key_idx,</div>
<div class="line">output_key_idx, key_length, next] permutes = tensor(</div>
<div class="line">            [</div>
<div class="line">                [0, 0, 0, 0, 3, 4],  # f1</div>
<div class="line">                [1, 0, 0, 3, 5, 0],  # f3</div>
<div class="line">                [0, 1, 3, 0, 4, 0],  # f2</div>
<div class="line">                [1, 2, 5, 0, 6, 0],  # f4</div>
<div class="line">                [0, 2, 0, 6, 3, -6],  # f1</div>
<div class="line">                [2, 2, 0, 9, 8, 0],  # f6</div>
<div class="line">                [0, 3, 0, 0, 3, -8],  # f1</div>
<div class="line">                [1, 3, 11, 3, 7, 0],  # f5</div>
<div class="line">            ]</div>
<div class="line">)</div>
</div><!-- fragment --> <h1><a class="anchor" id="autotoc_md0"></a>
details</h1>
<ol type="1">
<li>from the above example usage, we can clearly see that the operatior takes in the following: a) values: List[torch.Tensor], which represents the input KTs. b) permutes: torch.Tensor, which contains the permute information, will be explained later. c) output_lengths_list: List[int], the lengths of the output tensors (KTs), which is needed to allocate memory on device ahead. d) in_lengths: torch.Tensor, lengths of input tensors, which is on device. e) out_lengths: torch.Tensor, lengths of output tensors, which is on device</li>
<li>the operator returns a list of tensors, which represents the permuted KTs</li>
<li><code>permute</code> is the most critical argument in this operator: a) 2-D tensor b) each row represents a key (feature) permute move c) a permute move = [input_tensor_id, output_tensor_id, input_start_idx, output_start_idx, feature_length, next] d) next is used in backward when a key (feature) from the input tensor is mapped to multiple places in the output tensors</li>
<li>The next a) It's only used in the backward computation b) it's usually 0, means no next c) it's non-zero when there is a duplicate in the permute, e.g., the same feature appears more than once in the output. d) the <code>next</code> is the next index of the very same feature in the permute sequence with some modifications. e) modification-1: <code>next</code> is positive when it's the first of its kind [Start] f) modification-2: <code>next</code> is negative when it's not the first of its kind [Continue]. g) modification-3: <code>next</code> is the negative value of the length of the permute sequence when it's the last of its kind. [Stop].</li>
</ol>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">keys</td><td>List[List[str]], each string represents a feature/key in a KT a list of keys represents a KT </td></tr>
    <tr><td class="paramname">lengths</td><td>List[List[int64_t]], each int represents the length of a feature/key in a KT, and a list of lengths represents a KT </td></tr>
    <tr><td class="paramname">groups</td><td>List[List[str]], each string represents a feature/key in an output KT a list of strings represents one output KT </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>tuple of permutes, in_shapes, out_shapes and output_lengths. See the inputs of permute_multi_embedding for more details. The output vector is in int32_t</dd></dl>
<dl class="section note"><dt>Note</dt><dd>this function is used internally for the gpu and cpu versions operators </dd></dl>

</div>
</div>
<a id="a51f0921a8e934c6c4d0fca5ebb5d8338" name="a51f0921a8e934c6c4d0fca5ebb5d8338"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a51f0921a8e934c6c4d0fca5ebb5d8338">&#9670;&#160;</a></span>pack_segments_backward_cpu()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">Tensor pack_segments_backward_cpu </td>
          <td>(</td>
          <td class="paramtype">const Tensor &amp;</td>          <td class="paramname"><span class="paramname"><em>data</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const Tensor &amp;</td>          <td class="paramname"><span class="paramname"><em>lengths</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const int64_t</td>          <td class="paramname"><span class="paramname"><em>total_length</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const int64_t</td>          <td class="paramname"><span class="paramname"><em>max_length</em></span>&#160;)</td>
        </tr>
      </table>
</div><div class="memdoc">
<p>Map N+1 dim tensor to N dim based on lengths tensor Sequences that are shorter than the longest sequence are padded with zeros. </p><dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">data</td><td>N+1 dim Tensor. </td></tr>
    <tr><td class="paramname">lengths</td><td>1D int/long tensor contains the length in each of the input. </td></tr>
    <tr><td class="paramname">total_length</td><td>Sum of elements in the 1D tensor legnths </td></tr>
    <tr><td class="paramname">max_length</td><td>The pre-defined max_length for the packed segments. -1 means autodetect </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>unpacked_tensor N-dimensional tensor </dd></dl>

</div>
</div>
<a id="a24acd9b73d0192a8b79da788762c7806" name="a24acd9b73d0192a8b79da788762c7806"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a24acd9b73d0192a8b79da788762c7806">&#9670;&#160;</a></span>pack_segments_backward_cuda()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">Tensor pack_segments_backward_cuda </td>
          <td>(</td>
          <td class="paramtype">const Tensor &amp;</td>          <td class="paramname"><span class="paramname"><em>data</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const Tensor &amp;</td>          <td class="paramname"><span class="paramname"><em>lengths</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int64_t</td>          <td class="paramname"><span class="paramname"><em>total_length</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int64_t</td>          <td class="paramname"><span class="paramname"><em>max_length</em></span>&#160;)</td>
        </tr>
      </table>
</div><div class="memdoc">
<p>Map N+1 dim tensor to N dim based on lengths tensor Sequences that are shorter than the longest sequence are padded with zeros. </p><dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">data</td><td>N+1 dim Tensor. </td></tr>
    <tr><td class="paramname">lengths</td><td>1D int/long tensor contains the length in each of the input. </td></tr>
    <tr><td class="paramname">total_length</td><td>Sum of elements in the 1D tensor legnths </td></tr>
    <tr><td class="paramname">max_length</td><td>The pre-defined max_length for the packed segments. </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>unpacked_tensor N-dimensional tensor </dd></dl>

</div>
</div>
<a id="a49cb5dd543cc63e932f458e1c79c0d00" name="a49cb5dd543cc63e932f458e1c79c0d00"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a49cb5dd543cc63e932f458e1c79c0d00">&#9670;&#160;</a></span>pack_segments_forward_cpu()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">Tensor pack_segments_forward_cpu </td>
          <td>(</td>
          <td class="paramtype">const Tensor &amp;</td>          <td class="paramname"><span class="paramname"><em>t_in</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const Tensor &amp;</td>          <td class="paramname"><span class="paramname"><em>lengths</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const int64_t</td>          <td class="paramname"><span class="paramname"><em>max_length</em></span>&#160;)</td>
        </tr>
      </table>
</div><div class="memdoc">
<p>Map N dim tensor to N+1 dim based on lengths tensor. Sequences that are shorter than the longest sequence are padded with zeros. </p><dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">t_in</td><td>N dim Tensor. </td></tr>
    <tr><td class="paramname">lengths</td><td>1D int/long tensor contains the length in each of the output. </td></tr>
    <tr><td class="paramname">max_length</td><td>The pre-defined max_length for the packed segments. -1 means autodetect </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>packed_tensor packed_tensor N + 1 dim Tensor where dim(1) is the max length, dim(0) is the batch size. </dd></dl>

</div>
</div>
<a id="a82c2bc63d382fd3881bcb6794b8d5e1d" name="a82c2bc63d382fd3881bcb6794b8d5e1d"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a82c2bc63d382fd3881bcb6794b8d5e1d">&#9670;&#160;</a></span>pack_segments_forward_cuda()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">Tensor pack_segments_forward_cuda </td>
          <td>(</td>
          <td class="paramtype">const Tensor &amp;</td>          <td class="paramname"><span class="paramname"><em>t_in</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const Tensor &amp;</td>          <td class="paramname"><span class="paramname"><em>lengths</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const int64_t</td>          <td class="paramname"><span class="paramname"><em>max_length</em></span>&#160;)</td>
        </tr>
      </table>
</div><div class="memdoc">
<p>Map N dim tensor to N+1 dim based on lengths tensor. Sequences that are shorter than the longest sequence are padded with zeros. </p><dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">t_in</td><td>N dim Tensor. </td></tr>
    <tr><td class="paramname">lengths</td><td>1D int/long tensor contains the length in each of the output. </td></tr>
    <tr><td class="paramname">max_length</td><td>The pre-defined max_length for the packed segments. </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>packed_tensor packed_tensor N + 1 dim Tensor where dim(1) is the max length, dim(0) is the batch size. </dd></dl>

</div>
</div>
<a id="af01b4023830652f0cc3e99c87f7b4526" name="af01b4023830652f0cc3e99c87f7b4526"></a>
<h2 class="memtitle"><span class="permalink"><a href="#af01b4023830652f0cc3e99c87f7b4526">&#9670;&#160;</a></span>padding_fused_tbe_input_combine_with_length_cpu()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">std::tuple&lt; Tensor, Tensor, Tensor &gt; padding_fused_tbe_input_combine_with_length_cpu </td>
          <td>(</td>
          <td class="paramtype">const std::vector&lt; Tensor &gt; &amp;</td>          <td class="paramname"><span class="paramname"><em>indices_list</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const std::vector&lt; Tensor &gt; &amp;</td>          <td class="paramname"><span class="paramname"><em>lengths_list</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const std::vector&lt; Tensor &gt; &amp;</td>          <td class="paramname"><span class="paramname"><em>per_sample_weights</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int64_t</td>          <td class="paramname"><span class="paramname"><em>batch_size</em></span>&#160;)</td>
        </tr>
      </table>
</div><div class="memdoc">
<p>padding_fused_tbe_input_combine_with_length_cpu is similar to tbe_input_combine_with_length_cpu, but padding all the lengths to the size specified by batch_size.</p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">indices_list</td><td>list of indices. </td></tr>
    <tr><td class="paramname">lengths_list</td><td>list of lengths. </td></tr>
    <tr><td class="paramname">per_sample_weights</td><td>list of per_sample_weights </td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>tuple of combined indices, lengths, and per_sample_weights </dd></dl>

</div>
</div>
<a id="ace574b23fe99327816839526e99298ac" name="ace574b23fe99327816839526e99298ac"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ace574b23fe99327816839526e99298ac">&#9670;&#160;</a></span>per_tensor_quantize_i8()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">at::Tensor per_tensor_quantize_i8 </td>
          <td>(</td>
          <td class="paramtype">at::Tensor</td>          <td class="paramname"><span class="paramname"><em>X</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">double</td>          <td class="paramname"><span class="paramname"><em>scale</em></span>&#160;)</td>
        </tr>
      </table>
</div><div class="memdoc">
<p>Apply int8 tensor-wise quantization on the input tensor X, with the scale</p>
<dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">X</td><td>The input tensor </td></tr>
    <tr><td class="paramname">scale</td><td>The scaling factor</td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>int8 tensor-wise quantized tensor </dd></dl>

</div>
</div>
<a id="ae85fe18941deb26c531c609da34d7c78" name="ae85fe18941deb26c531c609da34d7c78"></a>
<h2 class="memtitle"><span class="permalink"><a href="#ae85fe18941deb26c531c609da34d7c78">&#9670;&#160;</a></span>permute_multi_embedding_autograd()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">std::vector&lt; Tensor &gt; permute_multi_embedding_autograd </td>
          <td>(</td>
          <td class="paramtype">const at::TensorList &amp;</td>          <td class="paramname"><span class="paramname"><em>pooled_embs</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const Tensor &amp;</td>          <td class="paramname"><span class="paramname"><em>permutes</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const Tensor &amp;</td>          <td class="paramname"><span class="paramname"><em>in_shapes</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const Tensor &amp;</td>          <td class="paramname"><span class="paramname"><em>out_shapes</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const c10::SymIntArrayRef</td>          <td class="paramname"><span class="paramname"><em>out_lengths</em></span>&#160;)</td>
        </tr>
      </table>
</div><div class="memdoc">

<p>permute and regroup keyed tensors </p>
<p>We often need to regroup keyed tensors (KTs) in a batch. For example, we have two KTs A and B, where A contains the pooled embeddings of two features (keys) F1 and F2, and B contains the pooled embeddings of two features (keys) F3 and F4. Both KTs have the same batch size.</p>
<p>We want to permute and regroup the KTs so that in the new KTs, F1 and F3 are grouped together, and F2 and F4 are grouped together.</p>
<p><b>Example:</b> </p><div class="fragment"><div class="line"><span class="comment"># input arguments</span></div>
<div class="line">keys = [[<span class="stringliteral">&quot;F1&quot;</span>, <span class="stringliteral">&quot;F2&quot;</span>], [<span class="stringliteral">&quot;F3&quot;</span>, <span class="stringliteral">&quot;F4&quot;</span>]]</div>
<div class="line">lengths = [[128, 128], [64, 32]]</div>
<div class="line">batch_size = 1024</div>
<div class="line">values = [torch.randn(batch_size, 256), torch.randn(batch_size, 96)]</div>
<div class="line"> </div>
<div class="line"><span class="comment"># target output KTs</span></div>
<div class="line">groups = [[<span class="stringliteral">&quot;F1&quot;</span>, <span class="stringliteral">&quot;F3&quot;</span>], [<span class="stringliteral">&quot;F2&quot;</span>, <span class="stringliteral">&quot;F4&quot;</span>]]</div>
<div class="line"> </div>
<div class="line"><span class="comment"># generate permutes</span></div>
<div class="line">permutes, in_shapes, out_shapes, out_lengths = kt_regroup_arguments(keys,</div>
<div class="line">lengths, groups)</div>
<div class="line"> </div>
<div class="line"><span class="comment"># permute and regroup</span></div>
<div class="line">permuted_values = permute_multi_embedding(values, permutes, in_shapes,</div>
<div class="line">out_shapes, lengths)</div>
</div><!-- fragment --><dl class="params"><dt>Parameters</dt><dd>
  <table class="params">
    <tr><td class="paramname">pooled_embs</td><td>list of tensors that from KTs' values </td></tr>
    <tr><td class="paramname">permutes</td><td>a 2D tensor with each row representing a permute operation. a permute operation is about how to move/copy a feature from the input KT to the output KT. the first column is the input tensor index, and the second column is the output tensor index. the third column is the feature's offset of input tensor, and the fourth column is the feature's offset of output tensor. the fifth column is the length of the feature in a permute, and the last column is a next permute row to operate on (used in backward only). </td></tr>
    <tr><td class="paramname">in_shapes</td><td>a 1D tensor with each element representing the length of an input KT. </td></tr>
    <tr><td class="paramname">out_shapes</td><td>a 1D tensor with each element representing the length of an output KT. </td></tr>
    <tr><td class="paramname">out_lengths</td><td>a 1D vector with each element representing the length of an output KT.</td></tr>
  </table>
  </dd>
</dl>
<dl class="section return"><dt>Returns</dt><dd>the values of the output KTs.</dd></dl>
<dl class="section note"><dt>Note</dt><dd>This operator supports autograd, and duplications in the output KTs are supported, such as [["F1", "F3"], ["F2", "F4"], ["F1", "F3"]]</dd></dl>
<dl class="section warning"><dt>Warning</dt><dd>when a feature is omitted from the output KTs, the gradient of the feature won't be set to 0. </dd></dl>

</div>
</div>
<a id="adda552b8784184a2f17aa997e10869f9" name="adda552b8784184a2f17aa997e10869f9"></a>
<h2 class="memtitle"><span class="permalink"><a href="#adda552b8784184a2f17aa997e10869f9">&#9670;&#160;</a></span>pruned_array_lookup_from_row_idx_cuda()</h2>

<div class="memitem">
<div class="memproto">
      <table class="memname">
        <tr>
          <td class="memname">Tensor pruned_array_lookup_from_row_idx_cuda </td>
          <td>(</td>
          <td class="paramtype">const Tensor &amp;</td>          <td class="paramname"><span class="paramname"><em>update_row_indices</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const Tensor &amp;</td>          <td class="paramname"><span class="paramname"><em>update_table_indices</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const Tensor &amp;</td>          <td class="paramname"><span class="paramname"><em>index_remappings</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const Tensor &amp;</td>          <td class="paramname"><span class="paramname"><em>index_remappings_offsets</em></span>&#160;)</td>
        </tr>
      </table>
</div><div class="memdoc">
<p>Index remapping function that returns the remapped indices.</p>
<p>Args: update_row_indices: row indices for every new row update_table_indices: table indices for every new row index_remappings: concated index remapping for every embedding table index_remappings_offsets: offset for each embedding table</p>
<p>Returns: remapped indices for each new row. </p>

</div>
</div>
<a id="a4f4260f8147fe33d5c0c5611ae0f7e0f" name="a4f4260f8147fe33d5c0c5611ae0f7e0f"></a>
<h2 class="memtitle"><span class="permalink"><a href="#a4f4260f8147fe33d5c0c5611ae0f7e0f">&#9670;&#160;</a></span>report_embedding_error()</h2>

<div class="memitem">
<div class="memproto">
<div class="memtemplate">
template&lt;typename IndexType , typename OffsetType &gt; </div>
      <table class="memname">
        <tr>
          <td class="memname">void report_embedding_error </td>
          <td>(</td>
          <td class="paramtype">int</td>          <td class="paramname"><span class="paramname"><em>t</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int</td>          <td class="paramname"><span class="paramname"><em>B</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int</td>          <td class="paramname"><span class="paramname"><em>b_begin</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int</td>          <td class="paramname"><span class="paramname"><em>b_end</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const OffsetType *</td>          <td class="paramname"><span class="paramname"><em>offsets_data</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">const IndexType *</td>          <td class="paramname"><span class="paramname"><em>indices_data</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">int64_t</td>          <td class="paramname"><span class="paramname"><em>hash_size</em></span>, </td>
        </tr>
        <tr>
          <td class="paramkey"></td>
          <td></td>
          <td class="paramtype">bool</td>          <td class="paramname"><span class="paramname"><em>allow_minus_one</em></span><span class="paramdefsep"> = </span><span class="paramdefval">false</span>&#160;)</td>
        </tr>
      </table>
</div><div class="memdoc">
<p>report error from fbgemm cpu embedding lookup kernels @params allow_minus_one true for embedding kernels generated with scale_bias_last == false that can take -1 indices (output from pruned embedding id mapping) </p>

</div>
</div>
</div><!-- contents -->
<!-- start footer part -->
<hr class="footer"/><address class="footer"><small>
Generated by&#160;<a href="https://www.doxygen.org/index.html"><img class="footer" src="doxygen.svg" width="104" height="31" alt="doxygen"/></a> 1.11.0
</small></address>
</div><!-- doc-content -->
</body>
</html>
