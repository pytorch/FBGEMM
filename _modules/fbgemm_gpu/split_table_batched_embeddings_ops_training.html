


<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>fbgemm_gpu.split_table_batched_embeddings_ops_training &mdash; FBGEMM 1.3.0 documentation</title>
  

  
  
  
  

  

  
  
    

  

  <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  <!-- <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" /> -->
  <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/graphviz.css" type="text/css" />
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
  <!-- Google Tag Manager -->
    <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
    new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
    j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
    'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
    })(window,document,'script','dataLayer','UA-117752657-2');</script>
    <!-- End Google Tag Manager -->
  

  
  <script src="../../_static/js/modernizr.min.js"></script>

  <!-- Preload the theme fonts -->

<link rel="preload" href="../../_static/fonts/FreightSans/freight-sans-book.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../_static/fonts/FreightSans/freight-sans-medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../_static/fonts/IBMPlexMono/IBMPlexMono-Medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../_static/fonts/FreightSans/freight-sans-bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../_static/fonts/FreightSans/freight-sans-medium-italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../_static/fonts/IBMPlexMono/IBMPlexMono-SemiBold.woff2" as="font" type="font/woff2" crossorigin="anonymous">

<!-- Preload the katex fonts -->

<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Math-Italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size1-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size4-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size2-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size3-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Caligraphic-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css" integrity="sha384-vSIIfh2YWi9wW0r9iZe7RJPrKwp6bG+s9QZMoITbCckVJqGCCRhc+ccxNcdpHuYu" crossorigin="anonymous">
</head>

<div class="container-fluid header-holder tutorials-header" id="header-holder">
  <div class="container">
    <div class="header-container">
      <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>

      <div class="main-menu">
        <ul>

          <li class="main-menu-item">
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Learn
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/get-started">
                  <span class=dropdown-title>Get Started</span>
                  <p>Run PyTorch locally or get started quickly with one of the supported cloud platforms</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tutorials">
                  <span class="dropdown-title">Tutorials</span>
                  <p>Whats new in PyTorch tutorials</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/beginner/basics/intro.html">
                  <span class="dropdown-title">Learn the Basics</span>
                  <p>Familiarize yourself with PyTorch concepts and modules</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/recipes/recipes_index.html">
                  <span class="dropdown-title">PyTorch Recipes</span>
                  <p>Bite-size, ready-to-deploy PyTorch code examples</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/beginner/introyt.html">
                  <span class="dropdown-title">Intro to PyTorch - YouTube Series</span>
                  <p>Master PyTorch basics with our engaging YouTube tutorial series</p>
                </a>
              </div>
            </div>
          </li>

          <li>
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Ecosystem
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/ecosystem">
                  <span class="dropdown-title">Tools</span>
                  <p>Learn about the tools and frameworks in the PyTorch Ecosystem</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/#community-module">
                  <span class=dropdown-title>Community</span>
                  <p>Join the PyTorch developer community to contribute, learn, and get your questions answered</p>
                </a>
                <a class="nav-dropdown-item" href="https://discuss.pytorch.org/" target="_blank">
                  <span class=dropdown-title>Forums</span>
                  <p>A place to discuss PyTorch code, issues, install, research</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/resources">
                  <span class=dropdown-title>Developer Resources</span>
                  <p>Find resources and get questions answered</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/ecosystem/contributor-awards-2024">
                  <span class="dropdown-title">Contributor Awards - 2024</span>
                  <p>Award winners announced at this year's PyTorch Conference</p>
                </a>
              </div>
            </div>
          </li>

          <li>
          <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Edge
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/edge">
                  <span class="dropdown-title">About PyTorch Edge</span>
                  <p>Build innovative and privacy-aware AI experiences for edge devices</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/executorch-overview">
                  <span class="dropdown-title">ExecuTorch</span>
                  <p>End-to-end solution for enabling on-device inference capabilities across mobile and edge devices</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/executorch/stable/index.html">
                  <span class="dropdown-title">ExecuTorch Docs</span>
                </a>
              </div>
            </div>  
          </li>

          <li class="main-menu-item">
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Docs
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/docs/stable/index.html">
                  <span class="dropdown-title">PyTorch</span>
                  <p>Explore the documentation for comprehensive guidance on how to use PyTorch</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/pytorch-domains">
                  <span class="dropdown-title">PyTorch Domains</span>
                  <p>Read the PyTorch Domains documentation to learn more about domain-specific libraries</p>
                </a>
              </div>
            </div>
          </li>

          <li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                Blogs & News 
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/blog/">
                  <span class="dropdown-title">PyTorch Blog</span>
                  <p>Catch up on the latest technical news and happenings</p>
                </a>
                 <a class="nav-dropdown-item" href="https://pytorch.org/community-blog">
                  <span class="dropdown-title">Community Blog</span>
                  <p>Stories from the PyTorch ecosystem</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/videos">
                  <span class="dropdown-title">Videos</span>
                  <p>Learn about the latest PyTorch tutorials, new, and more </p>
                <a class="nav-dropdown-item" href="https://pytorch.org/community-stories">
                  <span class="dropdown-title">Community Stories</span>
                  <p>Learn how our community solves real, everyday machine learning problems with PyTorch</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/events">
                  <span class="dropdown-title">Events</span>
                  <p>Find events, webinars, and podcasts</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/newsletter">
                  <span class="dropdown-title">Newsletter</span>
                  <p>Stay up-to-date with the latest updates</p>
                </a>
            </div>
          </li>

          <li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="with-down-arrow">
                About
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/foundation">
                  <span class="dropdown-title">PyTorch Foundation</span>
                  <p>Learn more about the PyTorch Foundation</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/governing-board">
                  <span class="dropdown-title">Governing Board</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/credits">
                  <span class="dropdown-title">Cloud Credit Program</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/tac">
                  <span class="dropdown-title">Technical Advisory Council</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/staff">
                  <span class="dropdown-title">Staff</span>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/contact-us">
                  <span class="dropdown-title">Contact Us</span>
                </a>
              </div>
            </div>
          </li>

          <li class="main-menu-item">
            <div class="no-dropdown">
              <a href="https://pytorch.org/join" data-cta="join">
                Become a Member
              </a>
            </div>
          </li>
          <li>
           <div class="main-menu-item">
             <a href="https://github.com/pytorch/pytorch" class="github-icon">
             </a>
           </div>
          </li>
          <!--- TODO: This block adds the search icon to the nav bar. We will enable it later. 
          <li>
            <div class="main-menu-item">
             <a href="https://github.com/pytorch/pytorch" class="search-icon">
             </a>
            </div>
          </li>
          --->
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
    </div>
  </div>
</div>

<body class="pytorch-body">

   

    

    <div class="table-of-contents-link-wrapper">
      <span>Table of Contents</span>
      <a href="#" class="toggle-table-of-contents" data-behavior="toggle-table-of-contents"></a>
    </div>

    <nav data-toggle="wy-nav-shift" class="pytorch-left-menu" id="pytorch-left-menu">
      <div class="pytorch-side-scroll">
        <div class="pytorch-menu pytorch-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          <div class="pytorch-left-menu-search">
            

            
              
              
                <div class="version">
                  1.3
                </div>
              
            

            


  


<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search Docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

            
          </div>

          
            
            
              
            
            
              <p class="caption" role="heading"><span class="caption-text">General Info</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../general/Releases.html">FBGEMM Releases</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../general/Contributing.html">Contributing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../general/documentation/Overview.html">Documentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../general/ContactUs.html">Contact Us</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../general/License.html">License</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">FBGEMM Development</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../fbgemm/development/BuildInstructions.html">Build Instructions</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">FBGEMM C++ API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../fbgemm/cpp-api/QuantUtils.html">Quantization Utilities</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../fbgemm/cpp-api/tbe_cpu_autovec.html">TBE CPU Autovectorization</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">FBGEMM_GPU Development</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../fbgemm_gpu/development/BuildInstructions.html">Build Instructions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../fbgemm_gpu/development/InstallationInstructions.html">Installation Instructions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../fbgemm_gpu/development/TestInstructions.html">Test Instructions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../fbgemm_gpu/development/FeatureGates.html">Feature Gates</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">FBGEMM_GPU Overview</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../fbgemm_gpu/overview/jagged-tensor-ops/JaggedTensorOps.html">Jagged Tensor Operators</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">FBGEMM Stable API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../fbgemm_gpu/stable-api/python_api.html">FBGEMM_GPU Stable Python API</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">FBGEMM_GPU C++ API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../fbgemm_gpu/cpp-api/sparse_ops.html">Sparse Data Operators</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../fbgemm_gpu/cpp-api/quantize_ops.html">Quantization Operators</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../fbgemm_gpu/cpp-api/merge_pooled_embeddings.html">Pooled Embeddings Operators</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../fbgemm_gpu/cpp-api/split_table_batched_embeddings.html">Table Batched Embedding Operators</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../fbgemm_gpu/cpp-api/jagged_tensor_ops.html">Jagged Tensor Operators</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../fbgemm_gpu/cpp-api/memory_utils.html">CUDA Memory Operators</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../fbgemm_gpu/cpp-api/input_combine.html">Combine Input Operators</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../fbgemm_gpu/cpp-api/layout_transform_ops.html">Layout Transformation Operators</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../fbgemm_gpu/cpp-api/embedding_ops.html">Embedding Operators</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../fbgemm_gpu/cpp-api/ssd_embedding_ops.html">SSD Embedding Operators</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../fbgemm_gpu/cpp-api/experimental_ops.html">Experimental Operators</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../fbgemm_gpu/cpp-api/feature_gates.html">Feature Gates (C++)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../fbgemm_gpu/cpp-api/faster_hash_ops.html">Faster Hash Operators</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">FBGEMM_GPU Python API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../fbgemm_gpu/python-api/sparse_ops.html">Sparse Operators</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../fbgemm_gpu/python-api/pooled_embedding_ops.html">Pooled Embedding Operators</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../fbgemm_gpu/python-api/quantize_ops.html">Quantization Operators</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../fbgemm_gpu/python-api/jagged_tensor_ops.html">Jagged Tensor Operators</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../fbgemm_gpu/python-api/tbe_ops_training.html">Table Batched Embedding (TBE) Training Module</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../fbgemm_gpu/python-api/tbe_ops_inference.html">Table Batched Embedding (TBE) Inference Module</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../fbgemm_gpu/python-api/pooled_embedding_modules.html">Pooled Embedding Modules</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../fbgemm_gpu/python-api/feature_gates.html">Feature Gates (Python)</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">FBGEMM GenAI Development</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../fbgemm_genai/development/BuildInstructions.html">Build Instructions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../fbgemm_genai/development/InstallationInstructions.html">Installation Instructions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../fbgemm_genai/development/TestInstructions.html">Test Instructions</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <div class="pytorch-container">
      <div class="pytorch-page-level-bar" id="pytorch-page-level-bar">
        <div class="pytorch-breadcrumbs-wrapper">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="pytorch-breadcrumbs">
    
      <li>
        <a href="../../index.html">
          
            Docs
          
        </a> &gt;
      </li>

        
          <li><a href="../index.html">Module code</a> &gt;</li>
        
      <li>fbgemm_gpu.split_table_batched_embeddings_ops_training</li>
    
    
      <li class="pytorch-breadcrumbs-aside">
        
      </li>
    
  </ul>

  
</div>
        </div>

        <div class="pytorch-shortcuts-wrapper" id="pytorch-shortcuts-wrapper">
          Shortcuts
        </div>
      </div>

      <section data-toggle="wy-nav-shift" id="pytorch-content-wrap" class="pytorch-content-wrap">
        <div class="pytorch-content-left">

        
          <!-- Google Tag Manager (noscript) -->
          <noscript><iframe src="https://www.googletagmanager.com/ns.html?id=UA-117752657-2"
          height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
          <!-- End Google Tag Manager (noscript) -->
          
          <div class="rst-content">
          
            <div role="main" class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article">
             <article itemprop="articleBody" id="pytorch-article" class="pytorch-article">
              
  <h1>Source code for fbgemm_gpu.split_table_batched_embeddings_ops_training</h1><div class="highlight"><pre>
<span></span><span class="ch">#!/usr/bin/env python3</span>
<span class="c1"># Copyright (c) Meta Platforms, Inc. and affiliates.</span>
<span class="c1"># All rights reserved.</span>
<span class="c1">#</span>
<span class="c1"># This source code is licensed under the BSD-style license found in the</span>
<span class="c1"># LICENSE file in the root directory of this source tree.</span>

<span class="c1"># pyre-strict</span>
<span class="c1"># pyre-ignore-all-errors[56]</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">contextlib</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">enum</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">functools</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">logging</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">math</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">os</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">uuid</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">dataclasses</span><span class="w"> </span><span class="kn">import</span> <span class="n">dataclass</span><span class="p">,</span> <span class="n">field</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">itertools</span><span class="w"> </span><span class="kn">import</span> <span class="n">accumulate</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">math</span><span class="w"> </span><span class="kn">import</span> <span class="n">log2</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">typing</span><span class="w"> </span><span class="kn">import</span> <span class="n">Any</span><span class="p">,</span> <span class="n">Callable</span><span class="p">,</span> <span class="n">Dict</span><span class="p">,</span> <span class="n">List</span><span class="p">,</span> <span class="n">Optional</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">,</span> <span class="n">Type</span><span class="p">,</span> <span class="n">Union</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>  <span class="c1"># usort:skip</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch</span><span class="w"> </span><span class="kn">import</span> <span class="n">nn</span><span class="p">,</span> <span class="n">Tensor</span>  <span class="c1"># usort:skip</span>

<span class="c1"># @manual=//deeplearning/fbgemm/fbgemm_gpu/codegen:split_embedding_codegen_lookup_invokers</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">fbgemm_gpu.split_embedding_codegen_lookup_invokers</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">invokers</span>

<span class="kn">from</span><span class="w"> </span><span class="nn">fbgemm_gpu.config</span><span class="w"> </span><span class="kn">import</span> <span class="n">FeatureGate</span><span class="p">,</span> <span class="n">FeatureGateName</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">fbgemm_gpu.runtime_monitor</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span>
    <span class="n">AsyncSeriesTimer</span><span class="p">,</span>
    <span class="n">TBEStatsReporter</span><span class="p">,</span>
    <span class="n">TBEStatsReporterConfig</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">fbgemm_gpu.split_embedding_configs</span><span class="w"> </span><span class="kn">import</span> <span class="n">EmbOptimType</span> <span class="k">as</span> <span class="n">OptimType</span><span class="p">,</span> <span class="n">SparseType</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">fbgemm_gpu.split_table_batched_embeddings_ops_common</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span>
    <span class="n">BoundsCheckMode</span><span class="p">,</span>
    <span class="n">CacheAlgorithm</span><span class="p">,</span>
    <span class="n">CacheState</span><span class="p">,</span>
    <span class="n">ComputeDevice</span><span class="p">,</span>
    <span class="n">construct_cache_state</span><span class="p">,</span>
    <span class="n">EmbeddingLocation</span><span class="p">,</span>
    <span class="n">get_bounds_check_version_for_platform</span><span class="p">,</span>
    <span class="n">MAX_PREFETCH_DEPTH</span><span class="p">,</span>
    <span class="n">MultiPassPrefetchConfig</span><span class="p">,</span>
    <span class="n">PoolingMode</span><span class="p">,</span>
    <span class="n">RecordCacheMetrics</span><span class="p">,</span>
    <span class="n">SplitState</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">fbgemm_gpu.split_table_batched_embeddings_ops_training_common</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span>
    <span class="n">generate_vbe_metadata</span><span class="p">,</span>
    <span class="n">is_torchdynamo_compiling</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">fbgemm_gpu.tbe_input_multiplexer</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span>
    <span class="n">TBEInfo</span><span class="p">,</span>
    <span class="n">TBEInputInfo</span><span class="p">,</span>
    <span class="n">TBEInputMultiplexer</span><span class="p">,</span>
    <span class="n">TBEInputMultiplexerConfig</span><span class="p">,</span>
<span class="p">)</span>

<span class="kn">from</span><span class="w"> </span><span class="nn">fbgemm_gpu.utils.loader</span><span class="w"> </span><span class="kn">import</span> <span class="n">load_torch_module</span><span class="p">,</span> <span class="n">load_torch_module_bc</span>

<span class="k">try</span><span class="p">:</span>
    <span class="n">load_torch_module</span><span class="p">(</span>
        <span class="s2">&quot;//deeplearning/fbgemm/fbgemm_gpu/codegen:embedding_ops_training_gpu&quot;</span><span class="p">,</span>
        <span class="s2">&quot;//deeplearning/fbgemm/fbgemm_gpu/codegen:embedding_ops_cuda_training&quot;</span><span class="p">,</span>
        <span class="s2">&quot;//deeplearning/fbgemm/fbgemm_gpu/codegen:embedding_ops_hip_training&quot;</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">load_torch_module_bc</span><span class="p">(</span>
        <span class="s2">&quot;//deeplearning/fbgemm/fbgemm_gpu/codegen:embedding_ops_training_cpu&quot;</span><span class="p">,</span>
        <span class="s2">&quot;//deeplearning/fbgemm/fbgemm_gpu/codegen:embedding_ops_cpu_training&quot;</span><span class="p">,</span>
    <span class="p">)</span>
<span class="k">except</span> <span class="ne">Exception</span><span class="p">:</span>
    <span class="k">pass</span>


<span class="n">DEFAULT_ASSOC</span> <span class="o">=</span> <span class="mi">32</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">version</span><span class="o">.</span><span class="n">hip</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="mi">64</span>
<span class="n">INT8_EMB_ROW_DIM_OFFSET</span> <span class="o">=</span> <span class="mi">8</span>


<span class="k">class</span><span class="w"> </span><span class="nc">DoesNotHavePrefix</span><span class="p">(</span><span class="ne">Exception</span><span class="p">):</span>
    <span class="k">pass</span>


<span class="k">class</span><span class="w"> </span><span class="nc">WeightDecayMode</span><span class="p">(</span><span class="n">enum</span><span class="o">.</span><span class="n">IntEnum</span><span class="p">):</span>
    <span class="n">NONE</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">L2</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="n">DECOUPLE</span> <span class="o">=</span> <span class="mi">2</span>
    <span class="n">COUNTER</span> <span class="o">=</span> <span class="mi">3</span>
    <span class="n">COWCLIP</span> <span class="o">=</span> <span class="mi">4</span>
    <span class="n">DECOUPLE_GLOBAL</span> <span class="o">=</span> <span class="mi">5</span>


<span class="k">class</span><span class="w"> </span><span class="nc">CounterWeightDecayMode</span><span class="p">(</span><span class="n">enum</span><span class="o">.</span><span class="n">IntEnum</span><span class="p">):</span>
    <span class="n">NONE</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">L2</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="n">DECOUPLE</span> <span class="o">=</span> <span class="mi">2</span>
    <span class="n">ADAGRADW</span> <span class="o">=</span> <span class="mi">3</span>


<span class="k">class</span><span class="w"> </span><span class="nc">StepMode</span><span class="p">(</span><span class="n">enum</span><span class="o">.</span><span class="n">IntEnum</span><span class="p">):</span>
    <span class="n">NONE</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">USE_COUNTER</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="n">USE_ITER</span> <span class="o">=</span> <span class="mi">2</span>


<span class="k">class</span><span class="w"> </span><span class="nc">LearningRateMode</span><span class="p">(</span><span class="n">enum</span><span class="o">.</span><span class="n">IntEnum</span><span class="p">):</span>
    <span class="n">EQUAL</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
    <span class="n">TAIL_ID_LR_INCREASE</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">TAIL_ID_LR_DECREASE</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="n">COUNTER_SGD</span> <span class="o">=</span> <span class="mi">2</span>


<span class="k">class</span><span class="w"> </span><span class="nc">GradSumDecay</span><span class="p">(</span><span class="n">enum</span><span class="o">.</span><span class="n">IntEnum</span><span class="p">):</span>
    <span class="n">NO_DECAY</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
    <span class="n">CTR_DECAY</span> <span class="o">=</span> <span class="mi">0</span>


<span class="nd">@dataclass</span><span class="p">(</span><span class="n">frozen</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="k">class</span><span class="w"> </span><span class="nc">TailIdThreshold</span><span class="p">:</span>
    <span class="n">val</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">is_ratio</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>


<span class="nd">@dataclass</span><span class="p">(</span><span class="n">frozen</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="k">class</span><span class="w"> </span><span class="nc">CounterBasedRegularizationDefinition</span><span class="p">:</span>
    <span class="n">counter_weight_decay_mode</span><span class="p">:</span> <span class="n">CounterWeightDecayMode</span> <span class="o">=</span> <span class="n">CounterWeightDecayMode</span><span class="o">.</span><span class="n">NONE</span>
    <span class="n">counter_halflife</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
    <span class="n">adjustment_iter</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
    <span class="n">adjustment_ub</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.0</span>
    <span class="n">learning_rate_mode</span><span class="p">:</span> <span class="n">LearningRateMode</span> <span class="o">=</span> <span class="n">LearningRateMode</span><span class="o">.</span><span class="n">EQUAL</span>
    <span class="n">grad_sum_decay</span><span class="p">:</span> <span class="n">GradSumDecay</span> <span class="o">=</span> <span class="n">GradSumDecay</span><span class="o">.</span><span class="n">NO_DECAY</span>
    <span class="n">tail_id_threshold</span><span class="p">:</span> <span class="n">TailIdThreshold</span> <span class="o">=</span> <span class="n">field</span><span class="p">(</span><span class="n">default_factory</span><span class="o">=</span><span class="n">TailIdThreshold</span><span class="p">)</span>
    <span class="n">max_counter_update_freq</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1000</span>


<span class="nd">@dataclass</span><span class="p">(</span><span class="n">frozen</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="k">class</span><span class="w"> </span><span class="nc">CowClipDefinition</span><span class="p">:</span>
    <span class="n">counter_weight_decay_mode</span><span class="p">:</span> <span class="n">CounterWeightDecayMode</span> <span class="o">=</span> <span class="n">CounterWeightDecayMode</span><span class="o">.</span><span class="n">NONE</span>
    <span class="n">counter_halflife</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
    <span class="n">weight_norm_coefficient</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.0</span>
    <span class="n">lower_bound</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.0</span>


<span class="nd">@dataclass</span><span class="p">(</span><span class="n">frozen</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="k">class</span><span class="w"> </span><span class="nc">GlobalWeightDecayDefinition</span><span class="p">:</span>
    <span class="n">start_iter</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">lower_bound</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.0</span>


<span class="nd">@dataclass</span><span class="p">(</span><span class="n">frozen</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="k">class</span><span class="w"> </span><span class="nc">UserEnabledConfigDefinition</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This class is used to configure whether certain modes are to be enabled</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c1"># This is used in Adam to perform rowwise bias correction using `row_counter`</span>
    <span class="c1"># More details can be found in D64848802.</span>
    <span class="n">use_rowwise_bias_correction</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="n">use_writeback_bwd_prehook</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>


<span class="nd">@dataclass</span><span class="p">(</span><span class="n">frozen</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="k">class</span><span class="w"> </span><span class="nc">EnsembleModeDefinition</span><span class="p">:</span>
    <span class="n">step_ema</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mi">10000</span>
    <span class="n">step_swap</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mi">10000</span>
    <span class="n">step_start</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">step_ema_coef</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.6</span>
    <span class="n">step_mode</span><span class="p">:</span> <span class="n">StepMode</span> <span class="o">=</span> <span class="n">StepMode</span><span class="o">.</span><span class="n">USE_ITER</span>


<span class="nd">@dataclass</span><span class="p">(</span><span class="n">frozen</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="k">class</span><span class="w"> </span><span class="nc">EmainplaceModeDefinition</span><span class="p">:</span>
    <span class="n">step_ema</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mi">10</span>
    <span class="n">step_start</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">step_ema_coef</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.6</span>


<span class="c1"># Keep in sync with fbgemm_gpu/include/fbgemm_gpu/split_embeddings_cache_cuda.cuh</span>
<span class="k">class</span><span class="w"> </span><span class="nc">UVMCacheStatsIndex</span><span class="p">(</span><span class="n">enum</span><span class="o">.</span><span class="n">IntEnum</span><span class="p">):</span>
    <span class="n">num_calls</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">num_requested_indices</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="n">num_unique_indices</span> <span class="o">=</span> <span class="mi">2</span>
    <span class="n">num_unique_misses</span> <span class="o">=</span> <span class="mi">3</span>
    <span class="n">num_conflict_unique_misses</span> <span class="o">=</span> <span class="mi">4</span>
    <span class="n">num_conflict_misses</span> <span class="o">=</span> <span class="mi">5</span>


<span class="nd">@dataclass</span>
<span class="k">class</span><span class="w"> </span><span class="nc">RESParams</span><span class="p">:</span>
    <span class="n">res_server_port</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span>  <span class="c1"># the port of the res server</span>
    <span class="n">res_store_shards</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span>  <span class="c1"># the number of shards to store the raw embeddings</span>
    <span class="n">table_names</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="n">field</span><span class="p">(</span><span class="n">default_factory</span><span class="o">=</span><span class="nb">list</span><span class="p">)</span>  <span class="c1"># table names the TBE holds</span>
    <span class="n">table_offsets</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="n">field</span><span class="p">(</span>
        <span class="n">default_factory</span><span class="o">=</span><span class="nb">list</span>
    <span class="p">)</span>  <span class="c1"># table offsets for the global rows the TBE holds</span>
    <span class="n">table_sizes</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="n">field</span><span class="p">(</span>
        <span class="n">default_factory</span><span class="o">=</span><span class="nb">list</span>
    <span class="p">)</span>  <span class="c1"># table sizes for the global rows the TBE holds</span>


<span class="k">def</span><span class="w"> </span><span class="nf">construct_split_state</span><span class="p">(</span>
    <span class="n">embedding_specs</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">,</span> <span class="n">EmbeddingLocation</span><span class="p">,</span> <span class="n">ComputeDevice</span><span class="p">]],</span>
    <span class="n">rowwise</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
    <span class="n">cacheable</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
    <span class="n">precision</span><span class="p">:</span> <span class="n">SparseType</span> <span class="o">=</span> <span class="n">SparseType</span><span class="o">.</span><span class="n">FP32</span><span class="p">,</span>
    <span class="n">int8_emb_row_dim_offset</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="n">INT8_EMB_ROW_DIM_OFFSET</span><span class="p">,</span>
    <span class="n">placement</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">EmbeddingLocation</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">SplitState</span><span class="p">:</span>
    <span class="n">placements</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">EmbeddingLocation</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">offsets</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">dev_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">host_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">uvm_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">num_embeddings</span><span class="p">,</span> <span class="n">embedding_dim</span><span class="p">,</span> <span class="n">location</span><span class="p">,</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">embedding_specs</span><span class="p">:</span>
        <span class="k">assert</span> <span class="p">(</span>
            <span class="n">embedding_dim</span> <span class="o">%</span> <span class="mi">4</span> <span class="o">==</span> <span class="mi">0</span>
        <span class="p">),</span> <span class="sa">f</span><span class="s2">&quot;embedding_dim must be a multiple of 4, but got </span><span class="si">{</span><span class="n">embedding_dim</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="k">if</span> <span class="n">precision</span> <span class="o">==</span> <span class="n">SparseType</span><span class="o">.</span><span class="n">INT8</span><span class="p">:</span>
            <span class="n">embedding_dim</span> <span class="o">+=</span> <span class="n">int8_emb_row_dim_offset</span>
        <span class="n">state_size</span> <span class="o">=</span> <span class="n">num_embeddings</span> <span class="o">*</span> <span class="n">embedding_dim</span> <span class="k">if</span> <span class="ow">not</span> <span class="n">rowwise</span> <span class="k">else</span> <span class="n">num_embeddings</span>
        <span class="n">location</span> <span class="o">=</span> <span class="n">placement</span> <span class="k">if</span> <span class="n">placement</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">location</span>
        <span class="k">if</span> <span class="n">location</span> <span class="o">==</span> <span class="n">EmbeddingLocation</span><span class="o">.</span><span class="n">HOST</span><span class="p">:</span>
            <span class="n">placements</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">EmbeddingLocation</span><span class="o">.</span><span class="n">HOST</span><span class="p">)</span>
            <span class="n">offsets</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">host_size</span><span class="p">)</span>
            <span class="n">host_size</span> <span class="o">+=</span> <span class="n">state_size</span>
        <span class="c1"># If table is on device, then opimtizer is on device.</span>
        <span class="c1"># If table is managed, then if optimizer state is rowwise, optimizer is on device, otherwise optimizer is managed.</span>
        <span class="k">elif</span> <span class="n">location</span> <span class="o">==</span> <span class="n">EmbeddingLocation</span><span class="o">.</span><span class="n">DEVICE</span> <span class="ow">or</span> <span class="n">rowwise</span><span class="p">:</span>
            <span class="n">placements</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">EmbeddingLocation</span><span class="o">.</span><span class="n">DEVICE</span><span class="p">)</span>
            <span class="n">offsets</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">dev_size</span><span class="p">)</span>
            <span class="n">dev_size</span> <span class="o">+=</span> <span class="n">state_size</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">cacheable</span> <span class="ow">and</span> <span class="n">location</span> <span class="o">==</span> <span class="n">EmbeddingLocation</span><span class="o">.</span><span class="n">MANAGED_CACHING</span><span class="p">:</span>
                <span class="n">placements</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">EmbeddingLocation</span><span class="o">.</span><span class="n">MANAGED_CACHING</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">placements</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">EmbeddingLocation</span><span class="o">.</span><span class="n">MANAGED</span><span class="p">)</span>
            <span class="n">offsets</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">uvm_size</span><span class="p">)</span>
            <span class="n">uvm_size</span> <span class="o">+=</span> <span class="n">state_size</span>
    <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">placements</span><span class="p">)</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">offsets</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">SplitState</span><span class="p">(</span>
        <span class="n">dev_size</span><span class="o">=</span><span class="n">dev_size</span><span class="p">,</span>
        <span class="n">host_size</span><span class="o">=</span><span class="n">host_size</span><span class="p">,</span>
        <span class="n">uvm_size</span><span class="o">=</span><span class="n">uvm_size</span><span class="p">,</span>
        <span class="n">placements</span><span class="o">=</span><span class="n">placements</span><span class="p">,</span>
        <span class="n">offsets</span><span class="o">=</span><span class="n">offsets</span><span class="p">,</span>
    <span class="p">)</span>


<span class="k">def</span><span class="w"> </span><span class="nf">apply_split_helper</span><span class="p">(</span>
    <span class="n">persistent_state_fn</span><span class="p">:</span> <span class="n">Callable</span><span class="p">[[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">],</span> <span class="kc">None</span><span class="p">],</span>
    <span class="n">set_attr_fn</span><span class="p">:</span> <span class="n">Callable</span><span class="p">[</span>
        <span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Union</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="n">List</span><span class="p">[</span><span class="n">EmbeddingLocation</span><span class="p">]]],</span> <span class="kc">None</span>
    <span class="p">],</span>
    <span class="n">current_device</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">,</span>
    <span class="n">use_cpu</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
    <span class="n">feature_table_map</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
    <span class="n">split</span><span class="p">:</span> <span class="n">SplitState</span><span class="p">,</span>
    <span class="n">prefix</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="n">dtype</span><span class="p">:</span> <span class="n">Type</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">dtype</span><span class="p">],</span>
    <span class="n">enforce_hbm</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">make_dev_param</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">dev_reshape</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="o">...</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">uvm_tensors_log</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">uvm_host_mapped</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">set_attr_fn</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">prefix</span><span class="si">}</span><span class="s2">_physical_placements&quot;</span><span class="p">,</span> <span class="n">split</span><span class="o">.</span><span class="n">placements</span><span class="p">)</span>
    <span class="n">set_attr_fn</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">prefix</span><span class="si">}</span><span class="s2">_physical_offsets&quot;</span><span class="p">,</span> <span class="n">split</span><span class="o">.</span><span class="n">offsets</span><span class="p">)</span>

    <span class="n">offsets</span> <span class="o">=</span> <span class="p">[</span><span class="n">split</span><span class="o">.</span><span class="n">offsets</span><span class="p">[</span><span class="n">t</span><span class="p">]</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">feature_table_map</span><span class="p">]</span>
    <span class="n">placements</span> <span class="o">=</span> <span class="p">[</span><span class="n">split</span><span class="o">.</span><span class="n">placements</span><span class="p">[</span><span class="n">t</span><span class="p">]</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">feature_table_map</span><span class="p">]</span>
    <span class="n">persistent_state_fn</span><span class="p">(</span>
        <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">prefix</span><span class="si">}</span><span class="s2">_offsets&quot;</span><span class="p">,</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">offsets</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">current_device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int64</span><span class="p">),</span>
    <span class="p">)</span>
    <span class="n">persistent_state_fn</span><span class="p">(</span>
        <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">prefix</span><span class="si">}</span><span class="s2">_placements&quot;</span><span class="p">,</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">placements</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">current_device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int32</span><span class="p">),</span>
    <span class="p">)</span>
    <span class="k">if</span> <span class="n">split</span><span class="o">.</span><span class="n">dev_size</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">dev_buffer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span>
            <span class="n">split</span><span class="o">.</span><span class="n">dev_size</span><span class="p">,</span>
            <span class="n">device</span><span class="o">=</span><span class="n">current_device</span><span class="p">,</span>
            <span class="c1"># pyre-fixme[6]</span>
            <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">dev_buffer</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">dev_buffer</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">*</span><span class="n">dev_reshape</span><span class="p">)</span> <span class="k">if</span> <span class="n">dev_reshape</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">dev_buffer</span>
        <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># pyre-fixme[6]</span>
        <span class="n">dev_buffer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">current_device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">make_dev_param</span><span class="p">:</span>
        <span class="n">set_attr_fn</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">prefix</span><span class="si">}</span><span class="s2">_dev&quot;</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">dev_buffer</span><span class="p">))</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">persistent_state_fn</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">prefix</span><span class="si">}</span><span class="s2">_dev&quot;</span><span class="p">,</span> <span class="n">dev_buffer</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">split</span><span class="o">.</span><span class="n">host_size</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">dtype</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">uint8</span><span class="p">:</span>
            <span class="n">persistent_state_fn</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">prefix</span><span class="si">}</span><span class="s2">_host&quot;</span><span class="p">,</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span>
                    <span class="n">split</span><span class="o">.</span><span class="n">host_size</span><span class="p">,</span>
                    <span class="n">device</span><span class="o">=</span><span class="n">current_device</span><span class="p">,</span>
                    <span class="c1"># pyre-fixme[6]: Expected `Optional[Type[torch._dtype]]` for</span>
                    <span class="c1">#  3rd param but got `Type[Type[torch._dtype]]`.</span>
                    <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span>
                <span class="p">),</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">set_attr_fn</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">prefix</span><span class="si">}</span><span class="s2">_host&quot;</span><span class="p">,</span>
                <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span>
                    <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span>
                        <span class="n">split</span><span class="o">.</span><span class="n">host_size</span><span class="p">,</span>
                        <span class="n">device</span><span class="o">=</span><span class="n">current_device</span><span class="p">,</span>
                        <span class="c1"># pyre-fixme[6]: Expected `Optional[Type[torch._dtype]]`</span>
                        <span class="c1">#  for 3rd param but got `Type[Type[torch._dtype]]`.</span>
                        <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span>
                    <span class="p">)</span>
                <span class="p">),</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="n">uvm_tensors_log</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">uvm_tensors_log</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">prefix</span><span class="si">}</span><span class="s2">_host&quot;</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">persistent_state_fn</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">prefix</span><span class="si">}</span><span class="s2">_host&quot;</span><span class="p">,</span>
            <span class="c1"># pyre-fixme[6]: For 3rd param expected `dtype` but got `Type[dtype]`.</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">current_device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">),</span>
        <span class="p">)</span>
    <span class="k">if</span> <span class="n">split</span><span class="o">.</span><span class="n">uvm_size</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">assert</span> <span class="ow">not</span> <span class="n">use_cpu</span>
        <span class="k">if</span> <span class="n">enforce_hbm</span><span class="p">:</span>
            <span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Enforce hbm for the cache location&quot;</span><span class="p">)</span>
            <span class="n">persistent_state_fn</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">prefix</span><span class="si">}</span><span class="s2">_uvm&quot;</span><span class="p">,</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span>
                    <span class="n">split</span><span class="o">.</span><span class="n">uvm_size</span><span class="p">,</span>
                    <span class="n">device</span><span class="o">=</span><span class="n">current_device</span><span class="p">,</span>
                    <span class="c1"># pyre-fixme[6]: Expected `Optional[Type[torch._dtype]]` for</span>
                    <span class="c1">#  3rd param but got `Type[Type[torch._dtype]]`.</span>
                    <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span>
                <span class="p">),</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">persistent_state_fn</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">prefix</span><span class="si">}</span><span class="s2">_uvm&quot;</span><span class="p">,</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span>
                    <span class="n">split</span><span class="o">.</span><span class="n">uvm_size</span><span class="p">,</span>
                    <span class="n">out</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">fbgemm</span><span class="o">.</span><span class="n">new_unified_tensor</span><span class="p">(</span>
                        <span class="c1"># pyre-fixme[6]: Expected `Optional[Type[torch._dtype]]`</span>
                        <span class="c1">#  for 3rd param but got `Type[Type[torch._dtype]]`.</span>
                        <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">current_device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">),</span>
                        <span class="p">[</span><span class="n">split</span><span class="o">.</span><span class="n">uvm_size</span><span class="p">],</span>
                        <span class="n">is_host_mapped</span><span class="o">=</span><span class="n">uvm_host_mapped</span><span class="p">,</span>
                    <span class="p">),</span>
                <span class="p">),</span>
            <span class="p">)</span>
            <span class="k">if</span> <span class="n">uvm_tensors_log</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">uvm_tensors_log</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">prefix</span><span class="si">}</span><span class="s2">_uvm&quot;</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">persistent_state_fn</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">prefix</span><span class="si">}</span><span class="s2">_uvm&quot;</span><span class="p">,</span>
            <span class="c1"># pyre-fixme[6]: For 3rd param expected `dtype` but got `Type[dtype]`.</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">current_device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">),</span>
        <span class="p">)</span>


<span class="k">def</span><span class="w"> </span><span class="nf">get_available_compute_device</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="n">ComputeDevice</span><span class="p">:</span>
    <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">():</span>
        <span class="k">return</span> <span class="n">ComputeDevice</span><span class="o">.</span><span class="n">CUDA</span>
    <span class="k">elif</span> <span class="n">torch</span><span class="o">.</span><span class="n">mtia</span><span class="o">.</span><span class="n">is_available</span><span class="p">():</span>
        <span class="k">return</span> <span class="n">ComputeDevice</span><span class="o">.</span><span class="n">MTIA</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">ComputeDevice</span><span class="o">.</span><span class="n">CPU</span>


<span class="c1"># pyre-fixme[13]: Attribute `uvm_cache_stats` is never initialized.</span>
<span class="c1"># pyre-fixme[13]: Attribute `local_uvm_cache_stats` is never initialized.</span>
<div class="viewcode-block" id="SplitTableBatchedEmbeddingBagsCodegen"><a class="viewcode-back" href="../../fbgemm_gpu/python-api/tbe_ops_training.html#fbgemm_gpu.split_table_batched_embeddings_ops_training.SplitTableBatchedEmbeddingBagsCodegen">[docs]</a><span class="k">class</span><span class="w"> </span><span class="nc">SplitTableBatchedEmbeddingBagsCodegen</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Table Batched Embedding (TBE) operator.  Looks up one or more embedding</span>
<span class="sd">    tables. The module is application for training. The backward operator is</span>
<span class="sd">    fused with optimizer. Thus, the embedding tables are updated during</span>
<span class="sd">    backward.</span>

<span class="sd">    Args:</span>
<span class="sd">        embedding_specs (List[Tuple[int, int, EmbeddingLocation, ComputeDevice]]):</span>
<span class="sd">            A list of embedding specifications. Each spec describes a</span>
<span class="sd">            specification of a physical embedding table. Each one is a tuple of</span>
<span class="sd">            number of embedding rows, embedding dimension (must be a multiple of</span>
<span class="sd">            4), table placement (`EmbeddingLocation`), and compute device</span>
<span class="sd">            (`ComputeDevice`).</span>

<span class="sd">            Available `EmbeddingLocation` options are</span>

<span class="sd">            (1) `DEVICE` = placing an embedding table in the GPU global memory</span>
<span class="sd">                (HBM)</span>

<span class="sd">            (2) `MANAGED` = placing an embedding in the unified virtual memory</span>
<span class="sd">                (accessible from both GPU and CPU)</span>

<span class="sd">            (3) `MANAGED_CACHING` = placing an embedding table in the unified</span>
<span class="sd">                virtual memory and using the GPU global memory (HBM) as a cache</span>

<span class="sd">            (4) `HOST` = placing an embedding table in the CPU memory (DRAM)</span>

<span class="sd">            (5) `MTIA` = placing an embedding table in the MTIA memory</span>

<span class="sd">            Available `ComputeDevice` options are</span>

<span class="sd">            (1) `CPU` = performing table lookup on CPU</span>

<span class="sd">            (2) `CUDA` = performing table lookup on GPU</span>

<span class="sd">            (3) `MTIA` = performing table lookup on MTIA</span>

<span class="sd">        feature_table_map (Optional[List[int]] = None): An optional list that</span>
<span class="sd">            specifies feature-table mapping. feature_table_map[i] indicates the</span>
<span class="sd">            physical embedding table that feature i maps to.</span>

<span class="sd">        cache_algorithm (CacheAlgorithm = CacheAlgorithm.LRU): The cache</span>
<span class="sd">            algorithm (used when `EmbeddingLocation` is set to</span>
<span class="sd">            `MANAGED_CACHING`).  Options are</span>

<span class="sd">            (1) `LRU` = least recently used</span>

<span class="sd">            (2) `LFU` = least frequently used</span>

<span class="sd">        cache_load_factor (float = 0.2): A factor used for determining the</span>
<span class="sd">            cache capacity when `EmbeddingLocation.MANAGED_CACHING` is used.</span>
<span class="sd">            The cache capacity is `cache_load_factor` * the total number of</span>
<span class="sd">            rows in all embedding tables</span>

<span class="sd">        cache_sets (int = 0): The number of cache sets (used when</span>
<span class="sd">            `EmbeddingLocation` is set to `MANAGED_CACHING`)</span>

<span class="sd">        cache_reserved_memory (float = 0.0): The amount of memory reserved in</span>
<span class="sd">            HBM for non-cache purpose (used when `EmbeddingLocation` is set to</span>
<span class="sd">            `MANAGED_CACHING`).</span>

<span class="sd">        cache_precision (SparseType = SparseType.FP32): The data type of the</span>
<span class="sd">            cache (used when `EmbeddingLocation` is set to `MANAGED_CACHING`).</span>
<span class="sd">            Options are `SparseType.FP32` and `SparseType.FP16`</span>

<span class="sd">        weights_precision (SparseType = SparseType.FP32): The data type of</span>
<span class="sd">            embedding tables (also known as weights). Options are</span>
<span class="sd">            `SparseType.FP32` and `SparseType.FP16`</span>

<span class="sd">        output_dtype (SparseType = SparseType.FP32): The data type of an output</span>
<span class="sd">            tensor. Options are `SparseType.FP32` and `SparseType.FP16`</span>

<span class="sd">        enforce_hbm (bool = False): If True, place all weights/momentums in HBM</span>
<span class="sd">            when using `EmbeddingLocation.MANAGED_CACHING`</span>

<span class="sd">        optimizer (OptimType = OptimType.EXACT_SGD): An optimizer to use for</span>
<span class="sd">            embedding table update in the backward pass.  Available `OptimType`</span>
<span class="sd">            options are</span>

<span class="sd">            (1) `ADAM` = Adam</span>

<span class="sd">            (2) `EXACT_ADAGRAD` = Adagrad</span>

<span class="sd">            (3) `EXACT_ROWWISE_ADAGRAD` = Rowwise-Aadagrad</span>

<span class="sd">            (4) `EXACT_SGD` = SGD</span>

<span class="sd">            (5) `LAMB` = Lamb</span>

<span class="sd">            (6) `LARS_SGD` = LARS-SGD</span>

<span class="sd">            (7) `PARTIAL_ROWWISE_ADAM` = Partial rowwise-Adam</span>

<span class="sd">            (8) `PARTIAL_ROWWISE_LAMB` = Partial rowwise-Lamb</span>

<span class="sd">            (9) `ENSEMBLE_ROWWISE_ADAGRAD` = Ensemble rowwise-Adagrad</span>

<span class="sd">            (10) `EMAINPLACE_ROWWISE_ADAGRAD` = Ema inplace rowwise-Adagrad</span>

<span class="sd">            (11) `NONE` = Not applying an optimizer update in the backward pass</span>
<span class="sd">                and outputting a sparse weight gradient</span>

<span class="sd">        record_cache_metrics (Optional[RecordCacheMetrics] = None): Record</span>
<span class="sd">            a number of hits, a number of requests, etc if</span>
<span class="sd">            `RecordCacheMetrics.record_cache_miss_counter` is True and record</span>
<span class="sd">            the similar metrics table-wise if</span>
<span class="sd">            `RecordCacheMetrics.record_tablewise_cache_miss is True`</span>

<span class="sd">        gather_uvm_cache_stats (Optional[bool] = False): If True, collect the</span>
<span class="sd">            cache statistics when `EmbeddingLocation` is set to</span>
<span class="sd">            `MANAGED_CACHING`</span>

<span class="sd">        stochastic_rounding (bool = True): If True, apply stochastic rounding</span>
<span class="sd">            for weight type that is not `SparseType.FP32`</span>

<span class="sd">        gradient_clipping (bool = False): If True, apply gradient clipping</span>

<span class="sd">        max_gradient (float = 1.0): The value for gradient clipping</span>

<span class="sd">        max_norm (float = 0.0): The max norm value</span>

<span class="sd">        learning_rate (float = 0.01): The learning rate</span>

<span class="sd">        eps (float = 1.0e-8): The epsilon value used by Adagrad, LAMB, and</span>
<span class="sd">            Adam. Note that default is different from torch.nn.optim.Adagrad</span>
<span class="sd">            default of 1e-10</span>

<span class="sd">        momentum (float = 0.9): Momentum used by LARS-SGD</span>

<span class="sd">        weight_decay (float = 0.0): Weight decay used by LARS-SGD, LAMB, ADAM,</span>
<span class="sd">            and rowwise-Adagrad.</span>

<span class="sd">            (1) EXACT_ADAGRAD, SGD, EXACT_SGD do not support weight decay</span>

<span class="sd">            (2) LAMB, ADAM, PARTIAL_ROWWISE_ADAM, PARTIAL_ROWWISE_LAMB, LARS_SGD</span>
<span class="sd">                support decoupled weight decay</span>

<span class="sd">            (3) EXACT_ROWWISE_ADAGRAD support both L2 and decoupled weight decay</span>
<span class="sd">                (via weight_decay_mode)</span>

<span class="sd">        weight_decay_mode (WeightDecayMode = WeightDecayMode.NONE): Weight decay</span>
<span class="sd">            mode. Options are `WeightDecayMode.NONE`, `WeightDecayMode.L2`,</span>
<span class="sd">            and `WeightDecayMode.DECOUPLE`</span>

<span class="sd">        eta (float = 0.001): The eta value used by LARS-SGD</span>

<span class="sd">        beta1 (float = 0.9): The beta1 value used by LAMB and ADAM</span>

<span class="sd">        beta2 (float = 0.999): The beta2 value used by LAMB and ADAM</span>

<span class="sd">        ensemble_mode (Optional[EnsembleModeDefinition] = None):</span>
<span class="sd">            Used by Ensemble Rowwise Adagrad</span>

<span class="sd">        emainplace_mode (Optional[EmainplaceModeDefinition] = None):</span>
<span class="sd">            Used by EMA in-place Rowwise Adagrad</span>

<span class="sd">        counter_based_regularization (Optional[CounterBasedRegularizationDefinition] = None):</span>
<span class="sd">            Used by Rowwise Adagrad</span>

<span class="sd">        cowclip_regularization (Optional[CowClipDefinition] = None): Used by</span>
<span class="sd">            Rowwise Adagrad</span>

<span class="sd">        pooling_mode (PoolingMode = PoolingMode.SUM): Pooling mode. Available</span>
<span class="sd">            `PoolingMode` options are</span>

<span class="sd">            (1) `SUM` = Sum pooling</span>

<span class="sd">            (2) `MEAN` = Mean pooling</span>

<span class="sd">            (3) `NONE` = No pooling (sequence embedding)</span>

<span class="sd">        device (Optional[Union[str, int, torch.device]] = None): The current</span>
<span class="sd">            device to place tensors on</span>

<span class="sd">        bounds_check_mode (BoundsCheckMode = BoundsCheckMode.WARNING): Input</span>
<span class="sd">            checking mode. Available `BoundsCheckMode` options are</span>

<span class="sd">            (1) `NONE` = skip bounds check</span>

<span class="sd">            (2) `FATAL` = throw an error when encountering an invalid</span>
<span class="sd">                index/offset</span>

<span class="sd">            (3) `WARNING` = print a warning message when encountering an</span>
<span class="sd">                invalid index/offset and fix it (setting an invalid index to</span>
<span class="sd">                zero and adjusting an invalid offset to be within the bound)</span>

<span class="sd">            (4) `IGNORE` = silently fix an invalid index/offset (setting an</span>
<span class="sd">                invalid index to zero and adjusting an invalid offset to be</span>
<span class="sd">                within the bound)</span>

<span class="sd">        uvm_non_rowwise_momentum (bool = False): If True, place non-rowwise</span>
<span class="sd">            momentum on the unified virtual memory</span>

<span class="sd">        use_experimental_tbe (bool = False): If True, use an optimized TBE</span>
<span class="sd">            implementation (TBE v2). Note that this is supported only on NVIDIA</span>
<span class="sd">            GPUs.</span>

<span class="sd">        prefetch_pipeline (bool = False): If True, enable cache prefetch</span>
<span class="sd">            pipeline when using `EmbeddingLocation.MANAGED_CACHING`. Currently</span>
<span class="sd">            only supports the LRU cache policy. If a separate stream is used</span>
<span class="sd">            for prefetch, the optional `forward_stream` arg of prefetch</span>
<span class="sd">            function must be set.</span>

<span class="sd">        stats_reporter_config (Optional[TBEStatsReporterConfig] = None):</span>
<span class="sd">            A config for TBE stats reporter</span>

<span class="sd">        table_names (Optional[List[str]] = None): A list of embedding table</span>
<span class="sd">            names in this TBE</span>

<span class="sd">        optimizer_state_dtypes (Optional[Dict[str, SparseType]] = None): A</span>
<span class="sd">            optimizer state data types dict. Keys are the optimizer state names</span>
<span class="sd">            and values are their corresponding types</span>

<span class="sd">        multipass_prefetch_config (Optional[MultiPassPrefetchConfig] = None):</span>
<span class="sd">            A config for multipass cache prefetching (when</span>
<span class="sd">            `EmbeddingLocation.MANAGED_CACHING` is used)</span>

<span class="sd">        global_weight_decay (Optional[GlobalWeightDecayDefinition] = None):</span>
<span class="sd">            A config for global weight decay</span>

<span class="sd">        uvm_host_mapped (bool = False): If True, allocate every UVM tensor</span>
<span class="sd">            using `malloc` + `cudaHostRegister`. Otherwise use</span>
<span class="sd">            `cudaMallocManaged`</span>

<span class="sd">        extra_optimizer_config Optional[UserEnabledConfigDefinition] = None):</span>
<span class="sd">            An extra config to enable certain modes for optimizer. These modes</span>
<span class="sd">            are not enabled by default.</span>
<span class="sd">            - `use_rowwise_bias_correction` is used in Adam to enable rowwise</span>
<span class="sd">                bias correction computation</span>

<span class="sd">        embedding_table_index_type (torch.dtype = torch.int64): The data type of</span>
<span class="sd">            the embedding table index tensor. Options are `torch.int32` and</span>
<span class="sd">            `torch.int64`</span>

<span class="sd">        embedding_table_offset_type (torch.dtype = torch.int64): The data type of</span>
<span class="sd">            the embedding table offset tensor. Options are `torch.int32` and</span>
<span class="sd">            `torch.int64`</span>

<span class="sd">        embedding_shard_info (Optional[List[Tuple[int, int, int, int]]] = None): the</span>
<span class="sd">            information about shard position and pre-sharded table size. If not set,</span>
<span class="sd">            the table is not sharded.</span>
<span class="sd">            (preshard_table_height, preshard_table_dim, height_offset, dim_offset)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">embedding_specs</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">,</span> <span class="n">EmbeddingLocation</span><span class="p">,</span> <span class="n">ComputeDevice</span><span class="p">]]</span>
    <span class="n">optimizer_args</span><span class="p">:</span> <span class="n">invokers</span><span class="o">.</span><span class="n">lookup_args</span><span class="o">.</span><span class="n">OptimizerArgs</span>
    <span class="n">lxu_cache_locations_list</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span>
    <span class="n">lxu_cache_locations_empty</span><span class="p">:</span> <span class="n">Tensor</span>
    <span class="n">timesteps_prefetched</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span>
    <span class="n">record_cache_metrics</span><span class="p">:</span> <span class="n">RecordCacheMetrics</span>
    <span class="c1"># pyre-fixme[13]: Attribute `uvm_cache_stats` is never initialized.</span>
    <span class="n">uvm_cache_stats</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>
    <span class="c1"># pyre-fixme[13]: Attribute `local_uvm_cache_stats` is never initialized.</span>
    <span class="n">local_uvm_cache_stats</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>
    <span class="n">uuid</span><span class="p">:</span> <span class="nb">str</span>
    <span class="c1"># pyre-fixme[13]: Attribute `last_uvm_cache_print_state` is never initialized.</span>
    <span class="n">last_uvm_cache_print_state</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>
    <span class="n">_vbe_B_offsets</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span>
    <span class="n">_vbe_max_B</span><span class="p">:</span> <span class="nb">int</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>  <span class="c1"># noqa C901</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">embedding_specs</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span>
            <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">,</span> <span class="n">EmbeddingLocation</span><span class="p">,</span> <span class="n">ComputeDevice</span><span class="p">]</span>
        <span class="p">],</span>  <span class="c1"># tuple of (rows, dims, placements, compute_devices)</span>
        <span class="n">feature_table_map</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>  <span class="c1"># [T]</span>
        <span class="n">cache_algorithm</span><span class="p">:</span> <span class="n">CacheAlgorithm</span> <span class="o">=</span> <span class="n">CacheAlgorithm</span><span class="o">.</span><span class="n">LRU</span><span class="p">,</span>
        <span class="n">cache_load_factor</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.2</span><span class="p">,</span>
        <span class="n">cache_sets</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
        <span class="n">cache_reserved_memory</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">,</span>
        <span class="n">cache_precision</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">SparseType</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">weights_precision</span><span class="p">:</span> <span class="n">SparseType</span> <span class="o">=</span> <span class="n">SparseType</span><span class="o">.</span><span class="n">FP32</span><span class="p">,</span>
        <span class="n">output_dtype</span><span class="p">:</span> <span class="n">SparseType</span> <span class="o">=</span> <span class="n">SparseType</span><span class="o">.</span><span class="n">FP32</span><span class="p">,</span>
        <span class="n">enforce_hbm</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">optimizer</span><span class="p">:</span> <span class="n">OptimType</span> <span class="o">=</span> <span class="n">OptimType</span><span class="o">.</span><span class="n">EXACT_SGD</span><span class="p">,</span>
        <span class="n">record_cache_metrics</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">RecordCacheMetrics</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">gather_uvm_cache_stats</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="c1"># General Optimizer args</span>
        <span class="n">stochastic_rounding</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">gradient_clipping</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">max_gradient</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">,</span>
        <span class="n">max_norm</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">,</span>
        <span class="n">learning_rate</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.01</span><span class="p">,</span>
        <span class="n">eps</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.0e-8</span><span class="p">,</span>
        <span class="n">momentum</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.9</span><span class="p">,</span>
        <span class="n">weight_decay</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">,</span>
        <span class="n">weight_decay_mode</span><span class="p">:</span> <span class="n">WeightDecayMode</span> <span class="o">=</span> <span class="n">WeightDecayMode</span><span class="o">.</span><span class="n">NONE</span><span class="p">,</span>
        <span class="n">eta</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.001</span><span class="p">,</span>
        <span class="n">beta1</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.9</span><span class="p">,</span>
        <span class="n">beta2</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.999</span><span class="p">,</span>
        <span class="n">ensemble_mode</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">EnsembleModeDefinition</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">emainplace_mode</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">EmainplaceModeDefinition</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">counter_based_regularization</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span>
            <span class="n">CounterBasedRegularizationDefinition</span>
        <span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">cowclip_regularization</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">CowClipDefinition</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">pooling_mode</span><span class="p">:</span> <span class="n">PoolingMode</span> <span class="o">=</span> <span class="n">PoolingMode</span><span class="o">.</span><span class="n">SUM</span><span class="p">,</span>
        <span class="n">device</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">int</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">bounds_check_mode</span><span class="p">:</span> <span class="n">BoundsCheckMode</span> <span class="o">=</span> <span class="n">BoundsCheckMode</span><span class="o">.</span><span class="n">WARNING</span><span class="p">,</span>
        <span class="n">uvm_non_rowwise_momentum</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">use_experimental_tbe</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">prefetch_pipeline</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">stats_reporter_config</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">TBEStatsReporterConfig</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">table_names</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">optimizer_state_dtypes</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">SparseType</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">multipass_prefetch_config</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">MultiPassPrefetchConfig</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">global_weight_decay</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">GlobalWeightDecayDefinition</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">uvm_host_mapped</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">extra_optimizer_config</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">UserEnabledConfigDefinition</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">tbe_input_multiplexer_config</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">TBEInputMultiplexerConfig</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">embedding_table_index_type</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">int64</span><span class="p">,</span>
        <span class="n">embedding_table_offset_type</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">int64</span><span class="p">,</span>
        <span class="n">embedding_shard_info</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">SplitTableBatchedEmbeddingBagsCodegen</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">uuid</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="n">uuid</span><span class="o">.</span><span class="n">uuid4</span><span class="p">())</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="s2">&quot;SplitTableBatchedEmbeddingBagsCodegen API: V2&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;SplitTableBatchedEmbeddingBagsCodegen Arguments: </span><span class="si">{</span><span class="nb">locals</span><span class="p">()</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">log</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;Feature Gates: </span><span class="si">{</span><span class="p">[(</span><span class="n">feature</span><span class="o">.</span><span class="n">name</span><span class="p">,</span><span class="w"> </span><span class="n">feature</span><span class="o">.</span><span class="n">is_enabled</span><span class="p">())</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">feature</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">FeatureGateName</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">logging_table_name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_table_name_for_logging</span><span class="p">(</span><span class="n">table_names</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pooling_mode</span> <span class="o">=</span> <span class="n">pooling_mode</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">is_nobag</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pooling_mode</span> <span class="o">==</span> <span class="n">PoolingMode</span><span class="o">.</span><span class="n">NONE</span>

        <span class="c1"># If environment variable is set, it overwrites the default bounds check mode.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bounds_check_version</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="p">(</span>
            <span class="mi">2</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_feature_is_enabled</span><span class="p">(</span><span class="n">FeatureGateName</span><span class="o">.</span><span class="n">BOUNDS_CHECK_INDICES_V2</span><span class="p">)</span>
            <span class="k">else</span> <span class="n">get_bounds_check_version_for_platform</span><span class="p">()</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bounds_check_mode_int</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span>
            <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;FBGEMM_TBE_BOUNDS_CHECK_MODE&quot;</span><span class="p">,</span> <span class="n">bounds_check_mode</span><span class="o">.</span><span class="n">value</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="c1"># Check if bounds_check_indices_v2 is enabled via the feature gate</span>
        <span class="n">bounds_check_mode</span> <span class="o">=</span> <span class="n">BoundsCheckMode</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bounds_check_mode_int</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">bounds_check_mode</span><span class="o">.</span><span class="n">name</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;V2_&quot;</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">bounds_check_version</span> <span class="o">=</span> <span class="mi">2</span>
            <span class="k">if</span> <span class="n">bounds_check_mode</span> <span class="o">==</span> <span class="n">BoundsCheckMode</span><span class="o">.</span><span class="n">V2_IGNORE</span><span class="p">:</span>
                <span class="n">bounds_check_mode</span> <span class="o">=</span> <span class="n">BoundsCheckMode</span><span class="o">.</span><span class="n">IGNORE</span>
            <span class="k">elif</span> <span class="n">bounds_check_mode</span> <span class="o">==</span> <span class="n">BoundsCheckMode</span><span class="o">.</span><span class="n">V2_WARNING</span><span class="p">:</span>
                <span class="n">bounds_check_mode</span> <span class="o">=</span> <span class="n">BoundsCheckMode</span><span class="o">.</span><span class="n">WARNING</span>
            <span class="k">elif</span> <span class="n">bounds_check_mode</span> <span class="o">==</span> <span class="n">BoundsCheckMode</span><span class="o">.</span><span class="n">V2_FATAL</span><span class="p">:</span>
                <span class="n">bounds_check_mode</span> <span class="o">=</span> <span class="n">BoundsCheckMode</span><span class="o">.</span><span class="n">FATAL</span>

        <span class="k">if</span> <span class="n">bounds_check_mode</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">(</span>
            <span class="n">BoundsCheckMode</span><span class="o">.</span><span class="n">IGNORE</span><span class="p">,</span>
            <span class="n">BoundsCheckMode</span><span class="o">.</span><span class="n">WARNING</span><span class="p">,</span>
            <span class="n">BoundsCheckMode</span><span class="o">.</span><span class="n">FATAL</span><span class="p">,</span>
            <span class="n">BoundsCheckMode</span><span class="o">.</span><span class="n">NONE</span><span class="p">,</span>
        <span class="p">):</span>
            <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;SplitTableBatchedEmbeddingBagsCodegen bounds_check_mode=</span><span class="si">{</span><span class="n">bounds_check_mode</span><span class="si">}</span><span class="s2"> is not supported&quot;</span>
            <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">bounds_check_mode_int</span> <span class="o">=</span> <span class="n">bounds_check_mode</span><span class="o">.</span><span class="n">value</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">log</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;SplitTableBatchedEmbeddingBagsCodegen bounds_check_mode=</span><span class="si">{</span><span class="n">bounds_check_mode</span><span class="si">}</span><span class="s2"> bounds_check_version=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">bounds_check_version</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">weights_precision</span> <span class="o">=</span> <span class="n">weights_precision</span>

        <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="ow">and</span> <span class="n">torch</span><span class="o">.</span><span class="n">version</span><span class="o">.</span><span class="n">hip</span><span class="p">:</span>
            <span class="c1"># NOTE: It was discovered that FP16 cache precision caused a 500x</span>
            <span class="c1"># slowdown in performance of split_embedding_nobag_backward_codegen_rowwise_adagrad_unweighted_kernel_warp_per_row_1</span>
            <span class="c1"># kernel on ROCm, so to work around this, we fix cache precision to</span>
            <span class="c1"># be FP32 always for the ROCm environment case.</span>
            <span class="c1">#</span>
            <span class="c1"># See:</span>
            <span class="c1">#   https://fb.workplace.com/groups/fbgemmusers/permalink/9438488366231860/</span>
            <span class="n">cache_precision</span> <span class="o">=</span> <span class="n">SparseType</span><span class="o">.</span><span class="n">FP32</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="s2">&quot;Override cache_precision=SparseType.FP32 on ROCm&quot;</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># NOTE: The changes from D65865527 are retained here until we can</span>
            <span class="c1"># test that the the hack also works for non-ROCm environments.</span>
            <span class="n">cache_precision</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">weights_precision</span> <span class="k">if</span> <span class="n">cache_precision</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">cache_precision</span>
            <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">output_dtype</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="n">output_dtype</span><span class="o">.</span><span class="n">as_int</span><span class="p">()</span>
        <span class="k">assert</span> <span class="p">(</span>
            <span class="ow">not</span> <span class="n">prefetch_pipeline</span> <span class="ow">or</span> <span class="n">cache_algorithm</span> <span class="o">==</span> <span class="n">CacheAlgorithm</span><span class="o">.</span><span class="n">LRU</span>
        <span class="p">),</span> <span class="s2">&quot;Only LRU cache policy supports prefetch_pipeline.&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">prefetch_pipeline</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="n">prefetch_pipeline</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lock_cache_line</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">prefetch_pipeline</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">use_uniq_cache_locations_bwd</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">prefetch_pipeline</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">multipass_prefetch_config</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">MultiPassPrefetchConfig</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">multipass_prefetch_config</span>
        <span class="p">)</span>

        <span class="k">if</span> <span class="n">record_cache_metrics</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">record_cache_metrics</span> <span class="o">=</span> <span class="n">record_cache_metrics</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">record_cache_metrics</span> <span class="o">=</span> <span class="n">RecordCacheMetrics</span><span class="p">(</span><span class="kc">False</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">multipass_prefetch_config</span><span class="p">:</span>
            <span class="k">assert</span> <span class="p">(</span>
                <span class="n">prefetch_pipeline</span>
            <span class="p">),</span> <span class="s2">&quot;Multipass prefetch makes no sense in non-prefetch mode.&quot;</span>
            <span class="k">assert</span> <span class="p">(</span>
                <span class="n">cache_algorithm</span> <span class="o">==</span> <span class="n">CacheAlgorithm</span><span class="o">.</span><span class="n">LRU</span>
            <span class="p">),</span> <span class="s2">&quot;Multipass prefetch is only supported in LRU cache.&quot;</span>
            <span class="k">assert</span> <span class="p">(</span>
                <span class="n">multipass_prefetch_config</span><span class="o">.</span><span class="n">num_passes</span> <span class="o">&gt;</span> <span class="mi">0</span>
            <span class="p">),</span> <span class="sa">f</span><span class="s2">&quot;num_passes must be positive, get </span><span class="si">{</span><span class="n">multipass_prefetch_config</span><span class="o">.</span><span class="n">num_passes</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="k">assert</span> <span class="p">(</span>
                <span class="n">multipass_prefetch_config</span><span class="o">.</span><span class="n">min_splitable_pass_size</span> <span class="o">&gt;</span> <span class="mi">0</span>
            <span class="p">),</span> <span class="sa">f</span><span class="s2">&quot;min_splitable_pass_size must be positive, get </span><span class="si">{</span><span class="n">multipass_prefetch_config</span><span class="o">.</span><span class="n">min_splitable_pass_size</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="k">assert</span> <span class="p">(</span>
                <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">record_cache_metrics</span><span class="o">.</span><span class="n">record_cache_miss_counter</span>
                <span class="ow">and</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">record_cache_metrics</span><span class="o">.</span><span class="n">record_tablewise_cache_miss</span>
            <span class="p">),</span> <span class="s2">&quot;Unique cache miss counters are not accurate in multipass prefetch and therefore not supported&quot;</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">embedding_specs</span> <span class="o">=</span> <span class="n">embedding_specs</span>
        <span class="p">(</span><span class="n">rows</span><span class="p">,</span> <span class="n">dims</span><span class="p">,</span> <span class="n">locations</span><span class="p">,</span> <span class="n">compute_devices</span><span class="p">)</span> <span class="o">=</span> <span class="nb">zip</span><span class="p">(</span><span class="o">*</span><span class="n">embedding_specs</span><span class="p">)</span>
        <span class="n">T_</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">embedding_specs</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dims</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="n">dims</span>
        <span class="k">assert</span> <span class="n">T_</span> <span class="o">&gt;</span> <span class="mi">0</span>
        <span class="c1"># mixed D is not supported by no bag kernels</span>
        <span class="n">mixed_D</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="n">D</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dims</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">dims</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">d</span> <span class="o">!=</span> <span class="n">D</span><span class="p">:</span>
                <span class="n">mixed_D</span> <span class="o">=</span> <span class="kc">True</span>
                <span class="k">break</span>
        <span class="k">if</span> <span class="n">mixed_D</span><span class="p">:</span>
            <span class="k">assert</span> <span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">pooling_mode</span> <span class="o">!=</span> <span class="n">PoolingMode</span><span class="o">.</span><span class="n">NONE</span>
            <span class="p">),</span> <span class="s2">&quot;Mixed dimension tables only supported for pooling tables.&quot;</span>

        <span class="k">assert</span> <span class="nb">all</span><span class="p">(</span>
            <span class="n">cd</span> <span class="o">==</span> <span class="n">compute_devices</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">cd</span> <span class="ow">in</span> <span class="n">compute_devices</span>
        <span class="p">),</span> <span class="s2">&quot;Heterogenous compute_devices are NOT supported!&quot;</span>
        <span class="c1"># Split TBE has different function schemas for CUDA and CPU.</span>
        <span class="c1"># For MTIA device type, it uses the CPU one.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">use_cpu</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">compute_devices</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="n">ComputeDevice</span><span class="o">.</span><span class="n">CPU</span>
            <span class="ow">or</span> <span class="n">compute_devices</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="n">ComputeDevice</span><span class="o">.</span><span class="n">MTIA</span>
        <span class="p">)</span>

        <span class="k">assert</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_cpu</span> <span class="ow">or</span> <span class="nb">all</span><span class="p">(</span>
            <span class="n">loc</span> <span class="o">==</span> <span class="n">EmbeddingLocation</span><span class="o">.</span><span class="n">HOST</span> <span class="k">for</span> <span class="n">loc</span> <span class="ow">in</span> <span class="n">locations</span>
        <span class="p">),</span> <span class="s2">&quot;ComputeDevice.CPU is only for EmbeddingLocation.HOST!&quot;</span>
        <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_cpu</span> <span class="ow">or</span> <span class="nb">all</span><span class="p">(</span>
            <span class="n">loc</span> <span class="o">!=</span> <span class="n">EmbeddingLocation</span><span class="o">.</span><span class="n">HOST</span> <span class="k">for</span> <span class="n">loc</span> <span class="ow">in</span> <span class="n">locations</span>
        <span class="p">),</span> <span class="s2">&quot;EmbeddingLocation.HOST doesn&#39;t work for CUDA device!&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_cpu</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">pooling_mode</span> <span class="o">==</span> <span class="n">PoolingMode</span><span class="o">.</span><span class="n">NONE</span><span class="p">:</span>
            <span class="k">assert</span> <span class="n">output_dtype</span> <span class="ow">in</span> <span class="p">[</span>
                <span class="n">SparseType</span><span class="o">.</span><span class="n">FP32</span><span class="p">,</span>
                <span class="n">SparseType</span><span class="o">.</span><span class="n">FP16</span><span class="p">,</span>
                <span class="n">SparseType</span><span class="o">.</span><span class="n">BF16</span><span class="p">,</span>
            <span class="p">],</span> <span class="s2">&quot;Fused pooled embedding quantization only supported for cuda.&quot;</span>

        <span class="k">if</span> <span class="n">optimizer</span> <span class="o">==</span> <span class="n">OptimType</span><span class="o">.</span><span class="n">NONE</span><span class="p">:</span>
            <span class="k">assert</span> <span class="nb">all</span><span class="p">(</span>
                <span class="n">loc</span> <span class="o">==</span> <span class="n">EmbeddingLocation</span><span class="o">.</span><span class="n">DEVICE</span> <span class="k">for</span> <span class="n">loc</span> <span class="ow">in</span> <span class="n">locations</span>
            <span class="p">),</span> <span class="s2">&quot;OptimType.NONE supports only EmbeddingLocation.DEVICE&quot;</span>
            <span class="k">assert</span> <span class="nb">all</span><span class="p">(</span>
                <span class="n">cd</span> <span class="o">==</span> <span class="n">ComputeDevice</span><span class="o">.</span><span class="n">CUDA</span> <span class="k">for</span> <span class="n">cd</span> <span class="ow">in</span> <span class="n">compute_devices</span>
            <span class="p">),</span> <span class="s2">&quot;OptimType.NONE supports only ComputeDevice.CUDA&quot;</span>
            <span class="k">assert</span> <span class="p">(</span>
                <span class="ow">not</span> <span class="n">mixed_D</span>
            <span class="p">),</span> <span class="s2">&quot;OptimType.NONE does not support mixed embedding dimension&quot;</span>

        <span class="k">if</span> <span class="n">device</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">current_device</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cpu&quot;</span><span class="p">)</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_cpu</span>
                <span class="k">else</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">current_device</span><span class="p">())</span>
            <span class="p">)</span>
        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">device</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">current_device</span> <span class="o">=</span> <span class="n">device</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">current_device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

        <span class="c1"># add placeholder require_grad param tensor to enable autograd with int8 weights</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">placeholder_autograd_tensor</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">current_device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">)</span>
        <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">gather_uvm_cache_stats</span> <span class="o">=</span> <span class="n">gather_uvm_cache_stats</span>
        <span class="c1"># Define the size of uvm cache stats as class variable</span>
        <span class="c1"># to make it work with torch jit script.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">uvm_cache_stats_size</span> <span class="o">=</span> <span class="mi">6</span>
        <span class="c1"># 0: N_calls, 1: N_requested_indices, 2: N_unique_indices, 3: N_unique_misses,</span>
        <span class="c1"># 4: N_conflict_unique_misses, 5: N_conflict_misses</span>

        <span class="c1"># Reporter to collect runtime performance stats bottom-up. Reporter may</span>
        <span class="c1"># do aggregation across TBEs and publish results per training batch.</span>
        <span class="c1"># Example of stats include UVM cache hit rate, table I/O size, etc.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">stats_reporter</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">TBEStatsReporter</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">stats_reporter_config</span><span class="o">.</span><span class="n">create_reporter</span><span class="p">()</span> <span class="k">if</span> <span class="n">stats_reporter_config</span> <span class="k">else</span> <span class="kc">None</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_uvm_tensors_log</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">bwd_wait_prefetch_timer</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">AsyncSeriesTimer</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">prefetch_duration_timer</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">AsyncSeriesTimer</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">stats_reporter</span><span class="p">:</span>
            <span class="c1"># When stats_reporter is present, we set up async series timer to</span>
            <span class="c1"># measure the GPU time per tracked event accordingly. Each of them</span>
            <span class="c1"># is attached to custom callback report function to report collected</span>
            <span class="c1"># duration with the corresponding event name.</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">bwd_wait_prefetch_timer</span> <span class="o">=</span> <span class="n">AsyncSeriesTimer</span><span class="p">(</span>
                <span class="n">functools</span><span class="o">.</span><span class="n">partial</span><span class="p">(</span>
                    <span class="n">SplitTableBatchedEmbeddingBagsCodegen</span><span class="o">.</span><span class="n">_report_duration</span><span class="p">,</span>
                    <span class="bp">self</span><span class="p">,</span>
                    <span class="n">event_name</span><span class="o">=</span><span class="s2">&quot;bwd_wait_for_prefetch&quot;</span><span class="p">,</span>
                <span class="p">)</span>
            <span class="p">)</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">prefetch_duration_timer</span> <span class="o">=</span> <span class="n">AsyncSeriesTimer</span><span class="p">(</span>
                <span class="n">functools</span><span class="o">.</span><span class="n">partial</span><span class="p">(</span>
                    <span class="n">SplitTableBatchedEmbeddingBagsCodegen</span><span class="o">.</span><span class="n">_report_duration</span><span class="p">,</span>
                    <span class="bp">self</span><span class="p">,</span>
                    <span class="n">event_name</span><span class="o">=</span><span class="s2">&quot;total_prefetch_duration&quot;</span><span class="p">,</span>
                <span class="p">)</span>
            <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">int8_emb_row_dim_offset</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="n">INT8_EMB_ROW_DIM_OFFSET</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">feature_table_map</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">feature_table_map</span> <span class="k">if</span> <span class="n">feature_table_map</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">T_</span><span class="p">))</span>
        <span class="p">)</span>

        <span class="k">if</span> <span class="n">embedding_shard_info</span><span class="p">:</span>
            <span class="p">(</span><span class="n">full_table_heights</span><span class="p">,</span> <span class="n">full_table_dims</span><span class="p">,</span> <span class="n">row_offset</span><span class="p">,</span> <span class="n">col_offset</span><span class="p">)</span> <span class="o">=</span> <span class="nb">zip</span><span class="p">(</span>
                <span class="o">*</span><span class="n">embedding_shard_info</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># Just assume the table is unsharded</span>
            <span class="n">full_table_heights</span> <span class="o">=</span> <span class="n">rows</span>
            <span class="n">full_table_dims</span> <span class="o">=</span> <span class="n">dims</span>
            <span class="n">row_offset</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">rows</span><span class="p">)</span>
            <span class="n">col_offset</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">rows</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tbe_input_multiplexer</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">TBEInputMultiplexer</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">tbe_input_multiplexer_config</span><span class="o">.</span><span class="n">create_tbe_input_multiplexer</span><span class="p">(</span>
                <span class="n">tbe_info</span><span class="o">=</span><span class="n">TBEInfo</span><span class="p">(</span>
                    <span class="n">table_names</span><span class="o">=</span><span class="p">(</span>
                        <span class="n">table_names</span>
                        <span class="k">if</span> <span class="n">table_names</span>
                        <span class="k">else</span> <span class="p">[</span><span class="sa">f</span><span class="s2">&quot;table-</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">&quot;</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">embedding_specs</span><span class="p">))]</span>
                    <span class="p">),</span>
                    <span class="n">table_heights</span><span class="o">=</span><span class="n">rows</span><span class="p">,</span>
                    <span class="n">tbe_uuid</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">uuid</span><span class="p">,</span>
                    <span class="n">feature_table_map</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">feature_table_map</span><span class="p">,</span>
                    <span class="n">table_dims</span><span class="o">=</span><span class="n">dims</span><span class="p">,</span>
                    <span class="n">full_table_heights</span><span class="o">=</span><span class="n">full_table_heights</span><span class="p">,</span>
                    <span class="n">full_table_dims</span><span class="o">=</span><span class="n">full_table_dims</span><span class="p">,</span>
                    <span class="n">row_offset</span><span class="o">=</span><span class="n">row_offset</span><span class="p">,</span>
                    <span class="n">col_offset</span><span class="o">=</span><span class="n">col_offset</span><span class="p">,</span>
                <span class="p">)</span>
            <span class="p">)</span>
            <span class="k">if</span> <span class="n">tbe_input_multiplexer_config</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
            <span class="k">else</span> <span class="kc">None</span>
        <span class="p">)</span>
        <span class="n">T</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">feature_table_map</span><span class="p">)</span>
        <span class="k">assert</span> <span class="n">T_</span> <span class="o">&lt;=</span> <span class="n">T</span>
        <span class="n">table_has_feature</span> <span class="o">=</span> <span class="p">[</span><span class="kc">False</span><span class="p">]</span> <span class="o">*</span> <span class="n">T_</span>
        <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">feature_table_map</span><span class="p">:</span>
            <span class="n">table_has_feature</span><span class="p">[</span><span class="n">t</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="k">assert</span> <span class="nb">all</span><span class="p">(</span><span class="n">table_has_feature</span><span class="p">),</span> <span class="s2">&quot;Each table must have at least one feature!&quot;</span>

        <span class="n">feature_dims</span> <span class="o">=</span> <span class="p">[</span><span class="n">dims</span><span class="p">[</span><span class="n">t</span><span class="p">]</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">feature_table_map</span><span class="p">]</span>
        <span class="n">D_offsets</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="nb">list</span><span class="p">(</span><span class="n">accumulate</span><span class="p">(</span><span class="n">feature_dims</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">total_D</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="n">D_offsets</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_D</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">dims</span><span class="p">)</span>
        <span class="n">cached_dims</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">embedding_spec</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
            <span class="k">for</span> <span class="n">embedding_spec</span> <span class="ow">in</span> <span class="n">embedding_specs</span>
            <span class="k">if</span> <span class="n">embedding_spec</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">==</span> <span class="n">EmbeddingLocation</span><span class="o">.</span><span class="n">MANAGED_CACHING</span>
        <span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_D_cache</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">cached_dims</span><span class="p">)</span> <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">cached_dims</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="k">else</span> <span class="mi">0</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span>
            <span class="s2">&quot;D_offsets&quot;</span><span class="p">,</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">D_offsets</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">current_device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int32</span><span class="p">),</span>
        <span class="p">)</span>
        <span class="n">hash_size_cumsum</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="nb">list</span><span class="p">(</span><span class="n">accumulate</span><span class="p">(</span><span class="n">rows</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">total_hash_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">hash_size_cumsum</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">total_hash_size</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">total_hash_size_bits</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">total_hash_size_bits</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">log2</span><span class="p">(</span><span class="nb">float</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">total_hash_size</span><span class="p">))</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
        <span class="c1"># The last element is to easily access # of rows of each table by</span>
        <span class="c1"># hash_size_cumsum[t + 1] - hash_size_cumsum[t]</span>
        <span class="n">hash_size_cumsum</span> <span class="o">=</span> <span class="p">[</span><span class="n">hash_size_cumsum</span><span class="p">[</span><span class="n">t</span><span class="p">]</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">feature_table_map</span><span class="p">]</span> <span class="o">+</span> <span class="p">[</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">total_hash_size</span>
        <span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span>
            <span class="s2">&quot;hash_size_cumsum&quot;</span><span class="p">,</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span>
                <span class="n">hash_size_cumsum</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">current_device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int64</span>
            <span class="p">),</span>
        <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span>
            <span class="s2">&quot;rows_per_table&quot;</span><span class="p">,</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span>
                <span class="p">[</span><span class="n">rows</span><span class="p">[</span><span class="n">t</span><span class="p">]</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">feature_table_map</span><span class="p">],</span>
                <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">current_device</span><span class="p">,</span>
                <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int64</span><span class="p">,</span>
            <span class="p">),</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span>
            <span class="s2">&quot;bounds_check_warning&quot;</span><span class="p">,</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">0</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">current_device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int64</span><span class="p">),</span>
        <span class="p">)</span>
        <span class="c1"># Required for VBE</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span>
            <span class="s2">&quot;feature_dims&quot;</span><span class="p">,</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">feature_dims</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s2">&quot;cpu&quot;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int64</span><span class="p">),</span>
        <span class="p">)</span>
        <span class="p">(</span><span class="n">_info_B_num_bits</span><span class="p">,</span> <span class="n">_info_B_mask</span><span class="p">)</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">fbgemm</span><span class="o">.</span><span class="n">get_infos_metadata</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">D_offsets</span><span class="p">,</span>  <span class="c1"># unused tensor</span>
            <span class="mi">1</span><span class="p">,</span>  <span class="c1"># max_B</span>
            <span class="n">T</span><span class="p">,</span>  <span class="c1"># T</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">info_B_num_bits</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="n">_info_B_num_bits</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">info_B_mask</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="n">_info_B_mask</span>

        <span class="c1"># A flag for indicating whether all embedding tables are placed in the</span>
        <span class="c1"># same locations</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">use_homogeneous_placements</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="nb">all</span><span class="p">(</span>
            <span class="n">loc</span> <span class="o">==</span> <span class="n">locations</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">loc</span> <span class="ow">in</span> <span class="n">locations</span>
        <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">uvm_host_mapped</span> <span class="o">=</span> <span class="n">uvm_host_mapped</span>

        <span class="n">weight_split</span> <span class="o">=</span> <span class="n">construct_split_state</span><span class="p">(</span>
            <span class="n">embedding_specs</span><span class="p">,</span>
            <span class="n">rowwise</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="n">cacheable</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="n">precision</span><span class="o">=</span><span class="n">weights_precision</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">table_embedding_dtype</span> <span class="o">=</span> <span class="n">weights_precision</span><span class="o">.</span><span class="n">as_dtype</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_apply_split</span><span class="p">(</span>
            <span class="n">weight_split</span><span class="p">,</span>
            <span class="n">prefix</span><span class="o">=</span><span class="s2">&quot;weights&quot;</span><span class="p">,</span>
            <span class="c1"># pyre-fixme[6]: For 3rd param expected `Type[Type[_dtype]]` but got</span>
            <span class="c1">#  `Type[_dtype]`.</span>
            <span class="n">dtype</span><span class="o">=</span><span class="n">table_embedding_dtype</span><span class="p">,</span>
            <span class="n">enforce_hbm</span><span class="o">=</span><span class="n">enforce_hbm</span><span class="p">,</span>
            <span class="n">make_dev_param</span><span class="o">=</span><span class="n">optimizer</span> <span class="o">==</span> <span class="n">OptimType</span><span class="o">.</span><span class="n">NONE</span><span class="p">,</span>
            <span class="n">dev_reshape</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_D</span><span class="p">)</span> <span class="k">if</span> <span class="n">optimizer</span> <span class="o">==</span> <span class="n">OptimType</span><span class="o">.</span><span class="n">NONE</span> <span class="k">else</span> <span class="kc">None</span><span class="p">,</span>
            <span class="n">uvm_host_mapped</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">uvm_host_mapped</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="k">assert</span> <span class="n">optimizer</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">(</span>
            <span class="n">OptimType</span><span class="o">.</span><span class="n">SGD</span><span class="p">,</span>
            <span class="n">OptimType</span><span class="o">.</span><span class="n">ROWWISE_ADAGRAD</span><span class="p">,</span>
        <span class="p">),</span> <span class="sa">f</span><span class="s2">&quot;Optimizer </span><span class="si">{</span><span class="n">optimizer</span><span class="si">}</span><span class="s2"> is deprecated in the CPU + GPU modes.&quot;</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_cpu</span><span class="p">:</span>
            <span class="c1"># Construct optimizer states</span>
            <span class="k">assert</span> <span class="n">optimizer</span> <span class="ow">in</span> <span class="p">(</span>
                <span class="n">OptimType</span><span class="o">.</span><span class="n">EXACT_ADAGRAD</span><span class="p">,</span>
                <span class="n">OptimType</span><span class="o">.</span><span class="n">EXACT_ROWWISE_ADAGRAD</span><span class="p">,</span>
                <span class="n">OptimType</span><span class="o">.</span><span class="n">EXACT_SGD</span><span class="p">,</span>
                <span class="n">OptimType</span><span class="o">.</span><span class="n">EMAINPLACE_ROWWISE_ADAGRAD</span><span class="p">,</span>
            <span class="p">),</span> <span class="sa">f</span><span class="s2">&quot;Optimizer </span><span class="si">{</span><span class="n">optimizer</span><span class="si">}</span><span class="s2"> is not supported in CPU mode.&quot;</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">assert</span> <span class="n">optimizer</span> <span class="ow">in</span> <span class="p">(</span>
                <span class="n">OptimType</span><span class="o">.</span><span class="n">ADAM</span><span class="p">,</span>
                <span class="n">OptimType</span><span class="o">.</span><span class="n">EXACT_ADAGRAD</span><span class="p">,</span>
                <span class="n">OptimType</span><span class="o">.</span><span class="n">EXACT_ROWWISE_ADAGRAD</span><span class="p">,</span>
                <span class="n">OptimType</span><span class="o">.</span><span class="n">EXACT_SGD</span><span class="p">,</span>
                <span class="n">OptimType</span><span class="o">.</span><span class="n">LAMB</span><span class="p">,</span>
                <span class="n">OptimType</span><span class="o">.</span><span class="n">LARS_SGD</span><span class="p">,</span>
                <span class="n">OptimType</span><span class="o">.</span><span class="n">PARTIAL_ROWWISE_ADAM</span><span class="p">,</span>
                <span class="n">OptimType</span><span class="o">.</span><span class="n">PARTIAL_ROWWISE_LAMB</span><span class="p">,</span>
                <span class="n">OptimType</span><span class="o">.</span><span class="n">ENSEMBLE_ROWWISE_ADAGRAD</span><span class="p">,</span>
                <span class="n">OptimType</span><span class="o">.</span><span class="n">EMAINPLACE_ROWWISE_ADAGRAD</span><span class="p">,</span>
                <span class="n">OptimType</span><span class="o">.</span><span class="n">NONE</span><span class="p">,</span>
            <span class="p">),</span> <span class="sa">f</span><span class="s2">&quot;Optimizer </span><span class="si">{</span><span class="n">optimizer</span><span class="si">}</span><span class="s2"> is not supported.&quot;</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">stochastic_rounding</span> <span class="o">=</span> <span class="n">stochastic_rounding</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">optimizer</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">weight_decay_mode</span> <span class="o">=</span> <span class="n">weight_decay_mode</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">weight_decay_mode</span> <span class="o">==</span> <span class="n">WeightDecayMode</span><span class="o">.</span><span class="n">COUNTER</span><span class="p">)</span> <span class="o">!=</span> <span class="p">(</span>
            <span class="n">counter_based_regularization</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
        <span class="p">):</span>
            <span class="k">raise</span> <span class="ne">AssertionError</span><span class="p">(</span>
                <span class="s2">&quot;Need to set weight_decay_mode=WeightDecayMode.COUNTER together with valid counter_based_regularization&quot;</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">weight_decay_mode</span> <span class="o">==</span> <span class="n">WeightDecayMode</span><span class="o">.</span><span class="n">COWCLIP</span><span class="p">)</span> <span class="o">!=</span> <span class="p">(</span>
            <span class="n">cowclip_regularization</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
        <span class="p">):</span>
            <span class="k">raise</span> <span class="ne">AssertionError</span><span class="p">(</span>
                <span class="s2">&quot;Need to set weight_decay_mode=WeightDecayMode.COWCLIP together with valid cowclip_regularization&quot;</span>
            <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_used_rowwise_adagrad_with_counter</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">optimizer</span> <span class="o">==</span> <span class="n">OptimType</span><span class="o">.</span><span class="n">EXACT_ROWWISE_ADAGRAD</span>
            <span class="ow">and</span> <span class="p">(</span>
                <span class="n">weight_decay_mode</span> <span class="ow">in</span> <span class="p">(</span><span class="n">WeightDecayMode</span><span class="o">.</span><span class="n">COUNTER</span><span class="p">,</span> <span class="n">WeightDecayMode</span><span class="o">.</span><span class="n">COWCLIP</span><span class="p">)</span>
            <span class="p">)</span>
        <span class="p">)</span>

        <span class="k">if</span> <span class="n">weight_decay_mode</span> <span class="o">==</span> <span class="n">WeightDecayMode</span><span class="o">.</span><span class="n">DECOUPLE_GLOBAL</span> <span class="ow">and</span> <span class="p">(</span>
            <span class="ow">not</span> <span class="n">optimizer</span> <span class="o">==</span> <span class="n">OptimType</span><span class="o">.</span><span class="n">EXACT_ROWWISE_ADAGRAD</span>
            <span class="ow">or</span> <span class="n">global_weight_decay</span> <span class="ow">is</span> <span class="kc">None</span>
        <span class="p">):</span>
            <span class="k">raise</span> <span class="ne">AssertionError</span><span class="p">(</span>
<span class="w">                </span><span class="sd">&quot;&quot;&quot;weight_decay_mode=WeightDecayMode.DECOUPLE_GLOBAL is supported for</span>
<span class="sd">                optimizer=OptimType.EXACT_ROWWISE_ADAGRAD and global_weight_decay cannot be None.</span>
<span class="sd">                &quot;&quot;&quot;</span>
            <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_used_rowwise_adagrad_with_global_weight_decay</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">optimizer</span> <span class="o">==</span> <span class="n">OptimType</span><span class="o">.</span><span class="n">EXACT_ROWWISE_ADAGRAD</span>
            <span class="ow">and</span> <span class="p">(</span><span class="n">weight_decay_mode</span> <span class="o">==</span> <span class="n">WeightDecayMode</span><span class="o">.</span><span class="n">DECOUPLE_GLOBAL</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">log</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;Using global weight decay = </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">_used_rowwise_adagrad_with_global_weight_decay</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="p">)</span>
        <span class="c1"># Declare GWD params here to avoid torch.jit.script error</span>
        <span class="k">if</span> <span class="n">global_weight_decay</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">global_weight_decay</span> <span class="o">=</span> <span class="n">GlobalWeightDecayDefinition</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">gwd_start_iter</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="n">global_weight_decay</span><span class="o">.</span><span class="n">start_iter</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gwd_lower_bound</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="n">global_weight_decay</span><span class="o">.</span><span class="n">lower_bound</span>

        <span class="k">if</span> <span class="n">ensemble_mode</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">ensemble_mode</span> <span class="o">=</span> <span class="n">EnsembleModeDefinition</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_ensemble_mode</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="p">{</span>
            <span class="n">key</span><span class="p">:</span> <span class="nb">float</span><span class="p">(</span><span class="n">fval</span><span class="p">)</span> <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">fval</span> <span class="ow">in</span> <span class="n">ensemble_mode</span><span class="o">.</span><span class="vm">__dict__</span><span class="o">.</span><span class="n">items</span><span class="p">()</span>
        <span class="p">}</span>

        <span class="k">if</span> <span class="n">emainplace_mode</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">emainplace_mode</span> <span class="o">=</span> <span class="n">EmainplaceModeDefinition</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_emainplace_mode</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="p">{</span>
            <span class="n">key</span><span class="p">:</span> <span class="nb">float</span><span class="p">(</span><span class="n">fval</span><span class="p">)</span> <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">fval</span> <span class="ow">in</span> <span class="n">emainplace_mode</span><span class="o">.</span><span class="vm">__dict__</span><span class="o">.</span><span class="n">items</span><span class="p">()</span>
        <span class="p">}</span>

        <span class="k">if</span> <span class="n">counter_based_regularization</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">counter_based_regularization</span> <span class="o">=</span> <span class="n">CounterBasedRegularizationDefinition</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">cowclip_regularization</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">cowclip_regularization</span> <span class="o">=</span> <span class="n">CowClipDefinition</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_max_counter_update_freq</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
        <span class="c1"># Extract parameters from CounterBasedRegularizationDefinition or CowClipDefinition</span>
        <span class="c1"># which are passed as entries for OptimizerArgs</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_used_rowwise_adagrad_with_counter</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight_decay_mode</span> <span class="o">==</span> <span class="n">WeightDecayMode</span><span class="o">.</span><span class="n">COUNTER</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_max_counter_update_freq</span> <span class="o">=</span> <span class="p">(</span>
                    <span class="n">counter_based_regularization</span><span class="o">.</span><span class="n">max_counter_update_freq</span>
                <span class="p">)</span>
                <span class="n">opt_arg_weight_decay_mode</span> <span class="o">=</span> <span class="p">(</span>
                    <span class="n">counter_based_regularization</span><span class="o">.</span><span class="n">counter_weight_decay_mode</span>
                <span class="p">)</span>
                <span class="n">counter_halflife</span> <span class="o">=</span> <span class="n">counter_based_regularization</span><span class="o">.</span><span class="n">counter_halflife</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">opt_arg_weight_decay_mode</span> <span class="o">=</span> <span class="p">(</span>
                    <span class="n">cowclip_regularization</span><span class="o">.</span><span class="n">counter_weight_decay_mode</span>
                <span class="p">)</span>
                <span class="n">counter_halflife</span> <span class="o">=</span> <span class="n">cowclip_regularization</span><span class="o">.</span><span class="n">counter_halflife</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">opt_arg_weight_decay_mode</span> <span class="o">=</span> <span class="n">weight_decay_mode</span>
            <span class="c1"># Default: -1, no decay applied, as a placeholder for OptimizerArgs</span>
            <span class="c1"># which should not be effective when CounterBasedRegularizationDefinition</span>
            <span class="c1"># and CowClipDefinition are not used</span>
            <span class="n">counter_halflife</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>

        <span class="k">if</span> <span class="n">extra_optimizer_config</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">extra_optimizer_config</span> <span class="o">=</span> <span class="n">UserEnabledConfigDefinition</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">use_rowwise_bias_correction</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">extra_optimizer_config</span><span class="o">.</span><span class="n">use_rowwise_bias_correction</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">use_writeback_bwd_prehook</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">extra_optimizer_config</span><span class="o">.</span><span class="n">use_writeback_bwd_prehook</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;self.extra_optimizer_config is </span><span class="si">{</span><span class="n">extra_optimizer_config</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_rowwise_bias_correction</span> <span class="ow">and</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span> <span class="o">==</span> <span class="n">OptimType</span><span class="o">.</span><span class="n">ADAM</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">AssertionError</span><span class="p">(</span>
                <span class="s2">&quot;`use_rowwise_bias_correction` is only supported for OptimType.ADAM&quot;</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_writeback_bwd_prehook</span> <span class="ow">and</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span> <span class="o">==</span> <span class="n">OptimType</span><span class="o">.</span><span class="n">EXACT_SGD</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">AssertionError</span><span class="p">(</span>
                <span class="s2">&quot;`use_writeback_bwd_prehook` is only supported for OptimType.EXACT_SGD&quot;</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">learning_rate_tensor</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span>
            <span class="n">learning_rate</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cpu&quot;</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span>
        <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">optimizer_args</span> <span class="o">=</span> <span class="n">invokers</span><span class="o">.</span><span class="n">lookup_args</span><span class="o">.</span><span class="n">OptimizerArgs</span><span class="p">(</span>
            <span class="n">stochastic_rounding</span><span class="o">=</span><span class="n">stochastic_rounding</span><span class="p">,</span>
            <span class="n">gradient_clipping</span><span class="o">=</span><span class="n">gradient_clipping</span><span class="p">,</span>
            <span class="n">max_gradient</span><span class="o">=</span><span class="n">max_gradient</span><span class="p">,</span>
            <span class="n">max_norm</span><span class="o">=</span><span class="n">max_norm</span><span class="p">,</span>
            <span class="n">eps</span><span class="o">=</span><span class="n">eps</span><span class="p">,</span>
            <span class="n">beta1</span><span class="o">=</span><span class="n">beta1</span><span class="p">,</span>
            <span class="n">beta2</span><span class="o">=</span><span class="n">beta2</span><span class="p">,</span>
            <span class="n">weight_decay</span><span class="o">=</span><span class="n">weight_decay</span><span class="p">,</span>
            <span class="n">weight_decay_mode</span><span class="o">=</span><span class="n">opt_arg_weight_decay_mode</span><span class="o">.</span><span class="n">value</span><span class="p">,</span>
            <span class="n">eta</span><span class="o">=</span><span class="n">eta</span><span class="p">,</span>
            <span class="n">momentum</span><span class="o">=</span><span class="n">momentum</span><span class="p">,</span>
            <span class="n">counter_halflife</span><span class="o">=</span><span class="n">counter_halflife</span><span class="p">,</span>
            <span class="n">adjustment_iter</span><span class="o">=</span><span class="n">counter_based_regularization</span><span class="o">.</span><span class="n">adjustment_iter</span><span class="p">,</span>
            <span class="n">adjustment_ub</span><span class="o">=</span><span class="n">counter_based_regularization</span><span class="o">.</span><span class="n">adjustment_ub</span><span class="p">,</span>
            <span class="n">learning_rate_mode</span><span class="o">=</span><span class="n">counter_based_regularization</span><span class="o">.</span><span class="n">learning_rate_mode</span><span class="o">.</span><span class="n">value</span><span class="p">,</span>
            <span class="n">grad_sum_decay</span><span class="o">=</span><span class="n">counter_based_regularization</span><span class="o">.</span><span class="n">grad_sum_decay</span><span class="o">.</span><span class="n">value</span><span class="p">,</span>
            <span class="n">tail_id_threshold</span><span class="o">=</span><span class="n">counter_based_regularization</span><span class="o">.</span><span class="n">tail_id_threshold</span><span class="o">.</span><span class="n">val</span><span class="p">,</span>
            <span class="n">is_tail_id_thresh_ratio</span><span class="o">=</span><span class="nb">int</span><span class="p">(</span>
                <span class="n">counter_based_regularization</span><span class="o">.</span><span class="n">tail_id_threshold</span><span class="o">.</span><span class="n">is_ratio</span>
            <span class="p">),</span>
            <span class="n">total_hash_size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">total_hash_size</span><span class="p">,</span>
            <span class="n">weight_norm_coefficient</span><span class="o">=</span><span class="n">cowclip_regularization</span><span class="o">.</span><span class="n">weight_norm_coefficient</span><span class="p">,</span>
            <span class="n">lower_bound</span><span class="o">=</span><span class="n">cowclip_regularization</span><span class="o">.</span><span class="n">lower_bound</span><span class="p">,</span>
            <span class="n">regularization_mode</span><span class="o">=</span><span class="n">weight_decay_mode</span><span class="o">.</span><span class="n">value</span><span class="p">,</span>
            <span class="n">use_rowwise_bias_correction</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">use_rowwise_bias_correction</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="k">if</span> <span class="n">optimizer</span> <span class="o">!=</span> <span class="n">OptimType</span><span class="o">.</span><span class="n">NONE</span><span class="p">:</span>
            <span class="k">assert</span> <span class="p">(</span>
                <span class="n">optimizer</span>
                <span class="ow">in</span> <span class="p">(</span><span class="n">OptimType</span><span class="o">.</span><span class="n">PARTIAL_ROWWISE_ADAM</span><span class="p">,</span> <span class="n">OptimType</span><span class="o">.</span><span class="n">ENSEMBLE_ROWWISE_ADAGRAD</span><span class="p">)</span>
                <span class="ow">or</span> <span class="n">optimizer_state_dtypes</span> <span class="ow">is</span> <span class="kc">None</span>
            <span class="p">),</span> <span class="s2">&quot;optimizer_state_dtypes option is only supported for OptimType.PARTIAL_ROWWISE_ADAM and OptimType.ENSEMBLE_ROWWISE_ADAGRAD&quot;</span>
            <span class="k">if</span> <span class="n">optimizer</span> <span class="ow">in</span> <span class="p">(</span><span class="n">OptimType</span><span class="o">.</span><span class="n">EXACT_SGD</span><span class="p">,):</span>
                <span class="c1"># NOTE: make TorchScript work!</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_register_nonpersistent_buffers</span><span class="p">(</span><span class="s2">&quot;momentum1&quot;</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">momentum1_dtype</span> <span class="o">=</span> <span class="p">(</span>
                    <span class="n">torch</span><span class="o">.</span><span class="n">float32</span>
                    <span class="k">if</span> <span class="p">(</span>
                        <span class="n">optimizer_state_dtypes</span> <span class="ow">is</span> <span class="kc">None</span>
                        <span class="ow">or</span> <span class="s2">&quot;momentum1&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">optimizer_state_dtypes</span>
                        <span class="ow">or</span> <span class="n">optimizer</span> <span class="o">==</span> <span class="n">OptimType</span><span class="o">.</span><span class="n">ENSEMBLE_ROWWISE_ADAGRAD</span>
                    <span class="p">)</span>
                    <span class="k">else</span> <span class="n">optimizer_state_dtypes</span><span class="p">[</span><span class="s2">&quot;momentum1&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">as_dtype</span><span class="p">()</span>
                <span class="p">)</span>
                <span class="n">rowwise</span> <span class="o">=</span> <span class="n">optimizer</span> <span class="ow">in</span> <span class="p">[</span>
                    <span class="n">OptimType</span><span class="o">.</span><span class="n">EXACT_ROWWISE_ADAGRAD</span><span class="p">,</span>
                    <span class="n">OptimType</span><span class="o">.</span><span class="n">ENSEMBLE_ROWWISE_ADAGRAD</span><span class="p">,</span>
                    <span class="n">OptimType</span><span class="o">.</span><span class="n">EMAINPLACE_ROWWISE_ADAGRAD</span><span class="p">,</span>
                <span class="p">]</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_apply_split</span><span class="p">(</span>
                    <span class="n">construct_split_state</span><span class="p">(</span>
                        <span class="n">embedding_specs</span><span class="p">,</span>
                        <span class="n">rowwise</span><span class="o">=</span><span class="n">rowwise</span><span class="p">,</span>
                        <span class="n">cacheable</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                        <span class="n">placement</span><span class="o">=</span><span class="p">(</span>
                            <span class="n">EmbeddingLocation</span><span class="o">.</span><span class="n">MANAGED</span>
                            <span class="k">if</span> <span class="p">((</span><span class="ow">not</span> <span class="n">rowwise</span><span class="p">)</span> <span class="ow">and</span> <span class="n">uvm_non_rowwise_momentum</span><span class="p">)</span>
                            <span class="k">else</span> <span class="kc">None</span>
                        <span class="p">),</span>
                    <span class="p">),</span>
                    <span class="n">prefix</span><span class="o">=</span><span class="s2">&quot;momentum1&quot;</span><span class="p">,</span>
                    <span class="c1"># pyre-fixme[6]: Expected `Type[Type[torch._dtype]]` for 3rd param</span>
                    <span class="c1">#  but got `Type[torch.float32]`.</span>
                    <span class="n">dtype</span><span class="o">=</span><span class="n">momentum1_dtype</span><span class="p">,</span>
                    <span class="n">enforce_hbm</span><span class="o">=</span><span class="n">enforce_hbm</span><span class="p">,</span>
                    <span class="n">uvm_host_mapped</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">uvm_host_mapped</span><span class="p">,</span>
                <span class="p">)</span>
            <span class="k">if</span> <span class="n">optimizer</span> <span class="ow">in</span> <span class="p">(</span>
                <span class="n">OptimType</span><span class="o">.</span><span class="n">ADAM</span><span class="p">,</span>
                <span class="n">OptimType</span><span class="o">.</span><span class="n">PARTIAL_ROWWISE_ADAM</span><span class="p">,</span>
                <span class="n">OptimType</span><span class="o">.</span><span class="n">LAMB</span><span class="p">,</span>
                <span class="n">OptimType</span><span class="o">.</span><span class="n">PARTIAL_ROWWISE_LAMB</span><span class="p">,</span>
                <span class="n">OptimType</span><span class="o">.</span><span class="n">ENSEMBLE_ROWWISE_ADAGRAD</span><span class="p">,</span>
            <span class="p">):</span>
                <span class="n">rowwise</span> <span class="o">=</span> <span class="n">optimizer</span> <span class="ow">in</span> <span class="p">(</span>
                    <span class="n">OptimType</span><span class="o">.</span><span class="n">PARTIAL_ROWWISE_ADAM</span><span class="p">,</span>
                    <span class="n">OptimType</span><span class="o">.</span><span class="n">PARTIAL_ROWWISE_LAMB</span><span class="p">,</span>
                <span class="p">)</span>
                <span class="n">momentum2_dtype</span> <span class="o">=</span> <span class="p">(</span>
                    <span class="n">torch</span><span class="o">.</span><span class="n">float32</span>
                    <span class="k">if</span> <span class="p">(</span>
                        <span class="n">optimizer_state_dtypes</span> <span class="ow">is</span> <span class="kc">None</span>
                        <span class="ow">or</span> <span class="s2">&quot;momentum2&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">optimizer_state_dtypes</span>
                    <span class="p">)</span>
                    <span class="k">else</span> <span class="n">optimizer_state_dtypes</span><span class="p">[</span><span class="s2">&quot;momentum2&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">as_dtype</span><span class="p">()</span>
                <span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_apply_split</span><span class="p">(</span>
                    <span class="n">construct_split_state</span><span class="p">(</span>
                        <span class="n">embedding_specs</span><span class="p">,</span>
                        <span class="n">rowwise</span><span class="o">=</span><span class="n">rowwise</span><span class="p">,</span>
                        <span class="n">cacheable</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                        <span class="n">placement</span><span class="o">=</span><span class="p">(</span>
                            <span class="n">EmbeddingLocation</span><span class="o">.</span><span class="n">MANAGED</span>
                            <span class="k">if</span> <span class="p">((</span><span class="ow">not</span> <span class="n">rowwise</span><span class="p">)</span> <span class="ow">and</span> <span class="n">uvm_non_rowwise_momentum</span><span class="p">)</span>
                            <span class="k">else</span> <span class="kc">None</span>
                        <span class="p">),</span>
                    <span class="p">),</span>
                    <span class="n">prefix</span><span class="o">=</span><span class="s2">&quot;momentum2&quot;</span><span class="p">,</span>
                    <span class="c1"># pyre-fixme[6]: Expected `Type[Type[torch._dtype]]` for 3rd param</span>
                    <span class="c1">#  but got `Type[torch.float32]`.</span>
                    <span class="n">dtype</span><span class="o">=</span><span class="n">momentum2_dtype</span><span class="p">,</span>
                    <span class="n">uvm_host_mapped</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">uvm_host_mapped</span><span class="p">,</span>
                <span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="c1"># NOTE: make TorchScript work!</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_register_nonpersistent_buffers</span><span class="p">(</span><span class="s2">&quot;momentum2&quot;</span><span class="p">)</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_used_rowwise_adagrad_with_counter</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_apply_split</span><span class="p">(</span>
                    <span class="n">construct_split_state</span><span class="p">(</span>
                        <span class="n">embedding_specs</span><span class="p">,</span>
                        <span class="n">rowwise</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                        <span class="n">cacheable</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                    <span class="p">),</span>
                    <span class="n">prefix</span><span class="o">=</span><span class="s2">&quot;prev_iter&quot;</span><span class="p">,</span>
                    <span class="c1"># TODO: ideally we should use int64 to track iter but it failed to compile.</span>
                    <span class="c1"># It may be related to low precision training code. Currently using float32</span>
                    <span class="c1"># as a workaround while investigating the issue.</span>
                    <span class="c1"># pyre-fixme[6]: Expected `Type[Type[torch._dtype]]` for 3rd param</span>
                    <span class="c1">#  but got `Type[torch.float32]`.</span>
                    <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span>
                    <span class="n">uvm_host_mapped</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">uvm_host_mapped</span><span class="p">,</span>
                <span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_apply_split</span><span class="p">(</span>
                    <span class="n">construct_split_state</span><span class="p">(</span>
                        <span class="n">embedding_specs</span><span class="p">,</span>
                        <span class="n">rowwise</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                        <span class="n">cacheable</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                    <span class="p">),</span>
                    <span class="n">prefix</span><span class="o">=</span><span class="s2">&quot;row_counter&quot;</span><span class="p">,</span>
                    <span class="c1"># pyre-fixme[6]: Expected `Type[Type[torch._dtype]]` for 3rd param</span>
                    <span class="c1">#  but got `Type[torch.float32]`.</span>
                    <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span>
                    <span class="n">uvm_host_mapped</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">uvm_host_mapped</span><span class="p">,</span>
                <span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span>
                    <span class="s2">&quot;max_counter&quot;</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
                <span class="p">)</span>
            <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">_used_rowwise_adagrad_with_global_weight_decay</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_apply_split</span><span class="p">(</span>
                    <span class="n">construct_split_state</span><span class="p">(</span>
                        <span class="n">embedding_specs</span><span class="p">,</span>
                        <span class="n">rowwise</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                        <span class="n">cacheable</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                    <span class="p">),</span>
                    <span class="n">prefix</span><span class="o">=</span><span class="s2">&quot;prev_iter&quot;</span><span class="p">,</span>
                    <span class="c1"># TODO: ideally we should use int64 to track iter but it failed to compile.</span>
                    <span class="c1"># It may be related to low precision training code. Currently using float32</span>
                    <span class="c1"># as a workaround while investigating the issue.</span>
                    <span class="c1"># pyre-fixme[6]: Expected `Type[Type[torch._dtype]]` for 3rd param</span>
                    <span class="c1">#  but got `Type[torch.float32]`.</span>
                    <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span>
                    <span class="n">uvm_host_mapped</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">uvm_host_mapped</span><span class="p">,</span>
                <span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_register_nonpersistent_buffers</span><span class="p">(</span><span class="s2">&quot;row_counter&quot;</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span>
                    <span class="s2">&quot;max_counter&quot;</span><span class="p">,</span>
                    <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">current_device</span><span class="p">),</span>
                    <span class="n">persistent</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                <span class="p">)</span>
            <span class="k">elif</span> <span class="n">optimizer</span> <span class="o">==</span> <span class="n">OptimType</span><span class="o">.</span><span class="n">ADAM</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_rowwise_bias_correction</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_apply_split</span><span class="p">(</span>
                    <span class="n">construct_split_state</span><span class="p">(</span>
                        <span class="n">embedding_specs</span><span class="p">,</span>
                        <span class="n">rowwise</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                        <span class="n">cacheable</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                    <span class="p">),</span>
                    <span class="n">prefix</span><span class="o">=</span><span class="s2">&quot;row_counter&quot;</span><span class="p">,</span>
                    <span class="c1"># pyre-fixme[6]: Expected `Type[Type[torch._dtype]]` for 3rd param</span>
                    <span class="c1">#  but got `Type[torch.float32]`.</span>
                    <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span>
                    <span class="n">uvm_host_mapped</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">uvm_host_mapped</span><span class="p">,</span>
                <span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_register_nonpersistent_buffers</span><span class="p">(</span><span class="s2">&quot;prev_iter&quot;</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_register_nonpersistent_buffers</span><span class="p">(</span><span class="s2">&quot;row_counter&quot;</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span>
                    <span class="s2">&quot;max_counter&quot;</span><span class="p">,</span>
                    <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">current_device</span><span class="p">),</span>
                    <span class="n">persistent</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                <span class="p">)</span>
            <span class="k">if</span> <span class="p">(</span>
                <span class="n">optimizer</span>
                <span class="ow">in</span> <span class="p">(</span>
                    <span class="n">OptimType</span><span class="o">.</span><span class="n">ADAM</span><span class="p">,</span>
                    <span class="n">OptimType</span><span class="o">.</span><span class="n">LAMB</span><span class="p">,</span>
                    <span class="n">OptimType</span><span class="o">.</span><span class="n">PARTIAL_ROWWISE_ADAM</span><span class="p">,</span>
                    <span class="n">OptimType</span><span class="o">.</span><span class="n">PARTIAL_ROWWISE_LAMB</span><span class="p">,</span>
                    <span class="n">OptimType</span><span class="o">.</span><span class="n">ENSEMBLE_ROWWISE_ADAGRAD</span><span class="p">,</span>
                    <span class="n">OptimType</span><span class="o">.</span><span class="n">EMAINPLACE_ROWWISE_ADAGRAD</span><span class="p">,</span>
                <span class="p">)</span>
                <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">_used_rowwise_adagrad_with_global_weight_decay</span>
            <span class="p">):</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span>
                    <span class="s2">&quot;iter&quot;</span><span class="p">,</span>
                    <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int64</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">current_device</span><span class="p">),</span>
                <span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span>
                    <span class="s2">&quot;iter&quot;</span><span class="p">,</span>
                    <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int64</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">current_device</span><span class="p">),</span>
                    <span class="n">persistent</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">iter_cpu</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int64</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s2">&quot;cpu&quot;</span><span class="p">)</span>

        <span class="n">cache_state</span> <span class="o">=</span> <span class="n">construct_cache_state</span><span class="p">(</span><span class="n">rows</span><span class="p">,</span> <span class="n">locations</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">feature_table_map</span><span class="p">)</span>

        <span class="c1"># Add table-wise cache miss counter</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">record_cache_metrics</span><span class="o">.</span><span class="n">record_tablewise_cache_miss</span><span class="p">:</span>
            <span class="n">num_tables</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">cache_state</span><span class="o">.</span><span class="n">cache_hash_size_cumsum</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span>
                <span class="s2">&quot;table_wise_cache_miss&quot;</span><span class="p">,</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span>
                    <span class="n">num_tables</span><span class="p">,</span>
                    <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">current_device</span><span class="p">,</span>
                    <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int64</span><span class="p">,</span>
                <span class="p">),</span>
            <span class="p">)</span>
        <span class="c1"># NOTE: make TorchScript work!</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span>
                <span class="s2">&quot;table_wise_cache_miss&quot;</span><span class="p">,</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span>
                    <span class="mi">0</span><span class="p">,</span>
                    <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">current_device</span><span class="p">,</span>
                    <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int64</span><span class="p">,</span>
                <span class="p">),</span>
            <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_apply_cache_state</span><span class="p">(</span>
            <span class="n">cache_state</span><span class="p">,</span>
            <span class="n">cache_algorithm</span><span class="p">,</span>
            <span class="n">cache_load_factor</span><span class="p">,</span>
            <span class="n">cache_sets</span><span class="p">,</span>
            <span class="n">cache_reserved_memory</span><span class="p">,</span>
            <span class="n">cache_precision</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Contents: </span><span class="si">{</span><span class="n">table_names</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">log</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;Using fused </span><span class="si">{</span><span class="n">optimizer</span><span class="si">}</span><span class="s2"> with optimizer_args=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">optimizer_args</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="n">optimizer</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="n">OptimType</span><span class="o">.</span><span class="n">NONE</span><span class="w"> </span><span class="k">else</span><span class="w"> </span><span class="kc">None</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">log</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;Using rowwise_adagrad_with_counter=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">_used_rowwise_adagrad_with_counter</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">step</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">last_reported_step</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">last_reported_uvm_stats</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="c1"># Check whether to use TBE v2</span>
        <span class="n">is_experimental</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="k">if</span> <span class="n">use_experimental_tbe</span><span class="p">:</span>
            <span class="n">is_experimental</span> <span class="o">=</span> <span class="kc">True</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="s2">&quot;use_experimental_tbe is set to True; Using experimental TBE&quot;</span><span class="p">)</span>

        <span class="k">elif</span> <span class="nb">int</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;FBGEMM_EXPERIMENTAL_TBE&quot;</span><span class="p">,</span> <span class="s2">&quot;0&quot;</span><span class="p">))</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="c1"># Keep the old feature enablement mechanism to ensure no negative impact on models that have already adopted TBE v2</span>
            <span class="n">is_experimental</span> <span class="o">=</span> <span class="kc">True</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="s2">&quot;FBGEMM_EXPERIMENTAL_TBE is set to True; Using experimental TBE&quot;</span><span class="p">)</span>

        <span class="c1"># NOTE: Keep this disabled for now until the backend lands into Pyper</span>
        <span class="c1"># elif FeatureGateName.TBE_V2.is_enabled():</span>
        <span class="c1">#     is_experimental = True</span>
        <span class="c1">#     self.log(&quot;TBE_V2 Knob is set to True; Using experimental TBE&quot;)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">is_experimental</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="n">is_experimental</span>

        <span class="c1"># Get a debug function pointer</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_debug_print_input_stats</span><span class="p">:</span> <span class="n">Callable</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="kc">None</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_debug_print_input_stats_factory</span><span class="p">()</span>
        <span class="p">)</span>

        <span class="k">if</span> <span class="n">optimizer</span> <span class="o">==</span> <span class="n">OptimType</span><span class="o">.</span><span class="n">EXACT_SGD</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_writeback_bwd_prehook</span><span class="p">:</span>
            <span class="c1"># Register writeback hook for Exact_SGD optimizer</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">log</span><span class="p">(</span>
                <span class="s2">&quot;SplitTableBatchedEmbeddingBagsCodegen:  use_writeback_bwd_prehook is enabled.&quot;</span>
            <span class="p">)</span>
            <span class="c1"># pyre-fixme[6]: Expected `typing.Callable[[Module, Union[Tensor, typing.Tuple[Tensor, ...]]], Union[None, Tensor, typing.Tuple[Tensor, ...]]]`</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">register_full_backward_pre_hook</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">writeback_hook</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">embedding_table_index_type</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">int64</span><span class="p">]:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;embedding_table_index_type must be torch.int32 or torch.int64, but got </span><span class="si">{</span><span class="n">embedding_table_index_type</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">embedding_table_index_type</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">embedding_table_index_type</span>
        <span class="k">if</span> <span class="n">embedding_table_offset_type</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">int64</span><span class="p">]:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;embedding_table_offset_type must be torch.int32 or torch.int64, but got </span><span class="si">{</span><span class="n">embedding_table_offset_type</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">embedding_table_offset_type</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">embedding_table_offset_type</span>

    <span class="nd">@torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">ignore</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">log</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">msg</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Log with TBE id prefix to distinguish between multiple TBE instances</span>
<span class="sd">        per process</span>

<span class="sd">        Args:</span>
<span class="sd">            msg (str): The message to print</span>

<span class="sd">        Returns:</span>
<span class="sd">            None</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;[TBE=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">uuid</span><span class="si">}</span><span class="s2">] </span><span class="si">{</span><span class="n">msg</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_register_nonpersistent_buffers</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">prefix</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="c1"># NOTE: make TorchScript work!</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">prefix</span><span class="si">}</span><span class="s2">_dev&quot;</span><span class="p">,</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int64</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">current_device</span><span class="p">),</span>
            <span class="n">persistent</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">prefix</span><span class="si">}</span><span class="s2">_host&quot;</span><span class="p">,</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int64</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">current_device</span><span class="p">),</span>
            <span class="n">persistent</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">prefix</span><span class="si">}</span><span class="s2">_uvm&quot;</span><span class="p">,</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int64</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">current_device</span><span class="p">),</span>
            <span class="n">persistent</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">prefix</span><span class="si">}</span><span class="s2">_placements&quot;</span><span class="p">,</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int64</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">current_device</span><span class="p">),</span>
            <span class="n">persistent</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">prefix</span><span class="si">}</span><span class="s2">_offsets&quot;</span><span class="p">,</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int64</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">current_device</span><span class="p">),</span>
            <span class="n">persistent</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">get_table_name_for_logging</span><span class="p">(</span><span class="n">table_names</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]])</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Given a list of all table names in the TBE, generate a string to</span>
<span class="sd">        represent them in logging. If there is more than one table, this method</span>
<span class="sd">        will count them than list them.</span>

<span class="sd">        Args:</span>
<span class="sd">            table_names (Optional[List[str]]): A list of table anmes in TBE</span>

<span class="sd">        Returns:</span>
<span class="sd">            A string that represents tables in logging</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">table_names</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="s2">&quot;&lt;Unknown&gt;&quot;</span>
        <span class="c1"># Do this because sometimes multiple shards of the same table could appear</span>
        <span class="c1"># in one TBE.</span>
        <span class="n">table_name_set</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">table_names</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">table_name_set</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">return</span> <span class="nb">next</span><span class="p">(</span><span class="nb">iter</span><span class="p">(</span><span class="n">table_name_set</span><span class="p">))</span>
        <span class="k">return</span> <span class="sa">f</span><span class="s2">&quot;&lt;</span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">table_name_set</span><span class="p">)</span><span class="si">}</span><span class="s2"> tables&gt;&quot;</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">get_prefetch_passes</span><span class="p">(</span>
        <span class="n">multipass_prefetch_config</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">MultiPassPrefetchConfig</span><span class="p">],</span>
        <span class="n">input_tensor</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">output_tensor</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">,</span> <span class="nb">int</span><span class="p">]]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Given inputs (the indices to forward), partition the input and output</span>
<span class="sd">        into smaller chunks and return them as a list of tuples</span>
<span class="sd">        (input[start_idx:end_idx], output[start_idx:end_idx], start_idx).</span>

<span class="sd">        The caller must guarantee that input and output have non-zero dimension</span>
<span class="sd">        0. The returned segments are guaranteed to completely and</span>
<span class="sd">        non-overlappingly cover the input tensor.</span>

<span class="sd">        In non-multipass-prefetch mode, it returns the input/output tensor</span>
<span class="sd">        itself.</span>

<span class="sd">        Args:</span>
<span class="sd">            multipass_prefetch_config (Optional[MultiPassPrefetchConfig]):</span>
<span class="sd">                A config for multi-pass cache prefetch. If None, multi-pass</span>
<span class="sd">                prefetch is not used.</span>

<span class="sd">            input_tensor (Tensor): The input tensor to be partitioned</span>

<span class="sd">            output_tensor (Tensor): The output tensor to be partitioned</span>

<span class="sd">        Returns:</span>
<span class="sd">            A list of partitioned inputs and outputs (List[Tuple[Tensor,</span>
<span class="sd">                Tensor, int]])</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">multipass_prefetch_config</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="p">[(</span><span class="n">input_tensor</span><span class="p">,</span> <span class="n">output_tensor</span><span class="p">,</span> <span class="mi">0</span><span class="p">)]</span>
        <span class="n">mpp_config</span><span class="p">:</span> <span class="n">MultiPassPrefetchConfig</span> <span class="o">=</span> <span class="n">multipass_prefetch_config</span>

        <span class="n">N</span> <span class="o">=</span> <span class="n">input_tensor</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">N</span> <span class="o">&lt;=</span> <span class="n">mpp_config</span><span class="o">.</span><span class="n">num_passes</span> <span class="ow">or</span> <span class="n">mpp_config</span><span class="o">.</span><span class="n">num_passes</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="c1"># One row per pass, just don&#39;t split</span>
            <span class="k">return</span> <span class="p">[(</span><span class="n">input_tensor</span><span class="p">,</span> <span class="n">output_tensor</span><span class="p">,</span> <span class="mi">0</span><span class="p">)]</span>

        <span class="n">pass_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span>
            <span class="p">(</span><span class="n">N</span> <span class="o">+</span> <span class="n">mpp_config</span><span class="o">.</span><span class="n">num_passes</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">//</span> <span class="n">mpp_config</span><span class="o">.</span><span class="n">num_passes</span><span class="p">,</span>
            <span class="n">mpp_config</span><span class="o">.</span><span class="n">min_splitable_pass_size</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="k">return</span> <span class="nb">list</span><span class="p">(</span>
            <span class="nb">zip</span><span class="p">(</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">,</span> <span class="n">pass_size</span><span class="p">),</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">output_tensor</span><span class="p">,</span> <span class="n">pass_size</span><span class="p">),</span>
                <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">pass_size</span><span class="p">),</span>
            <span class="p">)</span>
        <span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">get_states</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">prefix</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Get a state of a given tensor (`prefix`)</span>

<span class="sd">        Args:</span>
<span class="sd">            prefix (str): A prefix of the state to obtain</span>

<span class="sd">        Returns:</span>
<span class="sd">            A tuple of tensors corresponding to the obtained state containing</span>

<span class="sd">            (1) A GPU state tensor</span>

<span class="sd">            (2) A CPU state tensor</span>

<span class="sd">            (3) A UVM state tensor</span>

<span class="sd">            (4) A placement tensor - containing placements of embedding tables</span>
<span class="sd">                (torch.int32_t tensor). (0 = DEVICE, 1 = MANAGED, 2 =</span>
<span class="sd">                MANAGED_CACHING, 3 = HOST, 4 = MTIA)</span>

<span class="sd">            (5) An offset tensor - containing the relative positions of</span>
<span class="sd">                embedding tables in the corresponding state tensor (GPU, CPU,</span>
<span class="sd">                or UVM state tensor)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">prefix</span><span class="si">}</span><span class="s2">_physical_placements&quot;</span><span class="p">):</span>
            <span class="k">raise</span> <span class="n">DoesNotHavePrefix</span><span class="p">()</span>
        <span class="n">dev_param</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">prefix</span><span class="si">}</span><span class="s2">_dev&quot;</span><span class="p">)</span>
        <span class="n">host_param</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">prefix</span><span class="si">}</span><span class="s2">_host&quot;</span><span class="p">)</span>
        <span class="n">uvm_param</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">prefix</span><span class="si">}</span><span class="s2">_uvm&quot;</span><span class="p">)</span>
        <span class="n">placements</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">prefix</span><span class="si">}</span><span class="s2">_physical_placements&quot;</span><span class="p">)</span>
        <span class="n">offsets</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">prefix</span><span class="si">}</span><span class="s2">_physical_offsets&quot;</span><span class="p">)</span>
        <span class="k">return</span> <span class="p">(</span>
            <span class="n">dev_param</span><span class="p">,</span>
            <span class="n">host_param</span><span class="p">,</span>
            <span class="n">uvm_param</span><span class="p">,</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">placements</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int32</span><span class="p">),</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">offsets</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int64</span><span class="p">),</span>
        <span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">get_all_states</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">]]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Get all states in the TBE (`weights`, `momentum1`, `momentum2`,</span>
<span class="sd">        `prev_iter`, and `row_counter`)</span>

<span class="sd">        Returns:</span>
<span class="sd">            A list of states. Each state is a tuple of tensors (GPU state</span>
<span class="sd">            tensor, CPU state tensor, UVM state tensor, placement tensor and</span>
<span class="sd">            offset tensor)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">all_states</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">prefix</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;weights&quot;</span><span class="p">,</span> <span class="s2">&quot;momentum1&quot;</span><span class="p">,</span> <span class="s2">&quot;momentum2&quot;</span><span class="p">,</span> <span class="s2">&quot;prev_iter&quot;</span><span class="p">,</span> <span class="s2">&quot;row_counter&quot;</span><span class="p">]:</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="n">all_states</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">get_states</span><span class="p">(</span><span class="n">prefix</span><span class="p">))</span>
            <span class="k">except</span> <span class="n">DoesNotHavePrefix</span><span class="p">:</span>
                <span class="k">pass</span>
        <span class="k">return</span> <span class="n">all_states</span>

    <span class="nd">@torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">export</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">get_cache_miss_counter</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Get the cache miss counter. `cache_miss_counter` contains two items:</span>

<span class="sd">        (1) `cache_miss_forward_count` which records the total number of</span>
<span class="sd">            forwards which has at least one cache miss</span>

<span class="sd">        (2) `unique_cache_miss_count` which records to total number of unique</span>
<span class="sd">            (dedup) cache misses</span>

<span class="sd">        Returns:</span>
<span class="sd">            The cache miss counter</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># pyre-fixme[7]: Expected `Tensor` but got `Union[Module, Tensor]`.</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">cache_miss_counter</span>

    <span class="nd">@torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">export</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">get_table_wise_cache_miss</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Get the table-wise cache miss tensor. `table_wise_cache_miss` contains</span>
<span class="sd">        all the cache miss count for each table in this embedding table object:</span>

<span class="sd">        Returns:</span>
<span class="sd">            The table-wise cache miss tensor</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">table_wise_cache_miss</span>

    <span class="c1"># The callback function for AsyncTimer to record duration to different event</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">_report_duration</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">it_step</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">dur_ms</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
        <span class="n">event_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">assert</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">stats_reporter</span>
        <span class="p">),</span> <span class="s2">&quot;We should not be here. AsyncTimer only happens with reporter present.&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">stats_reporter</span><span class="o">.</span><span class="n">report_duration</span><span class="p">(</span>
            <span class="n">iteration_step</span><span class="o">=</span><span class="n">it_step</span><span class="p">,</span>
            <span class="n">event_name</span><span class="o">=</span><span class="n">event_name</span><span class="p">,</span>
            <span class="n">duration_ms</span><span class="o">=</span><span class="n">dur_ms</span><span class="p">,</span>
            <span class="n">embedding_id</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">logging_table_name</span><span class="p">,</span>
            <span class="n">tbe_id</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">uuid</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="nd">@torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">ignore</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">_report_tbe_mem_usage</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">stats_reporter</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span>

        <span class="n">stats_reporter</span><span class="p">:</span> <span class="n">TBEStatsReporter</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">stats_reporter</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">stats_reporter</span><span class="o">.</span><span class="n">should_report</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">step</span><span class="p">):</span>
            <span class="k">return</span>

        <span class="n">total_mem_usage</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span>
            <span class="n">param</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span> <span class="o">*</span> <span class="n">param</span><span class="o">.</span><span class="n">element_size</span><span class="p">()</span> <span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">parameters</span><span class="p">()</span>
        <span class="p">)</span> <span class="o">+</span> <span class="nb">sum</span><span class="p">(</span><span class="n">buffer</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span> <span class="o">*</span> <span class="n">buffer</span><span class="o">.</span><span class="n">element_size</span><span class="p">()</span> <span class="k">for</span> <span class="n">buffer</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">buffers</span><span class="p">())</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_cpu</span><span class="p">:</span>
            <span class="n">total_hbm_usage</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="n">total_uvm_usage</span> <span class="o">=</span> <span class="n">total_mem_usage</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># hbm usage is total usage minus uvm usage</span>
            <span class="n">total_uvm_usage</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span>
                <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tensor_name</span><span class="p">)</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span>
                <span class="o">*</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tensor_name</span><span class="p">)</span><span class="o">.</span><span class="n">element_size</span><span class="p">()</span>
                <span class="k">for</span> <span class="n">tensor_name</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_uvm_tensors_log</span>
                <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tensor_name</span><span class="p">)</span>
            <span class="p">)</span>
            <span class="n">total_hbm_usage</span> <span class="o">=</span> <span class="n">total_mem_usage</span> <span class="o">-</span> <span class="n">total_uvm_usage</span>

        <span class="n">stats_reporter</span><span class="o">.</span><span class="n">report_data_amount</span><span class="p">(</span>
            <span class="n">iteration_step</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">step</span><span class="p">,</span>
            <span class="n">event_name</span><span class="o">=</span><span class="s2">&quot;tbe.total_hbm_usage&quot;</span><span class="p">,</span>
            <span class="n">data_bytes</span><span class="o">=</span><span class="n">total_hbm_usage</span><span class="p">,</span>
            <span class="n">embedding_id</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">logging_table_name</span><span class="p">,</span>
            <span class="n">tbe_id</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">uuid</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">stats_reporter</span><span class="o">.</span><span class="n">report_data_amount</span><span class="p">(</span>
            <span class="n">iteration_step</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">step</span><span class="p">,</span>
            <span class="n">event_name</span><span class="o">=</span><span class="s2">&quot;tbe.total_uvm_usage&quot;</span><span class="p">,</span>
            <span class="n">data_bytes</span><span class="o">=</span><span class="n">total_uvm_usage</span><span class="p">,</span>
            <span class="n">embedding_id</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">logging_table_name</span><span class="p">,</span>
            <span class="n">tbe_id</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">uuid</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="nd">@torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">ignore</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">_report_io_size_count</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">event</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">data</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">stats_reporter</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">data</span>
        <span class="n">stats_reporter</span><span class="p">:</span> <span class="n">TBEStatsReporter</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">stats_reporter</span>
        <span class="k">if</span> <span class="n">stats_reporter</span><span class="o">.</span><span class="n">should_report</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">step</span><span class="p">):</span>
            <span class="n">stats_reporter</span><span class="o">.</span><span class="n">report_data_amount</span><span class="p">(</span>
                <span class="n">iteration_step</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">step</span><span class="p">,</span>
                <span class="n">event_name</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;tbe.</span><span class="si">{</span><span class="n">event</span><span class="si">}</span><span class="s2">_size&quot;</span><span class="p">,</span>
                <span class="n">data_bytes</span><span class="o">=</span><span class="n">data</span><span class="o">.</span><span class="n">element_size</span><span class="p">()</span> <span class="o">*</span> <span class="n">data</span><span class="o">.</span><span class="n">numel</span><span class="p">(),</span>
                <span class="n">embedding_id</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">logging_table_name</span><span class="p">,</span>
                <span class="n">tbe_id</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">uuid</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="n">stats_reporter</span><span class="o">.</span><span class="n">report_data_amount</span><span class="p">(</span>
                <span class="n">iteration_step</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">step</span><span class="p">,</span>
                <span class="n">event_name</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;tbe.</span><span class="si">{</span><span class="n">event</span><span class="si">}</span><span class="s2">_count&quot;</span><span class="p">,</span>
                <span class="n">data_bytes</span><span class="o">=</span><span class="n">data</span><span class="o">.</span><span class="n">numel</span><span class="p">(),</span>
                <span class="n">embedding_id</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">logging_table_name</span><span class="p">,</span>
                <span class="n">tbe_id</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">uuid</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="k">return</span> <span class="n">data</span>

    <span class="nd">@torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">ignore</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">_generate_vbe_metadata</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">offsets</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">batch_size_per_feature_per_rank</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]]],</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">invokers</span><span class="o">.</span><span class="n">lookup_args</span><span class="o">.</span><span class="n">VBEMetadata</span><span class="p">:</span>
        <span class="c1"># Blocking D2H copy, but only runs at first call</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">feature_dims</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">feature_dims</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">batch_size_per_feature_per_rank</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span> <span class="ow">in</span> <span class="p">(</span>
                <span class="n">OptimType</span><span class="o">.</span><span class="n">EXACT_ROWWISE_ADAGRAD</span><span class="p">,</span>
                <span class="n">OptimType</span><span class="o">.</span><span class="n">EXACT_SGD</span><span class="p">,</span>
                <span class="n">OptimType</span><span class="o">.</span><span class="n">ENSEMBLE_ROWWISE_ADAGRAD</span><span class="p">,</span>
                <span class="n">OptimType</span><span class="o">.</span><span class="n">EMAINPLACE_ROWWISE_ADAGRAD</span><span class="p">,</span>
                <span class="n">OptimType</span><span class="o">.</span><span class="n">NONE</span><span class="p">,</span>
                <span class="n">OptimType</span><span class="o">.</span><span class="n">ADAM</span><span class="p">,</span>
            <span class="p">),</span> <span class="p">(</span>
                <span class="s2">&quot;Variable batch size TBE support is enabled for &quot;</span>
                <span class="s2">&quot;OptimType.EXACT_ROWWISE_ADAGRAD,EXACT_SGD, &quot;</span>
                <span class="s2">&quot;ENSEMBLE_ROWWISE_ADAGRAD, NONE, and ADAM only&quot;</span>
            <span class="p">)</span>
        <span class="k">return</span> <span class="n">generate_vbe_metadata</span><span class="p">(</span>
            <span class="n">offsets</span><span class="p">,</span>
            <span class="n">batch_size_per_feature_per_rank</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">pooling_mode</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">feature_dims</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">current_device</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="nd">@torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">ignore</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">_feature_is_enabled</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">feature</span><span class="p">:</span> <span class="n">FeatureGateName</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
        <span class="c1"># Define proxy method so that it can be marked with @torch.jit.ignore</span>
        <span class="c1"># This allows models using this class to compile correctly</span>
        <span class="k">return</span> <span class="n">FeatureGate</span><span class="o">.</span><span class="n">is_enabled</span><span class="p">(</span><span class="n">feature</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">writeback_update_gradient</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">indices</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">offsets</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">grad</span><span class="p">:</span> <span class="n">Tensor</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">indices</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">grad</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">num_of_tables</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">feature_table_map</span><span class="p">))</span>
        <span class="k">assert</span> <span class="n">num_of_tables</span> <span class="o">*</span> <span class="n">indices</span><span class="o">.</span><span class="n">max</span><span class="p">()</span> <span class="o">&lt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">iinfo</span><span class="p">(</span><span class="n">indices</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span><span class="o">.</span><span class="n">max</span>
        <span class="n">batch_size</span> <span class="o">=</span> <span class="n">offsets</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">//</span> <span class="n">num_of_tables</span>
        <span class="n">max_indices</span> <span class="o">=</span> <span class="n">indices</span><span class="o">.</span><span class="n">max</span><span class="p">()</span>
        <span class="n">non_empty_index</span> <span class="o">=</span> <span class="p">(</span><span class="n">offsets</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span> <span class="o">-</span> <span class="n">offsets</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span><span class="o">.</span><span class="n">nonzero</span><span class="p">()</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
        <span class="c1"># disable dedup across different table</span>
        <span class="n">indices</span> <span class="o">=</span> <span class="p">((</span><span class="n">offsets</span><span class="p">[</span><span class="n">non_empty_index</span><span class="p">])</span> <span class="o">//</span> <span class="n">batch_size</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span>
            <span class="mi">1</span> <span class="o">+</span> <span class="n">max_indices</span>
        <span class="p">)</span> <span class="o">+</span> <span class="n">indices</span>
        <span class="n">grad</span> <span class="o">=</span> <span class="n">grad</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">_</span><span class="p">,</span> <span class="n">idx</span><span class="p">,</span> <span class="n">counts</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span>
            <span class="n">indices</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="nb">sorted</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">return_inverse</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">return_counts</span><span class="o">=</span><span class="kc">True</span>
        <span class="p">)</span>
        <span class="n">_</span><span class="p">,</span> <span class="n">ind_sorted</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">idx</span><span class="p">,</span> <span class="n">stable</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">cum_sum</span> <span class="o">=</span> <span class="n">counts</span><span class="o">.</span><span class="n">cumsum</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">cum_sum</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">0</span><span class="p">])</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">indices</span><span class="o">.</span><span class="n">device</span><span class="p">),</span> <span class="n">cum_sum</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]))</span>
        <span class="n">first_indicies</span> <span class="o">=</span> <span class="n">ind_sorted</span><span class="p">[</span><span class="n">cum_sum</span><span class="p">]</span>
        <span class="n">mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">grad</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">grad</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
        <span class="n">original_index</span> <span class="o">=</span> <span class="n">non_empty_index</span><span class="p">[</span><span class="n">first_indicies</span><span class="p">]</span>

        <span class="n">mask</span><span class="p">[</span><span class="n">original_index</span><span class="p">]</span> <span class="o">=</span> <span class="n">grad</span><span class="p">[</span><span class="n">original_index</span><span class="p">]</span>
        <span class="k">return</span> <span class="n">mask</span>

    <span class="c1"># pyre-fixme[2]: For 1st argument expected not ANY</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">writeback_hook</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">module</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span> <span class="n">grad</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]:</span>
        <span class="n">indices</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_indices</span>
        <span class="n">offsets</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_offsets</span>

        <span class="k">return</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">writeback_update_gradient</span><span class="p">(</span><span class="n">indices</span><span class="p">,</span> <span class="n">offsets</span><span class="p">,</span> <span class="n">grad</span><span class="p">),)</span>

<div class="viewcode-block" id="SplitTableBatchedEmbeddingBagsCodegen.forward"><a class="viewcode-back" href="../../fbgemm_gpu/python-api/tbe_ops_training.html#fbgemm_gpu.split_table_batched_embeddings_ops_training.SplitTableBatchedEmbeddingBagsCodegen.forward">[docs]</a>    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span>  <span class="c1"># noqa: C901</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">indices</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">offsets</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">per_sample_weights</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">feature_requires_grad</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">batch_size_per_feature_per_rank</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">total_unique_indices</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        The forward pass function that</span>

<span class="sd">        (1) Performs input bound checking</span>

<span class="sd">        (2) Generates necessary variable batch size embedding (VBE) metadata (if</span>
<span class="sd">            VBE is used)</span>

<span class="sd">        (3) Prefetches data from UVM to cache (if</span>
<span class="sd">            `EmbeddingLocation.MANAGED_CACHING` is used and the user has not</span>
<span class="sd">            explicitly prefetched data)</span>

<span class="sd">        (4) Performs the embedding table lookup by invoking a corresponding</span>
<span class="sd">            Autograd function (based on the chosen optimizer)</span>

<span class="sd">        Args:</span>
<span class="sd">            indices (Tensor): A 1D-tensor that contains indices to be looked up</span>
<span class="sd">                from all embedding table</span>

<span class="sd">            offsets (Tensor): A 1D-tensor that conatins offsets of indices.</span>
<span class="sd">                Shape `(B * T + 1)` where `B` = batch size and `T` = the number</span>
<span class="sd">                of features.  `offsets[t * B + b + 1] - offsets[t * B + b]` is</span>
<span class="sd">                the length of bag `b` of feature `t`</span>

<span class="sd">            per_sample_weights (Optional[Tensor]): An optional 1D-float-tensor that</span>
<span class="sd">                contains per sample weights. If None, **unweighted** embedding</span>
<span class="sd">                lookup will be perform. Otherwise, **weighted** will be used. The</span>
<span class="sd">                length of this tensor must be the same as the length of the</span>
<span class="sd">                `indices` tensor.  The value of `per_sample_weights[i]` will be</span>
<span class="sd">                used to multiply with every element in the looked up row</span>
<span class="sd">                `indices[i]`, where `0 &lt;= i &lt; len(per_sample_weights)`.</span>

<span class="sd">            feature_requires_grad (Optional[Tensor]): An optional 1D-tensor for</span>
<span class="sd">                indicating if `per_sample_weights` requires gradient. The</span>
<span class="sd">                length of the tensor must be equal to the number of features</span>

<span class="sd">            batch_size_per_feature_per_rank (Optional[List[List[int]]]): An</span>
<span class="sd">                optional 2D-tensor that contains batch sizes for every rank and</span>
<span class="sd">                every feature. If None, TBE assumes that **every feature has the</span>
<span class="sd">                same batch size** and computes the batch size from the `offsets`</span>
<span class="sd">                shape. Otherwise, TBE assumes that different features can have</span>
<span class="sd">                different batch sizes and uses the **variable batch size</span>
<span class="sd">                embedding look up mode (VBE)**. Shape (number of features,</span>
<span class="sd">                number of ranks). `batch_size_per_feature_per_rank[f][r]`</span>
<span class="sd">                represents the batch size of feature `f` and rank `r`</span>

<span class="sd">            total_unique_indices (Optional[int]): An optional integer that</span>
<span class="sd">                represents the total number of unique indices. This value must</span>
<span class="sd">                be set when using `OptimType.NONE`. This is because TBE</span>
<span class="sd">                requires this information for allocating the weight gradient</span>
<span class="sd">                tensor in the backward pass.</span>

<span class="sd">        Returns:</span>
<span class="sd">            A 2D-tensor containing looked up data. Shape `(B, total_D)` where `B` =</span>
<span class="sd">            batch size and `total_D` = the sum of all embedding dimensions in the</span>
<span class="sd">            table</span>

<span class="sd">        Example:</span>

<span class="sd">            &gt;&gt;&gt; import torch</span>
<span class="sd">            &gt;&gt;&gt;</span>
<span class="sd">            &gt;&gt;&gt; from fbgemm_gpu.split_table_batched_embeddings_ops_common import (</span>
<span class="sd">            &gt;&gt;&gt;    EmbeddingLocation,</span>
<span class="sd">            &gt;&gt;&gt; )</span>
<span class="sd">            &gt;&gt;&gt; from fbgemm_gpu.split_table_batched_embeddings_ops_training import (</span>
<span class="sd">            &gt;&gt;&gt;    SplitTableBatchedEmbeddingBagsCodegen,</span>
<span class="sd">            &gt;&gt;&gt;    ComputeDevice,</span>
<span class="sd">            &gt;&gt;&gt; )</span>
<span class="sd">            &gt;&gt;&gt;</span>
<span class="sd">            &gt;&gt;&gt; # Two tables</span>
<span class="sd">            &gt;&gt;&gt; embedding_specs = [</span>
<span class="sd">            &gt;&gt;&gt;     (3, 8, EmbeddingLocation.DEVICE, ComputeDevice.CUDA),</span>
<span class="sd">            &gt;&gt;&gt;     (5, 4, EmbeddingLocation.MANAGED, ComputeDevice.CUDA)</span>
<span class="sd">            &gt;&gt;&gt; ]</span>
<span class="sd">            &gt;&gt;&gt;</span>
<span class="sd">            &gt;&gt;&gt; tbe = SplitTableBatchedEmbeddingBagsCodegen(embedding_specs)</span>
<span class="sd">            &gt;&gt;&gt; tbe.init_embedding_weights_uniform(-1, 1)</span>
<span class="sd">            &gt;&gt;&gt;</span>
<span class="sd">            &gt;&gt;&gt; print(tbe.split_embedding_weights())</span>
<span class="sd">            [tensor([[-0.9426,  0.7046,  0.4214, -0.0419,  0.1331, -0.7856, -0.8124, -0.2021],</span>
<span class="sd">                    [-0.5771,  0.5911, -0.7792, -0.1068, -0.6203,  0.4813, -0.1677,  0.4790],</span>
<span class="sd">                    [-0.5587, -0.0941,  0.5754,  0.3475, -0.8952, -0.1964,  0.0810, -0.4174]],</span>
<span class="sd">                   device=&#39;cuda:0&#39;), tensor([[-0.2513, -0.4039, -0.3775,  0.3273],</span>
<span class="sd">                    [-0.5399, -0.0229, -0.1455, -0.8770],</span>
<span class="sd">                    [-0.9520,  0.4593, -0.7169,  0.6307],</span>
<span class="sd">                    [-0.1765,  0.8757,  0.8614,  0.2051],</span>
<span class="sd">                    [-0.0603, -0.9980, -0.7958, -0.5826]], device=&#39;cuda:0&#39;)]</span>


<span class="sd">            &gt;&gt;&gt; # Batch size = 3</span>
<span class="sd">            &gt;&gt;&gt; indices = torch.tensor([0, 1, 2, 0, 1, 2, 0, 3, 1, 4, 2, 0, 0],</span>
<span class="sd">            &gt;&gt;&gt;                        device=&quot;cuda&quot;,</span>
<span class="sd">            &gt;&gt;&gt;                        dtype=torch.long)</span>
<span class="sd">            &gt;&gt;&gt; offsets = torch.tensor([0, 2, 5, 7, 9, 12, 13],</span>
<span class="sd">            &gt;&gt;&gt;                        device=&quot;cuda&quot;,</span>
<span class="sd">            &gt;&gt;&gt;                        dtype=torch.long)</span>
<span class="sd">            &gt;&gt;&gt;</span>
<span class="sd">            &gt;&gt;&gt; output = tbe(indices, offsets)</span>
<span class="sd">            &gt;&gt;&gt;</span>
<span class="sd">            &gt;&gt;&gt; # Batch size = 3, total embedding dimension = 12</span>
<span class="sd">            &gt;&gt;&gt; print(output.shape)</span>
<span class="sd">            torch.Size([3, 12])</span>

<span class="sd">            &gt;&gt;&gt; print(output)</span>
<span class="sd">            tensor([[-1.5197,  1.2957, -0.3578, -0.1487, -0.4873, -0.3044, -0.9801,  0.2769,</span>
<span class="sd">                     -0.7164,  0.8528,  0.7159, -0.6719],</span>
<span class="sd">                    [-2.0784,  1.2016,  0.2176,  0.1988, -1.3825, -0.5008, -0.8991, -0.1405,</span>
<span class="sd">                     -1.2637, -0.9427, -1.8902,  0.3754],</span>
<span class="sd">                    [-1.5013,  0.6105,  0.9968,  0.3057, -0.7621, -0.9821, -0.7314, -0.6195,</span>
<span class="sd">                     -0.2513, -0.4039, -0.3775,  0.3273]], device=&#39;cuda:0&#39;,</span>
<span class="sd">                   grad_fn=&lt;CppNode&lt;SplitLookupFunction_sgd_Op&gt;&gt;)</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="p">(</span>
            <span class="n">indices</span><span class="p">,</span>
            <span class="n">offsets</span><span class="p">,</span>
            <span class="n">per_sample_weights</span><span class="p">,</span>
            <span class="n">vbe_metadata</span><span class="p">,</span>
        <span class="p">)</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">prepare_inputs</span><span class="p">(</span>
            <span class="n">indices</span><span class="p">,</span>
            <span class="n">offsets</span><span class="p">,</span>
            <span class="n">per_sample_weights</span><span class="p">,</span>
            <span class="n">batch_size_per_feature_per_rank</span><span class="p">,</span>
            <span class="n">force_cast_input_types</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="n">prefetch_pipeline</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="c1"># Print input stats if enable (for debugging purpose only)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_debug_print_input_stats</span><span class="p">(</span><span class="n">indices</span><span class="p">,</span> <span class="n">offsets</span><span class="p">,</span> <span class="n">per_sample_weights</span><span class="p">)</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="n">is_torchdynamo_compiling</span><span class="p">():</span>
            <span class="c1"># Mutations of nn.Module attr forces dynamo restart of Analysis which increases compilation time</span>

            <span class="c1"># Storing tensors for linear_cache_indices recomputation</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_indices</span> <span class="o">=</span> <span class="n">indices</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_offsets</span> <span class="o">=</span> <span class="n">offsets</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_vbe_B_offsets</span> <span class="o">=</span> <span class="n">vbe_metadata</span><span class="o">.</span><span class="n">B_offsets</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_vbe_max_B</span> <span class="o">=</span> <span class="n">vbe_metadata</span><span class="o">.</span><span class="n">max_B</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">step</span> <span class="o">+=</span> <span class="mi">1</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_report_io_size_count</span><span class="p">(</span><span class="s2">&quot;fwd_input&quot;</span><span class="p">,</span> <span class="n">indices</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_report_tbe_mem_usage</span><span class="p">()</span>

            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">tbe_input_multiplexer</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">tbe_input_multiplexer</span><span class="p">:</span> <span class="n">TBEInputMultiplexer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tbe_input_multiplexer</span>
                <span class="k">if</span> <span class="n">tbe_input_multiplexer</span><span class="o">.</span><span class="n">should_run</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">step</span><span class="p">):</span>
                    <span class="n">tbe_input_multiplexer</span><span class="o">.</span><span class="n">run</span><span class="p">(</span>
                        <span class="n">tbe_input_info</span><span class="o">=</span><span class="n">TBEInputInfo</span><span class="p">(</span>
                            <span class="n">indices</span><span class="p">,</span> <span class="n">offsets</span><span class="p">,</span> <span class="n">batch_size_per_feature_per_rank</span>
                        <span class="p">)</span>
                    <span class="p">)</span>

        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">timesteps_prefetched</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="c1"># In forward, we don&#39;t enable multi-pass prefetch as we want the process</span>
            <span class="c1"># to be as fast as possible and memory usage doesn&#39;t matter (will be recycled</span>
            <span class="c1"># by dense fwd/bwd)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_prefetch</span><span class="p">(</span>
                <span class="n">indices</span><span class="p">,</span> <span class="n">offsets</span><span class="p">,</span> <span class="n">vbe_metadata</span><span class="p">,</span> <span class="n">multipass_prefetch_config</span><span class="o">=</span><span class="kc">None</span>
            <span class="p">)</span>

        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">timesteps_prefetched</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">timesteps_prefetched</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">lxu_cache_locations</span> <span class="o">=</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">lxu_cache_locations_empty</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">lxu_cache_locations_list</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span>
            <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">lxu_cache_locations_list</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="n">common_args</span> <span class="o">=</span> <span class="n">invokers</span><span class="o">.</span><span class="n">lookup_args</span><span class="o">.</span><span class="n">CommonArgs</span><span class="p">(</span>
            <span class="n">placeholder_autograd_tensor</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">placeholder_autograd_tensor</span><span class="p">,</span>
            <span class="c1"># pyre-fixme[6]: For 2nd argument expected `Tensor` but got</span>
            <span class="c1">#  `Union[Module, Tensor]`.</span>
            <span class="n">dev_weights</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">weights_dev</span><span class="p">,</span>
            <span class="c1"># pyre-fixme[6]: For 3rd argument expected `Tensor` but got</span>
            <span class="c1">#  `Union[Module, Tensor]`.</span>
            <span class="n">host_weights</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">weights_host</span><span class="p">,</span>
            <span class="c1"># pyre-fixme[6]: For 4th argument expected `Tensor` but got</span>
            <span class="c1">#  `Union[Module, Tensor]`.</span>
            <span class="n">uvm_weights</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">weights_uvm</span><span class="p">,</span>
            <span class="c1"># pyre-fixme[6]: For 5th argument expected `Tensor` but got</span>
            <span class="c1">#  `Union[Module, Tensor]`.</span>
            <span class="n">lxu_cache_weights</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">lxu_cache_weights</span><span class="p">,</span>
            <span class="c1"># pyre-fixme[6]: For 6th argument expected `Tensor` but got</span>
            <span class="c1">#  `Union[Module, Tensor]`.</span>
            <span class="n">weights_placements</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">weights_placements</span><span class="p">,</span>
            <span class="c1"># pyre-fixme[6]: For 7th argument expected `Tensor` but got</span>
            <span class="c1">#  `Union[Module, Tensor]`.</span>
            <span class="n">weights_offsets</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">weights_offsets</span><span class="p">,</span>
            <span class="n">D_offsets</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">D_offsets</span><span class="p">,</span>
            <span class="n">total_D</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">total_D</span><span class="p">,</span>
            <span class="n">max_D</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">max_D</span><span class="p">,</span>
            <span class="n">hash_size_cumsum</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">hash_size_cumsum</span><span class="p">,</span>
            <span class="n">total_hash_size_bits</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">total_hash_size_bits</span><span class="p">,</span>
            <span class="n">indices</span><span class="o">=</span><span class="n">indices</span><span class="p">,</span>
            <span class="n">offsets</span><span class="o">=</span><span class="n">offsets</span><span class="p">,</span>
            <span class="n">pooling_mode</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">pooling_mode</span><span class="p">,</span>
            <span class="n">indice_weights</span><span class="o">=</span><span class="n">per_sample_weights</span><span class="p">,</span>
            <span class="n">feature_requires_grad</span><span class="o">=</span><span class="n">feature_requires_grad</span><span class="p">,</span>
            <span class="n">lxu_cache_locations</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">lxu_cache_locations</span><span class="p">,</span>
            <span class="c1"># Pass the local_uvm_cache_stats bc only that information is</span>
            <span class="c1"># relevant for the current iteration</span>
            <span class="n">uvm_cache_stats</span><span class="o">=</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">local_uvm_cache_stats</span>
                <span class="k">if</span> <span class="p">(</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">gather_uvm_cache_stats</span>
                    <span class="c1"># Unique conflict misses are only collected when using CacheAlgorithm.LRU</span>
                    <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">cache_algorithm</span> <span class="o">==</span> <span class="n">CacheAlgorithm</span><span class="o">.</span><span class="n">LRU</span>
                <span class="p">)</span>
                <span class="k">else</span> <span class="kc">None</span>
            <span class="p">),</span>
            <span class="n">output_dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">output_dtype</span><span class="p">,</span>
            <span class="n">vbe_metadata</span><span class="o">=</span><span class="n">vbe_metadata</span><span class="p">,</span>
            <span class="n">is_experimental</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">is_experimental</span><span class="p">,</span>
            <span class="n">use_uniq_cache_locations_bwd</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">use_uniq_cache_locations_bwd</span><span class="p">,</span>
            <span class="n">use_homogeneous_placements</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">use_homogeneous_placements</span><span class="p">,</span>
            <span class="n">learning_rate_tensor</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">learning_rate_tensor</span><span class="p">,</span>
            <span class="n">info_B_num_bits</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">info_B_num_bits</span><span class="p">,</span>
            <span class="n">info_B_mask</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">info_B_mask</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span> <span class="o">==</span> <span class="n">OptimType</span><span class="o">.</span><span class="n">NONE</span><span class="p">:</span>
            <span class="k">assert</span> <span class="p">(</span>
                <span class="n">total_unique_indices</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
                <span class="ow">and</span> <span class="n">total_unique_indices</span> <span class="o">&lt;=</span> <span class="n">indices</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span>
            <span class="p">),</span> <span class="sa">f</span><span class="s2">&quot;OptimType.NONE requires total_unique_indices. Please pass it or check the value (total_unique_indices = </span><span class="si">{</span><span class="n">total_unique_indices</span><span class="si">}</span><span class="s2">)&quot;</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_report_io_size_count</span><span class="p">(</span>
                <span class="s2">&quot;fwd_output&quot;</span><span class="p">,</span>
                <span class="n">invokers</span><span class="o">.</span><span class="n">lookup_none</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span>
                    <span class="n">common_args</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimizer_args</span><span class="p">,</span> <span class="n">total_unique_indices</span>
                <span class="p">),</span>
            <span class="p">)</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span> <span class="o">==</span> <span class="n">OptimType</span><span class="o">.</span><span class="n">EXACT_SGD</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_report_io_size_count</span><span class="p">(</span>
                <span class="s2">&quot;fwd_output&quot;</span><span class="p">,</span>
                <span class="n">invokers</span><span class="o">.</span><span class="n">lookup_sgd</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span><span class="n">common_args</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimizer_args</span><span class="p">),</span>
            <span class="p">)</span>

        <span class="n">momentum1</span> <span class="o">=</span> <span class="n">invokers</span><span class="o">.</span><span class="n">lookup_args</span><span class="o">.</span><span class="n">Momentum</span><span class="p">(</span>
            <span class="c1"># pyre-fixme[6]: For 1st argument expected `Tensor` but got</span>
            <span class="c1">#  `Union[Module, Tensor]`.</span>
            <span class="n">dev</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">momentum1_dev</span><span class="p">,</span>
            <span class="c1"># pyre-fixme[6]: For 2nd argument expected `Tensor` but got</span>
            <span class="c1">#  `Union[Module, Tensor]`.</span>
            <span class="n">host</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">momentum1_host</span><span class="p">,</span>
            <span class="c1"># pyre-fixme[6]: For 3rd argument expected `Tensor` but got</span>
            <span class="c1">#  `Union[Module, Tensor]`.</span>
            <span class="n">uvm</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">momentum1_uvm</span><span class="p">,</span>
            <span class="c1"># pyre-fixme[6]: For 4th argument expected `Tensor` but got</span>
            <span class="c1">#  `Union[Module, Tensor]`.</span>
            <span class="n">offsets</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">momentum1_offsets</span><span class="p">,</span>
            <span class="c1"># pyre-fixme[6]: For 5th argument expected `Tensor` but got</span>
            <span class="c1">#  `Union[Module, Tensor]`.</span>
            <span class="n">placements</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">momentum1_placements</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span> <span class="o">==</span> <span class="n">OptimType</span><span class="o">.</span><span class="n">LARS_SGD</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_report_io_size_count</span><span class="p">(</span>
                <span class="s2">&quot;fwd_output&quot;</span><span class="p">,</span>
                <span class="n">invokers</span><span class="o">.</span><span class="n">lookup_lars_sgd</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span>
                    <span class="n">common_args</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimizer_args</span><span class="p">,</span> <span class="n">momentum1</span>
                <span class="p">),</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span> <span class="o">==</span> <span class="n">OptimType</span><span class="o">.</span><span class="n">EXACT_ADAGRAD</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_report_io_size_count</span><span class="p">(</span>
                <span class="s2">&quot;fwd_output&quot;</span><span class="p">,</span>
                <span class="n">invokers</span><span class="o">.</span><span class="n">lookup_adagrad</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span>
                    <span class="n">common_args</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimizer_args</span><span class="p">,</span> <span class="n">momentum1</span>
                <span class="p">),</span>
            <span class="p">)</span>

        <span class="n">momentum2</span> <span class="o">=</span> <span class="n">invokers</span><span class="o">.</span><span class="n">lookup_args</span><span class="o">.</span><span class="n">Momentum</span><span class="p">(</span>
            <span class="c1"># pyre-fixme[6]: For 1st argument expected `Tensor` but got</span>
            <span class="c1">#  `Union[Module, Tensor]`.</span>
            <span class="n">dev</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">momentum2_dev</span><span class="p">,</span>
            <span class="c1"># pyre-fixme[6]: For 2nd argument expected `Tensor` but got</span>
            <span class="c1">#  `Union[Module, Tensor]`.</span>
            <span class="n">host</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">momentum2_host</span><span class="p">,</span>
            <span class="c1"># pyre-fixme[6]: For 3rd argument expected `Tensor` but got</span>
            <span class="c1">#  `Union[Module, Tensor]`.</span>
            <span class="n">uvm</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">momentum2_uvm</span><span class="p">,</span>
            <span class="c1"># pyre-fixme[6]: For 4th argument expected `Tensor` but got</span>
            <span class="c1">#  `Union[Module, Tensor]`.</span>
            <span class="n">offsets</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">momentum2_offsets</span><span class="p">,</span>
            <span class="c1"># pyre-fixme[6]: For 5th argument expected `Tensor` but got</span>
            <span class="c1">#  `Union[Module, Tensor]`.</span>
            <span class="n">placements</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">momentum2_placements</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="c1"># Although self.iter_cpu is created on CPU. It might be transferred to</span>
        <span class="c1"># GPU by the user. So, we need to transfer it to CPU explicitly. This</span>
        <span class="c1"># should be done only once.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">iter_cpu</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">iter_cpu</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span>

        <span class="c1"># Sync with loaded state</span>
        <span class="k">if</span> <span class="p">(</span>
            <span class="ow">not</span> <span class="n">is_torchdynamo_compiling</span><span class="p">()</span>
        <span class="p">):</span>  <span class="c1"># wrap to make it compatible with PT2 compile</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">iter_cpu</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">iter_cpu</span><span class="o">.</span><span class="n">fill_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">iter</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
        <span class="c1"># Increment the iteration counter</span>
        <span class="n">iter_int</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">iter_cpu</span><span class="o">.</span><span class="n">add_</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>  <span class="c1"># used for local computation</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">iter</span><span class="o">.</span><span class="n">add_</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># used for checkpointing</span>

        <span class="n">row_counter</span> <span class="o">=</span> <span class="n">invokers</span><span class="o">.</span><span class="n">lookup_args</span><span class="o">.</span><span class="n">Momentum</span><span class="p">(</span>
            <span class="c1"># pyre-fixme[6]: For 1st argument expected `Tensor` but got</span>
            <span class="c1">#  `Union[Module, Tensor]`.</span>
            <span class="n">dev</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">row_counter_dev</span><span class="p">,</span>
            <span class="c1"># pyre-fixme[6]: For 2nd argument expected `Tensor` but got</span>
            <span class="c1">#  `Union[Module, Tensor]`.</span>
            <span class="n">host</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">row_counter_host</span><span class="p">,</span>
            <span class="c1"># pyre-fixme[6]: For 3rd argument expected `Tensor` but got</span>
            <span class="c1">#  `Union[Module, Tensor]`.</span>
            <span class="n">uvm</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">row_counter_uvm</span><span class="p">,</span>
            <span class="c1"># pyre-fixme[6]: For 4th argument expected `Tensor` but got</span>
            <span class="c1">#  `Union[Module, Tensor]`.</span>
            <span class="n">offsets</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">row_counter_offsets</span><span class="p">,</span>
            <span class="c1"># pyre-fixme[6]: For 5th argument expected `Tensor` but got</span>
            <span class="c1">#  `Union[Module, Tensor]`.</span>
            <span class="n">placements</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">row_counter_placements</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span> <span class="o">==</span> <span class="n">OptimType</span><span class="o">.</span><span class="n">ADAM</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_report_io_size_count</span><span class="p">(</span>
                <span class="s2">&quot;fwd_output&quot;</span><span class="p">,</span>
                <span class="n">invokers</span><span class="o">.</span><span class="n">lookup_adam</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span>
                    <span class="n">common_args</span><span class="p">,</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">optimizer_args</span><span class="p">,</span>
                    <span class="n">momentum1</span><span class="p">,</span>
                    <span class="n">momentum2</span><span class="p">,</span>
                    <span class="n">iter_int</span><span class="p">,</span>
                    <span class="n">row_counter</span><span class="o">=</span><span class="p">(</span>
                        <span class="n">row_counter</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_rowwise_bias_correction</span> <span class="k">else</span> <span class="kc">None</span>
                    <span class="p">),</span>
                <span class="p">),</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span> <span class="o">==</span> <span class="n">OptimType</span><span class="o">.</span><span class="n">PARTIAL_ROWWISE_ADAM</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_report_io_size_count</span><span class="p">(</span>
                <span class="s2">&quot;fwd_output&quot;</span><span class="p">,</span>
                <span class="n">invokers</span><span class="o">.</span><span class="n">lookup_partial_rowwise_adam</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span>
                    <span class="n">common_args</span><span class="p">,</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">optimizer_args</span><span class="p">,</span>
                    <span class="n">momentum1</span><span class="p">,</span>
                    <span class="n">momentum2</span><span class="p">,</span>
                    <span class="n">iter_int</span><span class="p">,</span>
                <span class="p">),</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span> <span class="o">==</span> <span class="n">OptimType</span><span class="o">.</span><span class="n">LAMB</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_report_io_size_count</span><span class="p">(</span>
                <span class="s2">&quot;fwd_output&quot;</span><span class="p">,</span>
                <span class="n">invokers</span><span class="o">.</span><span class="n">lookup_lamb</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span>
                    <span class="n">common_args</span><span class="p">,</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">optimizer_args</span><span class="p">,</span>
                    <span class="n">momentum1</span><span class="p">,</span>
                    <span class="n">momentum2</span><span class="p">,</span>
                    <span class="n">iter_int</span><span class="p">,</span>
                <span class="p">),</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span> <span class="o">==</span> <span class="n">OptimType</span><span class="o">.</span><span class="n">PARTIAL_ROWWISE_LAMB</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_report_io_size_count</span><span class="p">(</span>
                <span class="s2">&quot;fwd_output&quot;</span><span class="p">,</span>
                <span class="n">invokers</span><span class="o">.</span><span class="n">lookup_partial_rowwise_lamb</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span>
                    <span class="n">common_args</span><span class="p">,</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">optimizer_args</span><span class="p">,</span>
                    <span class="n">momentum1</span><span class="p">,</span>
                    <span class="n">momentum2</span><span class="p">,</span>
                    <span class="n">iter_int</span><span class="p">,</span>
                <span class="p">),</span>
            <span class="p">)</span>

        <span class="n">prev_iter</span> <span class="o">=</span> <span class="n">invokers</span><span class="o">.</span><span class="n">lookup_args</span><span class="o">.</span><span class="n">Momentum</span><span class="p">(</span>
            <span class="c1"># pyre-fixme[6]: For 1st argument expected `Tensor` but got</span>
            <span class="c1">#  `Union[Module, Tensor]`.</span>
            <span class="n">dev</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">prev_iter_dev</span><span class="p">,</span>
            <span class="c1"># pyre-fixme[6]: For 2nd argument expected `Tensor` but got</span>
            <span class="c1">#  `Union[Module, Tensor]`.</span>
            <span class="n">host</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">prev_iter_host</span><span class="p">,</span>
            <span class="c1"># pyre-fixme[6]: For 3rd argument expected `Tensor` but got</span>
            <span class="c1">#  `Union[Module, Tensor]`.</span>
            <span class="n">uvm</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">prev_iter_uvm</span><span class="p">,</span>
            <span class="c1"># pyre-fixme[6]: For 4th argument expected `Tensor` but got</span>
            <span class="c1">#  `Union[Module, Tensor]`.</span>
            <span class="n">offsets</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">prev_iter_offsets</span><span class="p">,</span>
            <span class="c1"># pyre-fixme[6]: For 5th argument expected `Tensor` but got</span>
            <span class="c1">#  `Union[Module, Tensor]`.</span>
            <span class="n">placements</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">prev_iter_placements</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span> <span class="o">==</span> <span class="n">OptimType</span><span class="o">.</span><span class="n">EMAINPLACE_ROWWISE_ADAGRAD</span><span class="p">:</span>
            <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">training</span><span class="p">:</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">ema_inplace</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_emainplace_mode</span><span class="p">)</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_report_io_size_count</span><span class="p">(</span>
                <span class="s2">&quot;fwd_output&quot;</span><span class="p">,</span>
                <span class="n">invokers</span><span class="o">.</span><span class="n">lookup_rowwise_adagrad</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span>
                    <span class="n">common_args</span><span class="p">,</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">optimizer_args</span><span class="p">,</span>
                    <span class="n">momentum1</span><span class="p">,</span>
                <span class="p">),</span>
            <span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span> <span class="o">==</span> <span class="n">OptimType</span><span class="o">.</span><span class="n">ENSEMBLE_ROWWISE_ADAGRAD</span><span class="p">:</span>
            <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">_feature_is_enabled</span><span class="p">(</span>
                <span class="n">FeatureGateName</span><span class="o">.</span><span class="n">TBE_ENSEMBLE_ROWWISE_ADAGRAD</span>
            <span class="p">),</span> <span class="s2">&quot;ENSEMBLE_ROWWISE_ADAGRAD is an inactive or deprecated feature!&quot;</span>
            <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">training</span><span class="p">:</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">ensemble_and_swap</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_ensemble_mode</span><span class="p">)</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_report_io_size_count</span><span class="p">(</span>
                <span class="s2">&quot;fwd_output&quot;</span><span class="p">,</span>
                <span class="n">invokers</span><span class="o">.</span><span class="n">lookup_rowwise_adagrad</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span>
                    <span class="n">common_args</span><span class="p">,</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">optimizer_args</span><span class="p">,</span>
                    <span class="n">momentum1</span><span class="p">,</span>
                <span class="p">),</span>
            <span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_used_rowwise_adagrad_with_counter</span><span class="p">:</span>
            <span class="k">if</span> <span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_max_counter_update_freq</span> <span class="o">&gt;</span> <span class="mi">0</span>
                <span class="ow">and</span> <span class="n">iter_int</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">_max_counter_update_freq</span> <span class="o">==</span> <span class="mi">0</span>
            <span class="p">):</span>
                <span class="n">row_counter_dev</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">row_counter_dev</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>
                <span class="k">if</span> <span class="n">row_counter_dev</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">max_counter</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">row_counter_dev</span><span class="p">)</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="o">+</span> <span class="mi">1</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">max_counter</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span> <span class="o">==</span> <span class="n">OptimType</span><span class="o">.</span><span class="n">EXACT_ROWWISE_ADAGRAD</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_used_rowwise_adagrad_with_counter</span><span class="p">:</span>
                <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_report_io_size_count</span><span class="p">(</span>
                    <span class="s2">&quot;fwd_output&quot;</span><span class="p">,</span>
                    <span class="n">invokers</span><span class="o">.</span><span class="n">lookup_rowwise_adagrad_with_counter</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span>
                        <span class="n">common_args</span><span class="p">,</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">optimizer_args</span><span class="p">,</span>
                        <span class="n">momentum1</span><span class="p">,</span>
                        <span class="n">prev_iter</span><span class="p">,</span>
                        <span class="n">row_counter</span><span class="p">,</span>
                        <span class="n">iter_int</span><span class="p">,</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">max_counter</span><span class="o">.</span><span class="n">item</span><span class="p">(),</span>
                    <span class="p">),</span>
                <span class="p">)</span>
            <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">_used_rowwise_adagrad_with_global_weight_decay</span><span class="p">:</span>
                <span class="n">apply_global_weight_decay</span> <span class="o">=</span> <span class="p">(</span>
                    <span class="n">iter_int</span> <span class="o">&gt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">gwd_start_iter</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">training</span>
                <span class="p">)</span>
                <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_report_io_size_count</span><span class="p">(</span>
                    <span class="s2">&quot;fwd_output&quot;</span><span class="p">,</span>
                    <span class="n">invokers</span><span class="o">.</span><span class="n">lookup_rowwise_adagrad</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span>
                        <span class="n">common_args</span><span class="p">,</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">optimizer_args</span><span class="p">,</span>
                        <span class="n">momentum1</span><span class="p">,</span>
                        <span class="nb">iter</span><span class="o">=</span><span class="n">iter_int</span><span class="p">,</span>
                        <span class="n">apply_global_weight_decay</span><span class="o">=</span><span class="n">apply_global_weight_decay</span><span class="p">,</span>
                        <span class="c1"># pyre-fixme[6]: For 6th argument expected</span>
                        <span class="c1">#  `Optional[Tensor]` but got `Union[Module, Tensor]`.</span>
                        <span class="n">prev_iter_dev</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">prev_iter_dev</span><span class="p">,</span>
                        <span class="n">gwd_lower_bound</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">gwd_lower_bound</span><span class="p">,</span>
                    <span class="p">),</span>
                <span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_report_io_size_count</span><span class="p">(</span>
                    <span class="s2">&quot;fwd_output&quot;</span><span class="p">,</span>
                    <span class="n">invokers</span><span class="o">.</span><span class="n">lookup_rowwise_adagrad</span><span class="o">.</span><span class="n">invoke</span><span class="p">(</span>
                        <span class="n">common_args</span><span class="p">,</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">optimizer_args</span><span class="p">,</span>
                        <span class="n">momentum1</span><span class="p">,</span>
                    <span class="p">),</span>
                <span class="p">)</span>

        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Invalid OptimType: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span></div>

    <span class="k">def</span><span class="w"> </span><span class="nf">ema_inplace</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">emainplace_mode</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">float</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Perform ema operations on the full sparse embedding tables.</span>
<span class="sd">        We organize the sparse table, in the following way.</span>

<span class="sd">        Emb_table:</span>
<span class="sd">         -------------------------------------------------</span>
<span class="sd">         -                        --                     -</span>
<span class="sd">         -        Fast part       --      Slow part      -</span>
<span class="sd">         -    (RL) main part      --      target part    -</span>
<span class="sd">         -                        --                     -</span>
<span class="sd">         -------------------------------------------------</span>

<span class="sd">         In every &quot;step_ema&quot; step, we perform</span>
<span class="sd">            slow_part += coef_ema * (fast_part - slow_part)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">iter_int</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">iter_cpu</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
        <span class="k">if</span> <span class="n">iter_int</span> <span class="o">%</span> <span class="nb">int</span><span class="p">(</span><span class="n">emainplace_mode</span><span class="p">[</span><span class="s2">&quot;step_ema&quot;</span><span class="p">])</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">iter_int</span> <span class="o">&gt;=</span> <span class="nb">int</span><span class="p">(</span>
            <span class="n">emainplace_mode</span><span class="p">[</span><span class="s2">&quot;step_start&quot;</span><span class="p">]</span>
        <span class="p">):</span>
            <span class="n">weights</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">split_embedding_weights</span><span class="p">()</span>
            <span class="k">for</span> <span class="n">table_i</span><span class="p">,</span> <span class="p">(</span><span class="n">_</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">embedding_specs</span><span class="p">):</span>
                <span class="k">assert</span> <span class="p">(</span>
                    <span class="n">dim</span> <span class="o">&amp;</span> <span class="mi">1</span> <span class="o">==</span> <span class="mi">0</span>
                <span class="p">),</span> <span class="sa">f</span><span class="s2">&quot;table dimension </span><span class="si">{</span><span class="n">dim</span><span class="si">}</span><span class="s2"> is odd, not supported for ema_inplace_rowwise_adagrad&quot;</span>  <span class="c1"># make sure that the dimension is even</span>
                <span class="n">weights</span><span class="p">[</span><span class="n">table_i</span><span class="p">][:,</span> <span class="n">dim</span> <span class="o">//</span> <span class="mi">2</span> <span class="p">:]</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">lerp_</span><span class="p">(</span>
                    <span class="n">weights</span><span class="p">[</span><span class="n">table_i</span><span class="p">][:,</span> <span class="p">:</span> <span class="n">dim</span> <span class="o">//</span> <span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">data</span><span class="p">,</span>
                    <span class="n">emainplace_mode</span><span class="p">[</span><span class="s2">&quot;step_ema_coef&quot;</span><span class="p">],</span>
                <span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">ensemble_and_swap</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">ensemble_mode</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">float</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Perform ensemble and swap operations on the full sparse embedding tables.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Sparse embedding weights and optimizer states will be updated in-place.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">iter_int</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">iter_cpu</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
        <span class="n">should_ema</span> <span class="o">=</span> <span class="n">iter_int</span> <span class="o">%</span> <span class="nb">int</span><span class="p">(</span><span class="n">ensemble_mode</span><span class="p">[</span><span class="s2">&quot;step_ema&quot;</span><span class="p">])</span> <span class="o">==</span> <span class="mi">0</span>
        <span class="n">should_swap</span> <span class="o">=</span> <span class="n">iter_int</span> <span class="o">%</span> <span class="nb">int</span><span class="p">(</span><span class="n">ensemble_mode</span><span class="p">[</span><span class="s2">&quot;step_swap&quot;</span><span class="p">])</span> <span class="o">==</span> <span class="mi">0</span>
        <span class="k">if</span> <span class="n">should_ema</span> <span class="ow">or</span> <span class="n">should_swap</span><span class="p">:</span>
            <span class="n">weights</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">split_embedding_weights</span><span class="p">()</span>
            <span class="n">states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">split_optimizer_states</span><span class="p">()</span>
            <span class="n">coef_ema</span> <span class="o">=</span> <span class="p">(</span>
                <span class="mf">0.0</span>
                <span class="k">if</span> <span class="n">iter_int</span> <span class="o">&lt;=</span> <span class="nb">int</span><span class="p">(</span><span class="n">ensemble_mode</span><span class="p">[</span><span class="s2">&quot;step_start&quot;</span><span class="p">])</span>
                <span class="k">else</span> <span class="n">ensemble_mode</span><span class="p">[</span><span class="s2">&quot;step_ema_coef&quot;</span><span class="p">]</span>
            <span class="p">)</span>
            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">embedding_specs</span><span class="p">)):</span>
                <span class="c1"># 0) copying weights from gpu to cpu</span>
                <span class="n">weights_cpu</span> <span class="o">=</span> <span class="n">weights</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span>
                    <span class="n">dtype</span><span class="o">=</span><span class="n">states</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">states</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">device</span>
                <span class="p">)</span>
                <span class="c1"># 1) ema step</span>
                <span class="k">if</span> <span class="n">should_ema</span><span class="p">:</span>
                    <span class="n">states</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">lerp_</span><span class="p">(</span><span class="n">weights_cpu</span><span class="p">,</span> <span class="mf">1.0</span> <span class="o">-</span> <span class="n">coef_ema</span><span class="p">)</span>
                <span class="c1"># 2) swap step</span>
                <span class="k">if</span> <span class="n">should_swap</span><span class="p">:</span>
                    <span class="n">weights</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">copy_</span><span class="p">(</span><span class="n">states</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">1</span><span class="p">],</span> <span class="n">non_blocking</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
                <span class="c1"># 3) post-processing step</span>
                <span class="k">if</span> <span class="n">should_ema</span><span class="p">:</span>
                    <span class="k">if</span> <span class="nb">int</span><span class="p">(</span><span class="n">ensemble_mode</span><span class="p">[</span><span class="s2">&quot;step_mode&quot;</span><span class="p">])</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>  <span class="c1"># embedding scaling</span>
                        <span class="n">states</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">mul_</span><span class="p">(</span><span class="mf">0.0</span><span class="p">)</span>
                <span class="c1">#  elif int(ensemble_mode[&quot;step_mode&quot;]) == 2:  pure ema</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">reset_uvm_cache_stats</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">assert</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">gather_uvm_cache_stats</span>
        <span class="p">),</span> <span class="s2">&quot;gather_uvm_cache_stats should be set to true to access uvm cache stats.&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">uvm_cache_stats</span><span class="o">.</span><span class="n">zero_</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">local_uvm_cache_stats</span><span class="o">.</span><span class="n">zero_</span><span class="p">()</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">get_uvm_cache_stats</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">use_local_cache</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="k">assert</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">gather_uvm_cache_stats</span>
        <span class="p">),</span> <span class="s2">&quot;gather_uvm_cache_stats should be set to true to access uvm cache stats.&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">local_uvm_cache_stats</span> <span class="k">if</span> <span class="n">use_local_cache</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">uvm_cache_stats</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_get_uvm_cache_print_state</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">use_local_cache</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="nb">float</span><span class="p">]:</span>
        <span class="n">snapshot</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_uvm_cache_stats</span><span class="p">(</span><span class="n">use_local_cache</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">use_local_cache</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">snapshot</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>

        <span class="c1"># Stats are accumulated over multiple steps.  Compute delta, and update state.</span>
        <span class="n">delta</span> <span class="o">=</span> <span class="n">snapshot</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">last_uvm_cache_print_state</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">last_uvm_cache_print_state</span> <span class="o">=</span> <span class="n">snapshot</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">delta</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>

    <span class="nd">@torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">ignore</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">print_uvm_cache_stats</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">use_local_cache</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="c1"># TODO: Create a separate reporter class to unify the stdlog reporting</span>
        <span class="n">uvm_cache_stats</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_uvm_cache_print_state</span><span class="p">(</span><span class="n">use_local_cache</span><span class="p">)</span>
        <span class="n">N</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">uvm_cache_stats</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
        <span class="n">m</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s2">&quot;N_called&quot;</span><span class="p">:</span> <span class="n">uvm_cache_stats</span><span class="p">[</span><span class="n">UVMCacheStatsIndex</span><span class="o">.</span><span class="n">num_calls</span><span class="p">],</span>
            <span class="s2">&quot;requested_indices&quot;</span><span class="p">:</span> <span class="n">uvm_cache_stats</span><span class="p">[</span>
                <span class="n">UVMCacheStatsIndex</span><span class="o">.</span><span class="n">num_requested_indices</span>
            <span class="p">]</span>
            <span class="o">/</span> <span class="n">N</span><span class="p">,</span>
            <span class="s2">&quot;unique_indices&quot;</span><span class="p">:</span> <span class="n">uvm_cache_stats</span><span class="p">[</span><span class="n">UVMCacheStatsIndex</span><span class="o">.</span><span class="n">num_unique_indices</span><span class="p">]</span>
            <span class="o">/</span> <span class="n">N</span><span class="p">,</span>
            <span class="s2">&quot;unique_misses&quot;</span><span class="p">:</span> <span class="n">uvm_cache_stats</span><span class="p">[</span><span class="n">UVMCacheStatsIndex</span><span class="o">.</span><span class="n">num_unique_misses</span><span class="p">]</span> <span class="o">/</span> <span class="n">N</span><span class="p">,</span>
            <span class="s2">&quot;conflict_unique_misses&quot;</span><span class="p">:</span> <span class="n">uvm_cache_stats</span><span class="p">[</span>
                <span class="n">UVMCacheStatsIndex</span><span class="o">.</span><span class="n">num_conflict_unique_misses</span>
            <span class="p">]</span>
            <span class="o">/</span> <span class="n">N</span><span class="p">,</span>
            <span class="s2">&quot;conflict_misses&quot;</span><span class="p">:</span> <span class="n">uvm_cache_stats</span><span class="p">[</span><span class="n">UVMCacheStatsIndex</span><span class="o">.</span><span class="n">num_conflict_misses</span><span class="p">]</span>
            <span class="o">/</span> <span class="n">N</span><span class="p">,</span>
        <span class="p">}</span>
        <span class="k">if</span> <span class="n">uvm_cache_stats</span><span class="p">[</span><span class="mi">1</span><span class="p">]:</span>
            <span class="n">m</span><span class="o">.</span><span class="n">update</span><span class="p">(</span>
                <span class="p">{</span>
                    <span class="s2">&quot;unique indices / requested indices&quot;</span><span class="p">:</span> <span class="n">uvm_cache_stats</span><span class="p">[</span>
                        <span class="n">UVMCacheStatsIndex</span><span class="o">.</span><span class="n">num_unique_indices</span>
                    <span class="p">]</span>
                    <span class="o">/</span> <span class="n">uvm_cache_stats</span><span class="p">[</span><span class="n">UVMCacheStatsIndex</span><span class="o">.</span><span class="n">num_requested_indices</span><span class="p">],</span>
                    <span class="s2">&quot;unique misses / requested indices&quot;</span><span class="p">:</span> <span class="n">uvm_cache_stats</span><span class="p">[</span>
                        <span class="n">UVMCacheStatsIndex</span><span class="o">.</span><span class="n">num_unique_misses</span>
                    <span class="p">]</span>
                    <span class="o">/</span> <span class="n">uvm_cache_stats</span><span class="p">[</span><span class="n">UVMCacheStatsIndex</span><span class="o">.</span><span class="n">num_requested_indices</span><span class="p">],</span>
                <span class="p">}</span>
            <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;uvm_cache_stats=</span><span class="si">{</span><span class="n">m</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="nd">@torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">ignore</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">_report_uvm_cache_stats</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">stats_reporter</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span>
        <span class="n">stats_reporter</span><span class="p">:</span> <span class="n">TBEStatsReporter</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">stats_reporter</span>
        <span class="n">passed_steps</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">step</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">last_reported_step</span>
        <span class="k">if</span> <span class="n">passed_steps</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">return</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">stats_reporter</span><span class="o">.</span><span class="n">should_report</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">step</span><span class="p">):</span>
            <span class="k">return</span>

        <span class="n">uvm_cache_stats</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_uvm_cache_stats</span><span class="p">(</span>
            <span class="n">use_local_cache</span><span class="o">=</span><span class="kc">False</span>
        <span class="p">)</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">last_reported_step</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">step</span>

        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">last_reported_uvm_stats</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">last_reported_uvm_stats</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.0</span><span class="p">]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">uvm_cache_stats</span><span class="p">)</span>
        <span class="n">uvm_cache_stats_delta</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.0</span><span class="p">]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">uvm_cache_stats</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">uvm_cache_stats</span><span class="p">)):</span>
            <span class="n">uvm_cache_stats_delta</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">uvm_cache_stats</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">last_reported_uvm_stats</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
            <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">last_reported_uvm_stats</span> <span class="o">=</span> <span class="n">uvm_cache_stats</span>

        <span class="c1"># pyre-fixme[29]: `Union[(self: TensorBase) -&gt; int, Module, Tensor]` is not</span>
        <span class="c1">#  a function.</span>
        <span class="n">element_size</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lxu_cache_weights</span><span class="o">.</span><span class="n">element_size</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">stat_index</span> <span class="ow">in</span> <span class="n">UVMCacheStatsIndex</span><span class="p">:</span>
            <span class="n">stats_reporter</span><span class="o">.</span><span class="n">report_data_amount</span><span class="p">(</span>
                <span class="n">iteration_step</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">step</span><span class="p">,</span>
                <span class="n">event_name</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;tbe.prefetch.cache_stats_by_data_size.</span><span class="si">{</span><span class="n">stat_index</span><span class="o">.</span><span class="n">name</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span>
                <span class="n">data_bytes</span><span class="o">=</span><span class="nb">int</span><span class="p">(</span>
                    <span class="n">uvm_cache_stats_delta</span><span class="p">[</span><span class="n">stat_index</span><span class="o">.</span><span class="n">value</span><span class="p">]</span>
                    <span class="o">*</span> <span class="n">element_size</span>
                    <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_D_cache</span>
                    <span class="o">/</span> <span class="n">passed_steps</span>
                <span class="p">),</span>
                <span class="n">embedding_id</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">logging_table_name</span><span class="p">,</span>
                <span class="n">tbe_id</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">uuid</span><span class="p">,</span>
            <span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">prefetch</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">indices</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">offsets</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">forward_stream</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">Stream</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">batch_size_per_feature_per_rank</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">prefetch_stream</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">forward_stream</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">prefetch_stream</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">current_stream</span><span class="p">()</span>
            <span class="k">assert</span> <span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">prefetch_stream</span> <span class="o">!=</span> <span class="n">forward_stream</span>
            <span class="p">),</span> <span class="s2">&quot;prefetch_stream and forward_stream should not be the same stream&quot;</span>

        <span class="n">indices</span><span class="p">,</span> <span class="n">offsets</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">vbe_metadata</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">prepare_inputs</span><span class="p">(</span>
            <span class="n">indices</span><span class="p">,</span>
            <span class="n">offsets</span><span class="p">,</span>
            <span class="n">per_sample_weights</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
            <span class="n">batch_size_per_feature_per_rank</span><span class="o">=</span><span class="n">batch_size_per_feature_per_rank</span><span class="p">,</span>
            <span class="n">force_cast_input_types</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="n">prefetch_pipeline</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">prefetch_pipeline</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">_recording_to_timer</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">prefetch_duration_timer</span><span class="p">,</span>
            <span class="n">context</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">step</span><span class="p">,</span>
            <span class="n">stream</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">current_stream</span><span class="p">(),</span>
        <span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_prefetch</span><span class="p">(</span>
                <span class="n">indices</span><span class="p">,</span>
                <span class="n">offsets</span><span class="p">,</span>
                <span class="n">vbe_metadata</span><span class="p">,</span>
                <span class="n">multipass_prefetch_config</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">multipass_prefetch_config</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="k">if</span> <span class="n">forward_stream</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_prefetch_tensors_record_stream</span><span class="p">(</span><span class="n">forward_stream</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_prefetch</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">indices</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">offsets</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">vbe_metadata</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">invokers</span><span class="o">.</span><span class="n">lookup_args</span><span class="o">.</span><span class="n">VBEMetadata</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">multipass_prefetch_config</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">MultiPassPrefetchConfig</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">is_torchdynamo_compiling</span><span class="p">():</span>
            <span class="c1"># Mutations of nn.Module attr forces dynamo restart of Analysis which increases compilation time</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">timestep</span> <span class="o">+=</span> <span class="mi">1</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">timesteps_prefetched</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">timestep</span><span class="p">)</span>

        <span class="c1"># pyre-fixme[29]: `Union[(self: TensorBase) -&gt; int, Module, Tensor]` is not</span>
        <span class="c1">#  a function.</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">lxu_cache_weights</span><span class="o">.</span><span class="n">numel</span><span class="p">():</span>
            <span class="k">return</span>

        <span class="c1"># Clear the local_uvm_cache_stats before the prefetch instead of after</span>
        <span class="c1"># the prefetch step, since it will be used in the CommonArgs in the</span>
        <span class="c1"># forward step</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">gather_uvm_cache_stats</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">local_uvm_cache_stats</span><span class="o">.</span><span class="n">zero_</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_report_io_size_count</span><span class="p">(</span><span class="s2">&quot;prefetch_input&quot;</span><span class="p">,</span> <span class="n">indices</span><span class="p">)</span>

        <span class="n">final_lxu_cache_locations</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty_like</span><span class="p">(</span><span class="n">indices</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
        <span class="k">for</span> <span class="p">(</span>
            <span class="n">partial_indices</span><span class="p">,</span>
            <span class="n">partial_lxu_cache_locations</span><span class="p">,</span>
            <span class="n">base_offset</span><span class="p">,</span>
        <span class="p">)</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_prefetch_passes</span><span class="p">(</span>
            <span class="n">multipass_prefetch_config</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">final_lxu_cache_locations</span>
        <span class="p">):</span>
            <span class="n">linear_cache_indices</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">fbgemm</span><span class="o">.</span><span class="n">linearize_cache_indices</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">cache_hash_size_cumsum</span><span class="p">,</span>
                <span class="n">partial_indices</span><span class="p">,</span>
                <span class="n">offsets</span><span class="p">,</span>
                <span class="n">vbe_metadata</span><span class="o">.</span><span class="n">B_offsets</span> <span class="k">if</span> <span class="n">vbe_metadata</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="kc">None</span><span class="p">,</span>
                <span class="n">vbe_metadata</span><span class="o">.</span><span class="n">max_B</span> <span class="k">if</span> <span class="n">vbe_metadata</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span>
                <span class="n">base_offset</span><span class="p">,</span>
            <span class="p">)</span>

            <span class="k">if</span> <span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">record_cache_metrics</span><span class="o">.</span><span class="n">record_cache_miss_counter</span>
                <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">record_cache_metrics</span><span class="o">.</span><span class="n">record_tablewise_cache_miss</span>
            <span class="p">):</span>
                <span class="n">lxu_cache_locations</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">fbgemm</span><span class="o">.</span><span class="n">lxu_cache_lookup</span><span class="p">(</span>
                    <span class="n">linear_cache_indices</span><span class="p">,</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">lxu_cache_state</span><span class="p">,</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">total_cache_hash_size</span><span class="p">,</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">gather_uvm_cache_stats</span><span class="p">,</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">local_uvm_cache_stats</span><span class="p">,</span>
                <span class="p">)</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">record_cache_metrics</span><span class="o">.</span><span class="n">record_cache_miss_counter</span><span class="p">:</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">_update_cache_miss_counter</span><span class="p">(</span>
                        <span class="n">lxu_cache_locations</span><span class="p">,</span> <span class="n">linear_cache_indices</span>
                    <span class="p">)</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">record_cache_metrics</span><span class="o">.</span><span class="n">record_tablewise_cache_miss</span><span class="p">:</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">_update_tablewise_cache_miss</span><span class="p">(</span>
                        <span class="n">lxu_cache_locations</span><span class="p">,</span> <span class="n">linear_cache_indices</span><span class="p">,</span> <span class="n">offsets</span>
                    <span class="p">)</span>

            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">cache_algorithm</span> <span class="o">==</span> <span class="n">CacheAlgorithm</span><span class="o">.</span><span class="n">LRU</span><span class="p">:</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">fbgemm</span><span class="o">.</span><span class="n">lru_cache_populate</span><span class="p">(</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">weights_uvm</span><span class="p">,</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">cache_hash_size_cumsum</span><span class="p">,</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">total_cache_hash_size</span><span class="p">,</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">cache_index_table_map</span><span class="p">,</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">weights_offsets</span><span class="p">,</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">D_offsets</span><span class="p">,</span>
                    <span class="n">linear_cache_indices</span><span class="p">,</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">lxu_cache_state</span><span class="p">,</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">lxu_cache_weights</span><span class="p">,</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">timestep</span><span class="p">,</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">lxu_state</span><span class="p">,</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">stochastic_rounding</span><span class="p">,</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">gather_uvm_cache_stats</span><span class="p">,</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">local_uvm_cache_stats</span><span class="p">,</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">lock_cache_line</span><span class="p">,</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">lxu_cache_locking_counter</span><span class="p">,</span>
                <span class="p">)</span>
            <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">cache_algorithm</span> <span class="o">==</span> <span class="n">CacheAlgorithm</span><span class="o">.</span><span class="n">LFU</span><span class="p">:</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">fbgemm</span><span class="o">.</span><span class="n">lfu_cache_populate</span><span class="p">(</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">weights_uvm</span><span class="p">,</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">cache_hash_size_cumsum</span><span class="p">,</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">total_cache_hash_size</span><span class="p">,</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">cache_index_table_map</span><span class="p">,</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">weights_offsets</span><span class="p">,</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">D_offsets</span><span class="p">,</span>
                    <span class="n">linear_cache_indices</span><span class="p">,</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">lxu_cache_state</span><span class="p">,</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">lxu_cache_weights</span><span class="p">,</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">lxu_state</span><span class="p">,</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">stochastic_rounding</span><span class="p">,</span>
                <span class="p">)</span>

            <span class="n">torch</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">fbgemm</span><span class="o">.</span><span class="n">lxu_cache_lookup</span><span class="p">(</span>
                <span class="n">linear_cache_indices</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">lxu_cache_state</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">total_cache_hash_size</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">gather_uvm_cache_stats</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">local_uvm_cache_stats</span><span class="p">,</span>
                <span class="n">lxu_cache_locations_output</span><span class="o">=</span><span class="n">partial_lxu_cache_locations</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="k">assert</span> <span class="p">(</span>
            <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">lxu_cache_locations_list</span><span class="p">)</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_prefetch_depth</span>
        <span class="p">),</span> <span class="sa">f</span><span class="s2">&quot;self.lxu_cache_locations_list has grown to size: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">lxu_cache_locations_list</span><span class="p">)</span><span class="si">}</span><span class="s2">, this exceeds the maximum: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">max_prefetch_depth</span><span class="si">}</span><span class="s2">. This probably indicates an error in logic where prefetch() is being called more frequently than forward()&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lxu_cache_locations_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">final_lxu_cache_locations</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">gather_uvm_cache_stats</span><span class="p">:</span>
            <span class="c1"># Accumulate local_uvm_cache_stats (int32) into uvm_cache_stats (int64).</span>
            <span class="c1"># We may want to do this accumulation atomically, but as it&#39;s only</span>
            <span class="c1"># for monitoring, slightly inaccurate result may be acceptable.</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">uvm_cache_stats</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">add</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">uvm_cache_stats</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">local_uvm_cache_stats</span>
            <span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_report_uvm_cache_stats</span><span class="p">()</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">should_log</span><span class="p">():</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">print_uvm_cache_stats</span><span class="p">(</span><span class="n">use_local_cache</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">should_log</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Determines if we should log for this step, using exponentially decreasing frequency.</span>

<span class="sd">        Logs for steps: 100 200 ... 1,000 2,000 ... 10,000 20,000 ... 100,000 200,000 ...</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">s</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">step</span> <span class="o">+</span> <span class="mi">1</span>  <span class="c1"># step starts at 0</span>
        <span class="k">return</span> <span class="n">s</span> <span class="o">&gt;=</span> <span class="mi">100</span> <span class="ow">and</span> <span class="n">s</span> <span class="o">%</span> <span class="p">(</span><span class="mi">10</span> <span class="o">**</span> <span class="nb">int</span><span class="p">(</span><span class="n">math</span><span class="o">.</span><span class="n">log10</span><span class="p">(</span><span class="n">s</span><span class="p">)))</span> <span class="o">==</span> <span class="mi">0</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_prefetch_tensors_record_stream</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">forward_stream</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">Stream</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="c1"># Record the tensors created by prefetch stream and consumed by forward/backward</span>
        <span class="c1"># to the forward stream. In PyTorch, each backward CUDA op runs on the same</span>
        <span class="c1"># stream that was used for its corresponding forward op.</span>

        <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">lxu_cache_locations_list</span><span class="p">:</span>
            <span class="n">t</span><span class="o">.</span><span class="n">record_stream</span><span class="p">(</span><span class="n">forward_stream</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_update_cache_miss_counter</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">lxu_cache_locations</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">linear_cache_indices</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">CACHE_MISS</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
        <span class="n">CACHE_HIT</span> <span class="o">=</span> <span class="o">-</span><span class="mi">2</span>

        <span class="n">cache_missed_locations</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">where</span><span class="p">(</span>
            <span class="n">lxu_cache_locations</span> <span class="o">==</span> <span class="n">CACHE_MISS</span><span class="p">,</span> <span class="n">linear_cache_indices</span><span class="p">,</span> <span class="n">CACHE_HIT</span>
        <span class="p">)</span>
        <span class="n">unique_ids_list</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">cache_missed_locations</span><span class="p">)</span>
        <span class="n">unique_ids_count_list</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">unique_ids_list</span> <span class="o">==</span> <span class="n">CACHE_HIT</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

        <span class="n">miss_count</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">unique_ids_count_list</span><span class="p">)</span>

        <span class="c1"># pyre-fixme[29]: `Union[(self: TensorBase, indices: Union[None, slice[Any, A...</span>
        <span class="c1"># pyre-fixme[29]: `Union[(self: TensorBase, indices: Union[None, slice[Any, A...</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cache_miss_counter</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+=</span> <span class="p">(</span><span class="n">miss_count</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span>

        <span class="c1"># pyre-fixme[29]: `Union[(self: TensorBase, indices: Union[None, slice[Any, A...</span>
        <span class="c1"># pyre-fixme[29]: `Union[(self: TensorBase, indices: Union[None, slice[Any, A...</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cache_miss_counter</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+=</span> <span class="n">miss_count</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_update_tablewise_cache_miss</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">lxu_cache_locations</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">linear_cache_indices</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">offsets</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">CACHE_MISS</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
        <span class="n">CACHE_HIT</span> <span class="o">=</span> <span class="o">-</span><span class="mi">2</span>

        <span class="c1"># pyre-fixme[6]: For 1st argument expected</span>
        <span class="c1">#  `pyre_extensions.PyreReadOnly[Sized]` but got `Union[Module, Tensor]`.</span>
        <span class="n">num_tables</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">cache_hash_size_cumsum</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span>
        <span class="n">num_offsets_per_table</span> <span class="o">=</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">offsets</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">//</span> <span class="n">num_tables</span>
        <span class="n">cache_missed_locations</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">where</span><span class="p">(</span>
            <span class="n">lxu_cache_locations</span> <span class="o">==</span> <span class="n">CACHE_MISS</span><span class="p">,</span> <span class="n">linear_cache_indices</span><span class="p">,</span> <span class="n">CACHE_HIT</span>
        <span class="p">)</span>

        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_tables</span><span class="p">):</span>
            <span class="n">start</span> <span class="o">=</span> <span class="n">offsets</span><span class="p">[</span><span class="n">i</span> <span class="o">*</span> <span class="n">num_offsets_per_table</span><span class="p">]</span>
            <span class="n">end</span> <span class="o">=</span> <span class="n">offsets</span><span class="p">[(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">num_offsets_per_table</span><span class="p">]</span>

            <span class="n">current_cache_missed_locations</span> <span class="o">=</span> <span class="n">cache_missed_locations</span><span class="p">[</span><span class="n">start</span><span class="p">:</span><span class="n">end</span><span class="p">]</span>
            <span class="n">unique_ids_list</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">current_cache_missed_locations</span><span class="p">)</span>
            <span class="n">unique_ids_count_list</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">unique_ids_list</span> <span class="o">==</span> <span class="n">CACHE_HIT</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

            <span class="n">miss_count</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">unique_ids_count_list</span><span class="p">)</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">table_wise_cache_miss</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+=</span> <span class="n">miss_count</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">init_embedding_weights_uniform</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">min_val</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span> <span class="n">max_val</span><span class="p">:</span> <span class="nb">float</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">splits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">split_embedding_weights</span><span class="p">()</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights_precision</span> <span class="o">==</span> <span class="n">SparseType</span><span class="o">.</span><span class="n">INT8</span><span class="p">:</span>
            <span class="c1"># TODO: add in-place FloatToFused8BitRowwiseQuantized conversion</span>
            <span class="k">for</span> <span class="n">emb</span> <span class="ow">in</span> <span class="n">splits</span><span class="p">:</span>
                <span class="k">assert</span> <span class="p">(</span>
                    <span class="nb">len</span><span class="p">(</span><span class="n">emb</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">==</span> <span class="mi">2</span>
                <span class="p">),</span> <span class="s2">&quot;Int8 embedding only supported for 2D weight tensors.&quot;</span>
                <span class="n">shape</span> <span class="o">=</span> <span class="p">[</span><span class="n">emb</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">emb</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">int8_emb_row_dim_offset</span><span class="p">]</span>
                <span class="n">tmp_emb</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">current_device</span><span class="p">)</span>
                <span class="n">tmp_emb</span><span class="o">.</span><span class="n">uniform_</span><span class="p">(</span><span class="n">min_val</span><span class="p">,</span> <span class="n">max_val</span><span class="p">)</span>
                <span class="n">tmp_emb_i8</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">fbgemm</span><span class="o">.</span><span class="n">FloatToFused8BitRowwiseQuantized</span><span class="p">(</span><span class="n">tmp_emb</span><span class="p">)</span>
                <span class="n">emb</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">copy_</span><span class="p">(</span><span class="n">tmp_emb_i8</span><span class="p">)</span>
        <span class="c1"># Torch doesnt implement direct fp8 distribution functions, so we need to start in higher precision.</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights_precision</span> <span class="o">==</span> <span class="n">SparseType</span><span class="o">.</span><span class="n">NFP8</span><span class="p">:</span>
            <span class="k">assert</span> <span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">current_device</span><span class="o">.</span><span class="n">type</span> <span class="o">==</span> <span class="s2">&quot;cuda&quot;</span>
            <span class="p">),</span> <span class="s2">&quot;NFP8 is currently only supportd on GPU.&quot;</span>
            <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span> <span class="ow">in</span> <span class="p">[</span>
                <span class="n">OptimType</span><span class="o">.</span><span class="n">EXACT_ADAGRAD</span><span class="p">,</span>
                <span class="n">OptimType</span><span class="o">.</span><span class="n">ROWWISE_ADAGRAD</span><span class="p">,</span>
                <span class="n">OptimType</span><span class="o">.</span><span class="n">EXACT_ROWWISE_ADAGRAD</span><span class="p">,</span>
                <span class="n">OptimType</span><span class="o">.</span><span class="n">ENSEMBLE_ROWWISE_ADAGRAD</span><span class="p">,</span>
                <span class="n">OptimType</span><span class="o">.</span><span class="n">EMAINPLACE_ROWWISE_ADAGRAD</span><span class="p">,</span>
            <span class="p">],</span> <span class="s2">&quot;NFP8 is currently only supportd with adagrad optimizers.&quot;</span>
            <span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">splits</span><span class="p">:</span>
                <span class="n">tmp_param</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">param</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">current_device</span><span class="p">)</span>
                <span class="c1"># Create initialized weights and cast to fp8.</span>
                <span class="n">fp8_dtype</span> <span class="o">=</span> <span class="p">(</span>
                    <span class="n">torch</span><span class="o">.</span><span class="n">float8_e4m3fnuz</span>
                    <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">version</span><span class="o">.</span><span class="n">hip</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
                    <span class="k">else</span> <span class="n">torch</span><span class="o">.</span><span class="n">float8_e4m3fn</span>
                <span class="p">)</span>
                <span class="n">tmp_param</span><span class="o">.</span><span class="n">uniform_</span><span class="p">(</span><span class="n">min_val</span><span class="p">,</span> <span class="n">max_val</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">fp8_dtype</span><span class="p">)</span>
                <span class="n">param</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">copy_</span><span class="p">(</span><span class="n">tmp_param</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">splits</span><span class="p">:</span>
                <span class="n">param</span><span class="o">.</span><span class="n">uniform_</span><span class="p">(</span><span class="n">min_val</span><span class="p">,</span> <span class="n">max_val</span><span class="p">)</span>

<div class="viewcode-block" id="SplitTableBatchedEmbeddingBagsCodegen.split_embedding_weights"><a class="viewcode-back" href="../../fbgemm_gpu/python-api/tbe_ops_training.html#fbgemm_gpu.split_table_batched_embeddings_ops_training.SplitTableBatchedEmbeddingBagsCodegen.split_embedding_weights">[docs]</a>    <span class="nd">@torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">ignore</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">split_embedding_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns a list of embedding weights (view), split by table</span>

<span class="sd">        Returns:</span>
<span class="sd">            A list of weights. Length = the number of tables</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">splits</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">t</span><span class="p">,</span> <span class="p">(</span><span class="n">rows</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">embedding_specs</span><span class="p">):</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights_precision</span> <span class="o">==</span> <span class="n">SparseType</span><span class="o">.</span><span class="n">INT8</span><span class="p">:</span>
                <span class="n">dim</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">int8_emb_row_dim_offset</span>
            <span class="c1"># pyre-fixme[29]: `Union[(self: TensorBase, indices: Union[None, slice[An...</span>
            <span class="n">placement</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights_physical_placements</span><span class="p">[</span><span class="n">t</span><span class="p">]</span>
            <span class="c1"># pyre-fixme[29]: `Union[(self: TensorBase, indices: Union[None, slice[An...</span>
            <span class="n">offset</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights_physical_offsets</span><span class="p">[</span><span class="n">t</span><span class="p">]</span>
            <span class="k">if</span> <span class="n">placement</span> <span class="o">==</span> <span class="n">EmbeddingLocation</span><span class="o">.</span><span class="n">DEVICE</span><span class="o">.</span><span class="n">value</span><span class="p">:</span>
                <span class="n">weights</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights_dev</span>
            <span class="k">elif</span> <span class="n">placement</span> <span class="o">==</span> <span class="n">EmbeddingLocation</span><span class="o">.</span><span class="n">HOST</span><span class="o">.</span><span class="n">value</span><span class="p">:</span>
                <span class="n">weights</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights_host</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">weights</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights_uvm</span>
            <span class="c1"># pyre-fixme[29]: `Union[(self: TensorBase) -&gt; int, Module, Tensor]` is</span>
            <span class="c1">#  not a function.</span>
            <span class="k">if</span> <span class="n">weights</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
                <span class="n">weights</span> <span class="o">=</span> <span class="n">weights</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
            <span class="n">splits</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                <span class="n">weights</span><span class="o">.</span><span class="n">detach</span><span class="p">()[</span><span class="n">offset</span> <span class="p">:</span> <span class="n">offset</span> <span class="o">+</span> <span class="n">rows</span> <span class="o">*</span> <span class="n">dim</span><span class="p">]</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">rows</span><span class="p">,</span> <span class="n">dim</span><span class="p">)</span>
            <span class="p">)</span>
        <span class="k">return</span> <span class="n">splits</span></div>

    <span class="nd">@torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">ignore</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">get_optimizer_buffer</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span> <span class="o">==</span> <span class="n">OptimType</span><span class="o">.</span><span class="n">NONE</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Getting optimizer buffer is not supported for </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="p">)</span>
        <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">buffer</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">named_buffers</span><span class="p">():</span>
            <span class="k">if</span> <span class="n">name</span> <span class="o">==</span> <span class="n">state</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">buffer</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Optimizer buffer </span><span class="si">{</span><span class="n">state</span><span class="si">}</span><span class="s2"> not found&quot;</span><span class="p">)</span>

    <span class="nd">@torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">export</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">get_optimizer_state</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]:</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Get the optimizer state dict that matches the OSS Pytorch optims</span>
<span class="sd">        TODO: populate the supported list of optimizers</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">split_optimizer_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">split_optimizer_states</span><span class="p">()</span>
        <span class="k">if</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span> <span class="o">==</span> <span class="n">OptimType</span><span class="o">.</span><span class="n">EXACT_ROWWISE_ADAGRAD</span>
            <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span> <span class="o">==</span> <span class="n">OptimType</span><span class="o">.</span><span class="n">EXACT_ADAGRAD</span>
            <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span> <span class="o">==</span> <span class="n">OptimType</span><span class="o">.</span><span class="n">EMAINPLACE_ROWWISE_ADAGRAD</span>
        <span class="p">):</span>
            <span class="n">list_of_state_dict</span> <span class="o">=</span> <span class="p">[</span>
                <span class="p">(</span>
                    <span class="p">(</span>
                        <span class="p">{</span>
                            <span class="s2">&quot;sum&quot;</span><span class="p">:</span> <span class="n">states</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
                            <span class="s2">&quot;prev_iter&quot;</span><span class="p">:</span> <span class="n">states</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
                            <span class="s2">&quot;row_counter&quot;</span><span class="p">:</span> <span class="n">states</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span>
                            <span class="s2">&quot;iter&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">iter</span><span class="p">,</span>
                        <span class="p">}</span>
                        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimizer_args</span><span class="o">.</span><span class="n">regularization_mode</span>
                        <span class="o">==</span> <span class="n">WeightDecayMode</span><span class="o">.</span><span class="n">COUNTER</span><span class="o">.</span><span class="n">value</span>
                        <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimizer_args</span><span class="o">.</span><span class="n">weight_decay_mode</span>
                        <span class="o">==</span> <span class="n">CounterWeightDecayMode</span><span class="o">.</span><span class="n">ADAGRADW</span><span class="o">.</span><span class="n">value</span>
                        <span class="k">else</span> <span class="p">{</span>
                            <span class="s2">&quot;sum&quot;</span><span class="p">:</span> <span class="n">states</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
                            <span class="s2">&quot;prev_iter&quot;</span><span class="p">:</span> <span class="n">states</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
                            <span class="s2">&quot;row_counter&quot;</span><span class="p">:</span> <span class="n">states</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span>
                        <span class="p">}</span>
                    <span class="p">)</span>
                    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_used_rowwise_adagrad_with_counter</span>
                    <span class="k">else</span> <span class="p">(</span>
                        <span class="p">{</span>
                            <span class="s2">&quot;sum&quot;</span><span class="p">:</span> <span class="n">states</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
                            <span class="s2">&quot;prev_iter&quot;</span><span class="p">:</span> <span class="n">states</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
                            <span class="s2">&quot;iter&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">iter</span><span class="p">,</span>
                        <span class="p">}</span>
                        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_used_rowwise_adagrad_with_global_weight_decay</span>
                        <span class="k">else</span> <span class="p">{</span><span class="s2">&quot;sum&quot;</span><span class="p">:</span> <span class="n">states</span><span class="p">[</span><span class="mi">0</span><span class="p">]}</span>
                    <span class="p">)</span>
                <span class="p">)</span>
                <span class="k">for</span> <span class="n">states</span> <span class="ow">in</span> <span class="n">split_optimizer_states</span>
            <span class="p">]</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span> <span class="o">==</span> <span class="n">OptimType</span><span class="o">.</span><span class="n">SGD</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span> <span class="o">==</span> <span class="n">OptimType</span><span class="o">.</span><span class="n">EXACT_SGD</span><span class="p">:</span>
            <span class="n">list_of_state_dict</span> <span class="o">=</span> <span class="p">[</span>
                <span class="p">{</span><span class="s2">&quot;momentum_buffer&quot;</span><span class="p">:</span> <span class="n">states</span><span class="p">[</span><span class="mi">0</span><span class="p">]}</span> <span class="k">for</span> <span class="n">states</span> <span class="ow">in</span> <span class="n">split_optimizer_states</span>
            <span class="p">]</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span> <span class="o">==</span> <span class="n">OptimType</span><span class="o">.</span><span class="n">ADAM</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_rowwise_bias_correction</span><span class="p">:</span>
            <span class="n">list_of_state_dict</span> <span class="o">=</span> <span class="p">[</span>
                <span class="p">{</span>
                    <span class="s2">&quot;exp_avg&quot;</span><span class="p">:</span> <span class="n">states</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
                    <span class="s2">&quot;exp_avg_sq&quot;</span><span class="p">:</span> <span class="n">states</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
                    <span class="s2">&quot;row_counter&quot;</span><span class="p">:</span> <span class="n">states</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span>
                <span class="p">}</span>
                <span class="k">for</span> <span class="n">states</span> <span class="ow">in</span> <span class="n">split_optimizer_states</span>
            <span class="p">]</span>
        <span class="k">elif</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span> <span class="o">==</span> <span class="n">OptimType</span><span class="o">.</span><span class="n">ADAM</span>
            <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span> <span class="o">==</span> <span class="n">OptimType</span><span class="o">.</span><span class="n">PARTIAL_ROWWISE_ADAM</span>
            <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span> <span class="o">==</span> <span class="n">OptimType</span><span class="o">.</span><span class="n">LAMB</span>
            <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span> <span class="o">==</span> <span class="n">OptimType</span><span class="o">.</span><span class="n">PARTIAL_ROWWISE_LAMB</span>
        <span class="p">):</span>
            <span class="n">list_of_state_dict</span> <span class="o">=</span> <span class="p">[</span>
                <span class="p">{</span><span class="s2">&quot;exp_avg&quot;</span><span class="p">:</span> <span class="n">states</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="s2">&quot;exp_avg_sq&quot;</span><span class="p">:</span> <span class="n">states</span><span class="p">[</span><span class="mi">1</span><span class="p">]}</span>
                <span class="k">for</span> <span class="n">states</span> <span class="ow">in</span> <span class="n">split_optimizer_states</span>
            <span class="p">]</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span> <span class="o">==</span> <span class="n">OptimType</span><span class="o">.</span><span class="n">ENSEMBLE_ROWWISE_ADAGRAD</span><span class="p">:</span>
            <span class="n">list_of_state_dict</span> <span class="o">=</span> <span class="p">[</span>
                <span class="p">{</span>
                    <span class="s2">&quot;sum&quot;</span><span class="p">:</span> <span class="n">states</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
                    <span class="s2">&quot;sparse_ema&quot;</span><span class="p">:</span> <span class="n">states</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
                <span class="p">}</span>
                <span class="k">for</span> <span class="n">states</span> <span class="ow">in</span> <span class="n">split_optimizer_states</span>
            <span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Getting optimizer state </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="si">}</span><span class="s2"> is not implmeneted&quot;</span>
            <span class="p">)</span>

        <span class="k">return</span> <span class="n">list_of_state_dict</span>

<div class="viewcode-block" id="SplitTableBatchedEmbeddingBagsCodegen.split_optimizer_states"><a class="viewcode-back" href="../../fbgemm_gpu/python-api/tbe_ops_training.html#fbgemm_gpu.split_table_batched_embeddings_ops_training.SplitTableBatchedEmbeddingBagsCodegen.split_optimizer_states">[docs]</a>    <span class="nd">@torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">ignore</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">split_optimizer_states</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns a list of optimizer states (view), split by table</span>

<span class="sd">        Returns:</span>
<span class="sd">            A list of list of states. Shape = (the number of tables, the number</span>
<span class="sd">            of states).</span>

<span class="sd">            The following shows the list of states (in the returned order) for</span>
<span class="sd">            each optimizer:</span>

<span class="sd">            (1) `ADAM`: `momentum1`, `momentum2`</span>

<span class="sd">            (2) `EXACT_ADAGRAD`: `momentum1`</span>

<span class="sd">            (3) `EXACT_ROWWISE_ADAGRAD`: `momentum1` (rowwise), `prev_iter`</span>
<span class="sd">                (rowwise; only when using `WeightDecayMode` = `COUNTER` or</span>
<span class="sd">                `COWCLIP` or `global_weight_decay` is not None), `row_counter`</span>
<span class="sd">                (rowwise; only when using `WeightDecayMode` = `COUNTER` or</span>
<span class="sd">                `COWCLIP`)</span>

<span class="sd">            (4) `EXACT_SGD`: no states</span>

<span class="sd">            (5) `LAMB`: `momentum1`, `momentum2`</span>

<span class="sd">            (6) `LARS_SGD`: `momentum1`</span>

<span class="sd">            (7) `PARTIAL_ROWWISE_ADAM`: `momentum1`, `momentum2` (rowwise)</span>

<span class="sd">            (8) `PARTIAL_ROWWISE_LAMB`: `momentum1`, `momentum2` (rowwise)</span>

<span class="sd">            (9) `ENSEMBLE_ROWWISE_ADAGRAD`: `momentum1` (rowwise), `momentum2`</span>

<span class="sd">            (10) `NONE`: no states (throwing an error)</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span> <span class="o">==</span> <span class="n">OptimType</span><span class="o">.</span><span class="n">NONE</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Getting optimizer states is not supported for </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="p">)</span>

        <span class="k">def</span><span class="w"> </span><span class="nf">get_optimizer_states</span><span class="p">(</span>
            <span class="n">state_dev</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
            <span class="n">state_host</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
            <span class="n">state_uvm</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
            <span class="n">state_offsets</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
            <span class="n">state_placements</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
            <span class="n">rowwise</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
        <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]:</span>
            <span class="n">splits</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="k">for</span> <span class="n">t</span><span class="p">,</span> <span class="p">(</span><span class="n">rows</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">embedding_specs</span><span class="p">):</span>
                <span class="n">offset</span> <span class="o">=</span> <span class="n">state_offsets</span><span class="p">[</span><span class="n">t</span><span class="p">]</span>
                <span class="n">placement</span> <span class="o">=</span> <span class="n">state_placements</span><span class="p">[</span><span class="n">t</span><span class="p">]</span>
                <span class="k">if</span> <span class="n">placement</span> <span class="o">==</span> <span class="n">EmbeddingLocation</span><span class="o">.</span><span class="n">DEVICE</span><span class="p">:</span>
                    <span class="n">state</span> <span class="o">=</span> <span class="n">state_dev</span>
                <span class="k">elif</span> <span class="n">placement</span> <span class="o">==</span> <span class="n">EmbeddingLocation</span><span class="o">.</span><span class="n">HOST</span><span class="p">:</span>
                    <span class="n">state</span> <span class="o">=</span> <span class="n">state_host</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">state</span> <span class="o">=</span> <span class="n">state_uvm</span>
                <span class="k">if</span> <span class="ow">not</span> <span class="n">rowwise</span><span class="p">:</span>
                    <span class="n">splits</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                        <span class="n">state</span><span class="o">.</span><span class="n">detach</span><span class="p">()[</span><span class="n">offset</span> <span class="p">:</span> <span class="n">offset</span> <span class="o">+</span> <span class="n">rows</span> <span class="o">*</span> <span class="n">dim</span><span class="p">]</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">rows</span><span class="p">,</span> <span class="n">dim</span><span class="p">)</span>
                    <span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">splits</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">state</span><span class="o">.</span><span class="n">detach</span><span class="p">()[</span><span class="n">offset</span> <span class="p">:</span> <span class="n">offset</span> <span class="o">+</span> <span class="n">rows</span><span class="p">]</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">rows</span><span class="p">))</span>
            <span class="k">return</span> <span class="n">splits</span>

        <span class="n">states</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">(</span><span class="n">OptimType</span><span class="o">.</span><span class="n">EXACT_SGD</span><span class="p">,):</span>
            <span class="n">states</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                <span class="n">get_optimizer_states</span><span class="p">(</span>
                    <span class="c1"># pyre-fixme[6]: For 1st argument expected `Tensor` but got</span>
                    <span class="c1">#  `Union[Module, Tensor]`.</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">momentum1_dev</span><span class="p">,</span>
                    <span class="c1"># pyre-fixme[6]: For 2nd argument expected `Tensor` but got</span>
                    <span class="c1">#  `Union[Module, Tensor]`.</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">momentum1_host</span><span class="p">,</span>
                    <span class="c1"># pyre-fixme[6]: For 3rd argument expected `Tensor` but got</span>
                    <span class="c1">#  `Union[Module, Tensor]`.</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">momentum1_uvm</span><span class="p">,</span>
                    <span class="c1"># pyre-fixme[6]: For 4th argument expected `Tensor` but got</span>
                    <span class="c1">#  `Union[Module, Tensor]`.</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">momentum1_physical_offsets</span><span class="p">,</span>
                    <span class="c1"># pyre-fixme[6]: For 5th argument expected `Tensor` but got</span>
                    <span class="c1">#  `Union[Module, Tensor]`.</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">momentum1_physical_placements</span><span class="p">,</span>
                    <span class="n">rowwise</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span>
                    <span class="ow">in</span> <span class="p">[</span>
                        <span class="n">OptimType</span><span class="o">.</span><span class="n">EXACT_ROWWISE_ADAGRAD</span><span class="p">,</span>
                        <span class="n">OptimType</span><span class="o">.</span><span class="n">ENSEMBLE_ROWWISE_ADAGRAD</span><span class="p">,</span>
                        <span class="n">OptimType</span><span class="o">.</span><span class="n">EMAINPLACE_ROWWISE_ADAGRAD</span><span class="p">,</span>
                    <span class="p">],</span>
                <span class="p">)</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span> <span class="ow">in</span> <span class="p">(</span>
            <span class="n">OptimType</span><span class="o">.</span><span class="n">ADAM</span><span class="p">,</span>
            <span class="n">OptimType</span><span class="o">.</span><span class="n">PARTIAL_ROWWISE_ADAM</span><span class="p">,</span>
            <span class="n">OptimType</span><span class="o">.</span><span class="n">LAMB</span><span class="p">,</span>
            <span class="n">OptimType</span><span class="o">.</span><span class="n">PARTIAL_ROWWISE_LAMB</span><span class="p">,</span>
            <span class="n">OptimType</span><span class="o">.</span><span class="n">ENSEMBLE_ROWWISE_ADAGRAD</span><span class="p">,</span>
        <span class="p">):</span>
            <span class="n">states</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                <span class="n">get_optimizer_states</span><span class="p">(</span>
                    <span class="c1"># pyre-fixme[6]: For 1st argument expected `Tensor` but got</span>
                    <span class="c1">#  `Union[Module, Tensor]`.</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">momentum2_dev</span><span class="p">,</span>
                    <span class="c1"># pyre-fixme[6]: For 2nd argument expected `Tensor` but got</span>
                    <span class="c1">#  `Union[Module, Tensor]`.</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">momentum2_host</span><span class="p">,</span>
                    <span class="c1"># pyre-fixme[6]: For 3rd argument expected `Tensor` but got</span>
                    <span class="c1">#  `Union[Module, Tensor]`.</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">momentum2_uvm</span><span class="p">,</span>
                    <span class="c1"># pyre-fixme[6]: For 4th argument expected `Tensor` but got</span>
                    <span class="c1">#  `Union[Module, Tensor]`.</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">momentum2_physical_offsets</span><span class="p">,</span>
                    <span class="c1"># pyre-fixme[6]: For 5th argument expected `Tensor` but got</span>
                    <span class="c1">#  `Union[Module, Tensor]`.</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">momentum2_physical_placements</span><span class="p">,</span>
                    <span class="n">rowwise</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span>
                    <span class="ow">in</span> <span class="p">(</span><span class="n">OptimType</span><span class="o">.</span><span class="n">PARTIAL_ROWWISE_ADAM</span><span class="p">,</span> <span class="n">OptimType</span><span class="o">.</span><span class="n">PARTIAL_ROWWISE_LAMB</span><span class="p">),</span>
                <span class="p">)</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_used_rowwise_adagrad_with_counter</span>
            <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">_used_rowwise_adagrad_with_global_weight_decay</span>
        <span class="p">):</span>
            <span class="n">states</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                <span class="n">get_optimizer_states</span><span class="p">(</span>
                    <span class="c1"># pyre-fixme[6]: For 1st argument expected `Tensor` but got</span>
                    <span class="c1">#  `Union[Module, Tensor]`.</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">prev_iter_dev</span><span class="p">,</span>
                    <span class="c1"># pyre-fixme[6]: For 2nd argument expected `Tensor` but got</span>
                    <span class="c1">#  `Union[Module, Tensor]`.</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">prev_iter_host</span><span class="p">,</span>
                    <span class="c1"># pyre-fixme[6]: For 3rd argument expected `Tensor` but got</span>
                    <span class="c1">#  `Union[Module, Tensor]`.</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">prev_iter_uvm</span><span class="p">,</span>
                    <span class="c1"># pyre-fixme[6]: For 4th argument expected `Tensor` but got</span>
                    <span class="c1">#  `Union[Module, Tensor]`.</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">prev_iter_physical_offsets</span><span class="p">,</span>
                    <span class="c1"># pyre-fixme[6]: For 5th argument expected `Tensor` but got</span>
                    <span class="c1">#  `Union[Module, Tensor]`.</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">prev_iter_physical_placements</span><span class="p">,</span>
                    <span class="n">rowwise</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                <span class="p">)</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_used_rowwise_adagrad_with_counter</span> <span class="ow">or</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span> <span class="o">==</span> <span class="n">OptimType</span><span class="o">.</span><span class="n">ADAM</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_rowwise_bias_correction</span>
        <span class="p">):</span>
            <span class="n">states</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                <span class="n">get_optimizer_states</span><span class="p">(</span>
                    <span class="c1"># pyre-fixme[6]: For 1st argument expected `Tensor` but got</span>
                    <span class="c1">#  `Union[Module, Tensor]`.</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">row_counter_dev</span><span class="p">,</span>
                    <span class="c1"># pyre-fixme[6]: For 2nd argument expected `Tensor` but got</span>
                    <span class="c1">#  `Union[Module, Tensor]`.</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">row_counter_host</span><span class="p">,</span>
                    <span class="c1"># pyre-fixme[6]: For 3rd argument expected `Tensor` but got</span>
                    <span class="c1">#  `Union[Module, Tensor]`.</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">row_counter_uvm</span><span class="p">,</span>
                    <span class="c1"># pyre-fixme[6]: For 4th argument expected `Tensor` but got</span>
                    <span class="c1">#  `Union[Module, Tensor]`.</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">row_counter_physical_offsets</span><span class="p">,</span>
                    <span class="c1"># pyre-fixme[6]: For 5th argument expected `Tensor` but got</span>
                    <span class="c1">#  `Union[Module, Tensor]`.</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">row_counter_physical_placements</span><span class="p">,</span>
                    <span class="n">rowwise</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                <span class="p">)</span>
            <span class="p">)</span>
        <span class="n">return_states</span> <span class="o">=</span> <span class="p">[</span><span class="nb">list</span><span class="p">(</span><span class="n">s</span><span class="p">)</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="o">*</span><span class="n">states</span><span class="p">)]</span>
        <span class="k">return</span> <span class="n">return_states</span></div>

<div class="viewcode-block" id="SplitTableBatchedEmbeddingBagsCodegen.set_learning_rate"><a class="viewcode-back" href="../../fbgemm_gpu/python-api/tbe_ops_training.html#fbgemm_gpu.split_table_batched_embeddings_ops_training.SplitTableBatchedEmbeddingBagsCodegen.set_learning_rate">[docs]</a>    <span class="nd">@torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">export</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">set_learning_rate</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">lr</span><span class="p">:</span> <span class="nb">float</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Sets the learning rate.</span>

<span class="sd">        Args:</span>
<span class="sd">            lr (float): The learning rate value to set to</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span> <span class="o">==</span> <span class="n">OptimType</span><span class="o">.</span><span class="n">NONE</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Setting learning rate is not supported for </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_learning_rate</span><span class="p">(</span><span class="n">lr</span><span class="p">)</span></div>

    <span class="k">def</span><span class="w"> </span><span class="nf">get_learning_rate</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Get and return the learning rate.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">learning_rate_tensor</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>

<div class="viewcode-block" id="SplitTableBatchedEmbeddingBagsCodegen.update_hyper_parameters"><a class="viewcode-back" href="../../fbgemm_gpu/python-api/tbe_ops_training.html#fbgemm_gpu.split_table_batched_embeddings_ops_training.SplitTableBatchedEmbeddingBagsCodegen.update_hyper_parameters">[docs]</a>    <span class="nd">@torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">ignore</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">update_hyper_parameters</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params_dict</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">float</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Sets hyper-parameters from external control flow.</span>

<span class="sd">        Args:</span>
<span class="sd">            params_dict (Dict[str, float]): The dict that contains the</span>
<span class="sd">                hyper-parameter names and their values</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span> <span class="o">==</span> <span class="n">OptimType</span><span class="o">.</span><span class="n">NONE</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Setting learning rate is not supported for </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="p">)</span>
        <span class="k">for</span> <span class="n">parameter_name</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">params_dict</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="k">if</span> <span class="n">parameter_name</span> <span class="o">==</span> <span class="s2">&quot;lr&quot;</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_set_learning_rate</span><span class="p">(</span><span class="n">value</span><span class="p">)</span>
            <span class="k">elif</span> <span class="n">parameter_name</span> <span class="o">==</span> <span class="s2">&quot;eps&quot;</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">optimizer_args</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimizer_args</span><span class="o">.</span><span class="n">_replace</span><span class="p">(</span><span class="n">eps</span><span class="o">=</span><span class="n">value</span><span class="p">)</span>
            <span class="k">elif</span> <span class="n">parameter_name</span> <span class="o">==</span> <span class="s2">&quot;beta1&quot;</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">optimizer_args</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimizer_args</span><span class="o">.</span><span class="n">_replace</span><span class="p">(</span><span class="n">beta1</span><span class="o">=</span><span class="n">value</span><span class="p">)</span>
            <span class="k">elif</span> <span class="n">parameter_name</span> <span class="o">==</span> <span class="s2">&quot;beta2&quot;</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">optimizer_args</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimizer_args</span><span class="o">.</span><span class="n">_replace</span><span class="p">(</span><span class="n">beta2</span><span class="o">=</span><span class="n">value</span><span class="p">)</span>
            <span class="k">elif</span> <span class="n">parameter_name</span> <span class="o">==</span> <span class="s2">&quot;weight_decay&quot;</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">optimizer_args</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimizer_args</span><span class="o">.</span><span class="n">_replace</span><span class="p">(</span><span class="n">weight_decay</span><span class="o">=</span><span class="n">value</span><span class="p">)</span>
            <span class="k">elif</span> <span class="n">parameter_name</span> <span class="o">==</span> <span class="s2">&quot;lower_bound&quot;</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">gwd_lower_bound</span> <span class="o">=</span> <span class="n">value</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;Setting hyper-parameter </span><span class="si">{</span><span class="n">parameter_name</span><span class="si">}</span><span class="s2"> is not supported&quot;</span>
                <span class="p">)</span></div>

    <span class="nd">@torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">ignore</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">_set_learning_rate</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">lr</span><span class="p">:</span> <span class="nb">float</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Helper function to script `set_learning_rate`.</span>
<span class="sd">        Note that returning None does not work.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">learning_rate_tensor</span><span class="o">.</span><span class="n">fill_</span><span class="p">(</span><span class="n">lr</span><span class="p">)</span>
        <span class="k">return</span> <span class="mf">0.0</span>

<div class="viewcode-block" id="SplitTableBatchedEmbeddingBagsCodegen.set_optimizer_step"><a class="viewcode-back" href="../../fbgemm_gpu/python-api/tbe_ops_training.html#fbgemm_gpu.split_table_batched_embeddings_ops_training.SplitTableBatchedEmbeddingBagsCodegen.set_optimizer_step">[docs]</a>    <span class="nd">@torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">ignore</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">set_optimizer_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">step</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Sets the optimizer step.</span>

<span class="sd">        Args:</span>
<span class="sd">            step (int): The step value to set to</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;set_optimizer_step from </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">iter</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">=}</span><span class="s2"> to </span><span class="si">{</span><span class="n">step</span><span class="si">=}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span> <span class="o">==</span> <span class="n">OptimType</span><span class="o">.</span><span class="n">NONE</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Setting optimizer step is not supported for </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">iter</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">step</span></div>

    <span class="nd">@torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">export</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">flush</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="c1"># pyre-fixme[29]: `Union[(self: TensorBase) -&gt; int, Module, Tensor]` is not</span>
        <span class="c1">#  a function.</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">lxu_cache_weights</span><span class="o">.</span><span class="n">numel</span><span class="p">():</span>
            <span class="k">return</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">fbgemm</span><span class="o">.</span><span class="n">lxu_cache_flush</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">weights_uvm</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">cache_hash_size_cumsum</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">cache_index_table_map</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">weights_offsets</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">D_offsets</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">total_D</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">lxu_cache_state</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">lxu_cache_weights</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">stochastic_rounding</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_apply_split</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">split</span><span class="p">:</span> <span class="n">SplitState</span><span class="p">,</span>
        <span class="n">prefix</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
        <span class="n">dtype</span><span class="p">:</span> <span class="n">Type</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">dtype</span><span class="p">],</span>
        <span class="n">enforce_hbm</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">make_dev_param</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">dev_reshape</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="o">...</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">uvm_host_mapped</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">apply_split_helper</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">,</span>
            <span class="n">functools</span><span class="o">.</span><span class="n">partial</span><span class="p">(</span><span class="nb">setattr</span><span class="p">,</span> <span class="bp">self</span><span class="p">),</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">current_device</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">use_cpu</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">feature_table_map</span><span class="p">,</span>
            <span class="n">split</span><span class="p">,</span>
            <span class="n">prefix</span><span class="p">,</span>
            <span class="n">dtype</span><span class="p">,</span>
            <span class="n">enforce_hbm</span><span class="p">,</span>
            <span class="n">make_dev_param</span><span class="p">,</span>
            <span class="n">dev_reshape</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_uvm_tensors_log</span><span class="p">,</span>
            <span class="n">uvm_host_mapped</span><span class="o">=</span><span class="n">uvm_host_mapped</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_apply_cache_state</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">cache_state</span><span class="p">:</span> <span class="n">CacheState</span><span class="p">,</span>
        <span class="n">cache_algorithm</span><span class="p">:</span> <span class="n">CacheAlgorithm</span><span class="p">,</span>
        <span class="n">cache_load_factor</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
        <span class="n">cache_sets</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">cache_reserved_memory</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
        <span class="n">cache_precision</span><span class="p">:</span> <span class="n">SparseType</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cache_algorithm</span> <span class="o">=</span> <span class="n">cache_algorithm</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">timestep</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">timesteps_prefetched</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">max_prefetch_depth</span> <span class="o">=</span> <span class="n">MAX_PREFETCH_DEPTH</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lxu_cache_locations_list</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lxu_cache_locations_empty</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span>
            <span class="mi">0</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">current_device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int32</span>
        <span class="p">)</span><span class="o">.</span><span class="n">fill_</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lxu_cache_locations</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lxu_cache_locations_empty</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_indices</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lxu_cache_locations_empty</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_offsets</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lxu_cache_locations_empty</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_vbe_B_offsets</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lxu_cache_locations_empty</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_vbe_max_B</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">prefetch_stream</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">Stream</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_init_uvm_cache_stats</span><span class="p">()</span>

        <span class="k">if</span> <span class="n">cache_precision</span> <span class="o">==</span> <span class="n">SparseType</span><span class="o">.</span><span class="n">FP32</span><span class="p">:</span>
            <span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">float32</span>
        <span class="k">elif</span> <span class="n">cache_precision</span> <span class="o">==</span> <span class="n">SparseType</span><span class="o">.</span><span class="n">FP16</span><span class="p">:</span>
            <span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">float16</span>
        <span class="k">elif</span> <span class="n">cache_precision</span> <span class="o">==</span> <span class="n">SparseType</span><span class="o">.</span><span class="n">NFP8</span><span class="p">:</span>
            <span class="c1"># NFP8 weights use floating point cache.</span>
            <span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">float16</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">float32</span>  <span class="c1"># not relevant, but setting it to keep linter happy</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_cpu</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">AssertionError</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;cache_precision </span><span class="si">{</span><span class="n">cache_precision</span><span class="si">}</span><span class="s2"> not supported!&quot;</span>
                <span class="p">)</span>

        <span class="c1"># NOTE: no cache for CPU mode!</span>
        <span class="k">if</span> <span class="n">cache_state</span><span class="o">.</span><span class="n">total_cache_hash_size</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_cpu</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span>
                <span class="s2">&quot;lxu_cache_weights&quot;</span><span class="p">,</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">current_device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">),</span>
            <span class="p">)</span>
            <span class="c1"># NOTE: make TorchScript work!</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span>
                <span class="s2">&quot;cache_hash_size_cumsum&quot;</span><span class="p">,</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int64</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">current_device</span><span class="p">),</span>
                <span class="n">persistent</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span>
                <span class="s2">&quot;total_cache_hash_size&quot;</span><span class="p">,</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int64</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">current_device</span><span class="p">),</span>
                <span class="n">persistent</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span>
                <span class="s2">&quot;cache_index_table_map&quot;</span><span class="p">,</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int64</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">current_device</span><span class="p">),</span>
                <span class="n">persistent</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span>
                <span class="s2">&quot;lxu_cache_state&quot;</span><span class="p">,</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int64</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">current_device</span><span class="p">),</span>
                <span class="n">persistent</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span>
                <span class="s2">&quot;lxu_state&quot;</span><span class="p">,</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int64</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">current_device</span><span class="p">),</span>
                <span class="n">persistent</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span>
                <span class="s2">&quot;cache_miss_counter&quot;</span><span class="p">,</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int64</span><span class="p">),</span>
                <span class="n">persistent</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_init_uvm_cache_counter</span><span class="p">(</span><span class="n">cache_sets</span><span class="p">,</span> <span class="n">persistent</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
            <span class="k">return</span>

        <span class="k">assert</span> <span class="n">cache_load_factor</span> <span class="o">&gt;</span> <span class="mi">0</span>
        <span class="n">element_size</span> <span class="o">=</span> <span class="mi">2</span> <span class="k">if</span> <span class="n">dtype</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">float16</span> <span class="k">else</span> <span class="mi">4</span>
        <span class="k">if</span> <span class="n">cache_sets</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">total_memory</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">get_device_properties</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">current_device</span>
            <span class="p">)</span><span class="o">.</span><span class="n">total_memory</span>
            <span class="n">free_memory</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">total_memory</span>
                <span class="o">-</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">memory_reserved</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">current_device</span><span class="p">)</span>
                <span class="o">-</span> <span class="nb">int</span><span class="p">(</span><span class="n">cache_reserved_memory</span><span class="p">)</span>
            <span class="p">)</span>
            <span class="k">assert</span> <span class="n">free_memory</span> <span class="o">&gt;</span> <span class="mi">0</span>
            <span class="n">cache_sets</span> <span class="o">=</span> <span class="p">(</span>
                <span class="nb">int</span><span class="p">(</span><span class="n">cache_state</span><span class="o">.</span><span class="n">total_cache_hash_size</span> <span class="o">*</span> <span class="n">cache_load_factor</span><span class="p">)</span>
                <span class="o">+</span> <span class="n">DEFAULT_ASSOC</span>
                <span class="o">-</span> <span class="mi">1</span>
            <span class="p">)</span> <span class="o">//</span> <span class="n">DEFAULT_ASSOC</span>
            <span class="n">cache_sets</span> <span class="o">=</span> <span class="mi">1</span> <span class="k">if</span> <span class="n">cache_sets</span> <span class="o">==</span> <span class="mi">0</span> <span class="k">else</span> <span class="n">cache_sets</span>
            <span class="n">cache_size</span> <span class="o">=</span> <span class="n">cache_sets</span> <span class="o">*</span> <span class="n">DEFAULT_ASSOC</span> <span class="o">*</span> <span class="n">element_size</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_D_cache</span>
            <span class="k">if</span> <span class="n">cache_size</span> <span class="o">&gt;</span> <span class="n">free_memory</span><span class="p">:</span>
                <span class="n">cache_sets</span> <span class="o">=</span> <span class="p">(</span>
                    <span class="nb">int</span><span class="p">(</span><span class="mf">1.0</span> <span class="o">*</span> <span class="n">free_memory</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_D_cache</span> <span class="o">/</span> <span class="n">element_size</span><span class="p">)</span>
                    <span class="o">+</span> <span class="n">DEFAULT_ASSOC</span>
                    <span class="o">-</span> <span class="mi">1</span>
                <span class="p">)</span> <span class="o">//</span> <span class="n">DEFAULT_ASSOC</span>
        <span class="n">cache_load_factor</span> <span class="o">=</span> <span class="p">(</span>
            <span class="mf">1.0</span> <span class="o">*</span> <span class="n">cache_sets</span> <span class="o">*</span> <span class="n">DEFAULT_ASSOC</span> <span class="o">/</span> <span class="nb">int</span><span class="p">(</span><span class="n">cache_state</span><span class="o">.</span><span class="n">total_cache_hash_size</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="k">assert</span> <span class="n">cache_sets</span> <span class="o">&gt;</span> <span class="mi">0</span>
        <span class="k">if</span> <span class="n">cache_algorithm</span> <span class="o">==</span> <span class="n">CacheAlgorithm</span><span class="o">.</span><span class="n">LFU</span><span class="p">:</span>
            <span class="k">assert</span> <span class="n">cache_sets</span> <span class="o">&lt;</span> <span class="mi">2</span><span class="o">**</span><span class="mi">24</span> <span class="o">-</span> <span class="mi">1</span>
        <span class="n">cache_size</span> <span class="o">=</span> <span class="n">cache_sets</span> <span class="o">*</span> <span class="n">DEFAULT_ASSOC</span> <span class="o">*</span> <span class="n">element_size</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_D_cache</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">log</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;Using on-device cache with admission algorithm &quot;</span>
            <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">cache_algorithm</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">cache_sets</span><span class="si">}</span><span class="s2"> sets, &quot;</span>
            <span class="sa">f</span><span class="s2">&quot;load_factor: </span><span class="si">{</span><span class="n">cache_load_factor</span><span class="w"> </span><span class="si">:</span><span class="s2"> .3f</span><span class="si">}</span><span class="s2">, &quot;</span>
            <span class="sa">f</span><span class="s2">&quot;cache_size: </span><span class="si">{</span><span class="n">cache_size</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="mf">1024.0</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="mf">1024.0</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="mf">1024.0</span><span class="w"> </span><span class="si">:</span><span class="s2"> .2f</span><span class="si">}</span><span class="s2">GB, &quot;</span>
            <span class="sa">f</span><span class="s2">&quot;cache_precision: </span><span class="si">{</span><span class="n">dtype</span><span class="si">}</span><span class="s2">, &quot;</span>
            <span class="sa">f</span><span class="s2">&quot;weights_precision: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">weights_precision</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">total_cache_hash_size</span> <span class="o">=</span> <span class="n">cache_state</span><span class="o">.</span><span class="n">total_cache_hash_size</span>
        <span class="c1"># 8x of # tables, trivial size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span>
            <span class="s2">&quot;cache_hash_size_cumsum&quot;</span><span class="p">,</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span>
                <span class="n">cache_state</span><span class="o">.</span><span class="n">cache_hash_size_cumsum</span><span class="p">,</span>
                <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">current_device</span><span class="p">,</span>
                <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int64</span><span class="p">,</span>
            <span class="p">),</span>
        <span class="p">)</span>
        <span class="c1"># 4x total embedding hash size with uvm cache</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span>
            <span class="s2">&quot;cache_index_table_map&quot;</span><span class="p">,</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span>
                <span class="n">cache_state</span><span class="o">.</span><span class="n">cache_index_table_map</span><span class="p">,</span>
                <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">current_device</span><span class="p">,</span>
                <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span>
            <span class="p">),</span>
        <span class="p">)</span>
        <span class="c1"># 8x of total cache slots (embedding hash size * clf)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span>
            <span class="s2">&quot;lxu_cache_state&quot;</span><span class="p">,</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span>
                <span class="n">cache_sets</span><span class="p">,</span> <span class="n">DEFAULT_ASSOC</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">current_device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int64</span>
            <span class="p">)</span><span class="o">.</span><span class="n">fill_</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span>
        <span class="p">)</span>
        <span class="c1"># Cache itself, not auxiliary size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span>
            <span class="s2">&quot;lxu_cache_weights&quot;</span><span class="p">,</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span>
                <span class="n">cache_sets</span> <span class="o">*</span> <span class="n">DEFAULT_ASSOC</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">max_D_cache</span><span class="p">,</span>
                <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">current_device</span><span class="p">,</span>
                <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span>
            <span class="p">),</span>
        <span class="p">)</span>
        <span class="c1"># LRU: 8x of total cache slots (embedding hash size * clf)</span>
        <span class="c1"># LFU: 8x of total embedding hash size with uvm cache</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span>
            <span class="s2">&quot;lxu_state&quot;</span><span class="p">,</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span>
                <span class="n">size</span><span class="o">=</span><span class="p">(</span>
                    <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">total_cache_hash_size</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,)</span>
                    <span class="k">if</span> <span class="n">cache_algorithm</span> <span class="o">==</span> <span class="n">CacheAlgorithm</span><span class="o">.</span><span class="n">LFU</span>
                    <span class="k">else</span> <span class="p">(</span><span class="n">cache_sets</span><span class="p">,</span> <span class="n">DEFAULT_ASSOC</span><span class="p">)</span>
                <span class="p">),</span>
                <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">current_device</span><span class="p">,</span>
                <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int64</span><span class="p">,</span>
            <span class="p">),</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span>
            <span class="s2">&quot;cache_miss_counter&quot;</span><span class="p">,</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">current_device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int64</span><span class="p">),</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_init_uvm_cache_counter</span><span class="p">(</span><span class="n">cache_sets</span><span class="p">,</span> <span class="n">persistent</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">prefetch_pipeline</span><span class="p">:</span>
            <span class="c1"># using the placeholder_autograd_tensor to make sure</span>
            <span class="c1"># the hook is executed after the backward pass</span>
            <span class="c1"># not using register_module_full_backward_hook</span>
            <span class="c1"># due to https://github.com/pytorch/pytorch/issues/100528</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">placeholder_autograd_tensor</span><span class="o">.</span><span class="n">register_hook</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_sync_stream_post_backward</span>
            <span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">register_full_backward_pre_hook</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_update_cache_counter_and_locations</span>
            <span class="p">)</span>

        <span class="k">if</span> <span class="n">cache_algorithm</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">(</span><span class="n">CacheAlgorithm</span><span class="o">.</span><span class="n">LFU</span><span class="p">,</span> <span class="n">CacheAlgorithm</span><span class="o">.</span><span class="n">LRU</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;cache_algorithm must be </span><span class="si">{</span><span class="n">CacheAlgorithm</span><span class="o">.</span><span class="n">LRU</span><span class="si">}</span><span class="s2"> &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;or </span><span class="si">{</span><span class="n">CacheAlgorithm</span><span class="o">.</span><span class="n">LFU</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="p">)</span>

    <span class="c1"># pyre-ignore</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">_recording_to_timer</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">timer</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">AsyncSeriesTimer</span><span class="p">],</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="n">Any</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Any</span><span class="p">:</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">stats_reporter</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">stats_reporter</span><span class="o">.</span><span class="n">should_report</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">step</span>
        <span class="p">):</span>
            <span class="k">assert</span> <span class="p">(</span>
                <span class="n">timer</span>
            <span class="p">),</span> <span class="s2">&quot;We shouldn&#39;t be here, async timer must have been initiated if reporter is present.&quot;</span>
            <span class="k">return</span> <span class="n">timer</span><span class="o">.</span><span class="n">recording</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="c1"># No-Op context manager</span>
        <span class="k">return</span> <span class="n">contextlib</span><span class="o">.</span><span class="n">nullcontext</span><span class="p">()</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_sync_stream_post_backward</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">grad</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        backward hook function when prefetch_pipeline is enabled.</span>

<span class="sd">        With the pipeline, prefetch(batch_{i+2}) may overlap with backward(batch_{i}).</span>
<span class="sd">        There is race condition that backward(batch_i) writes to UVM memory and</span>
<span class="sd">        at the same time prefetch(batch_{i+2}) loads UVM memory to cache. This stream sync forces</span>
<span class="sd">        backward(batch_i) to finish before prefetch(batch_{i+2}).</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">prefetch_stream</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">prefetch_stream</span><span class="o">.</span><span class="n">wait_stream</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">current_stream</span><span class="p">())</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_update_cache_counter_and_locations</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">module</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
        <span class="n">grad_input</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span> <span class="n">Tensor</span><span class="p">],</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Backward prehook function when prefetch_pipeline is enabled.</span>

<span class="sd">        This function does 3 things:</span>
<span class="sd">        1. backward stream waits for prefetch stream to finish.</span>
<span class="sd">        Otherwise the prefetch(batch_{i+1}) might overlap with backward(batch_i).</span>
<span class="sd">        If an idx is not in cache in batch_i, but it is being inserted in batch_{i+1},</span>
<span class="sd">        there is race condition that backward(batch_i) writes to UVM memory and</span>
<span class="sd">        at the same time prefetch(batch_{i+1}) loads UVM memory to cache.</span>

<span class="sd">        2. decrement the lxu_cache_locking_counter to indicate the current batch is finished.</span>
<span class="sd">        The lxu_cache_locking_counter is updated in both prefetch and TBE backward.</span>
<span class="sd">        As there is no overlap between prefetch and backward, we can decrement either before or</span>
<span class="sd">        after backward. It&#39;s better to decrement before lxu_cache_locations gets updated.</span>

<span class="sd">        3. update lxu_cache_locations to address the cache inconsistency issue.</span>
<span class="sd">        In the case that the same index is not inserted into cache in batch_i,</span>
<span class="sd">        but it is inserted in batch_{i+1}, the cache can be invalid in</span>
<span class="sd">        the sense that the cached weight for this index does not have the</span>
<span class="sd">        backward update of batch_i.</span>

<span class="sd">        Example of the issue is as follows:</span>
<span class="sd">        idx is in batch_i, batch_{i+1}</span>
<span class="sd">        prefetch(batch_i)</span>
<span class="sd">          - failed to insert idx into cache, cache_locations_batch_i of idx is -1 (cache miss)</span>
<span class="sd">        forward(batch_i)</span>
<span class="sd">        prefetch(batch_{i+1})</span>
<span class="sd">          - insert idx into cache, cache is loaded from host memory</span>
<span class="sd">        backward(batch_i)</span>
<span class="sd">          - cache_locations_batch_i of idx is -1, the host memory is updated</span>
<span class="sd">        forward(batch_{i+1})</span>
<span class="sd">          - OUTPUT IS WRONG. the weight for idx is fetched from cache, but the cache is outdated.</span>

<span class="sd">        The fix to this cache inconsistency is to update the cache_locations_batch_i before backward of batch_i,</span>
<span class="sd">        so that the cache gets updated correctly by the backward pass of TBE.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">prefetch_stream</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># need to wait for the prefetch of next batch,</span>
            <span class="c1"># so that cache states are valid</span>
            <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">_recording_to_timer</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">bwd_wait_prefetch_timer</span><span class="p">,</span>
                <span class="n">context</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">step</span><span class="p">,</span>
                <span class="n">stream</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">current_stream</span><span class="p">(),</span>
            <span class="p">):</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">current_stream</span><span class="p">()</span><span class="o">.</span><span class="n">wait_stream</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">prefetch_stream</span><span class="p">)</span>

        <span class="n">torch</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">fbgemm</span><span class="o">.</span><span class="n">lxu_cache_locking_counter_decrement</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">lxu_cache_locking_counter</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">lxu_cache_locations</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="c1"># Recompute linear_cache_indices</span>
        <span class="n">linear_cache_indices</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">fbgemm</span><span class="o">.</span><span class="n">linearize_cache_indices</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">cache_hash_size_cumsum</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_indices</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_offsets</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_vbe_B_offsets</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_vbe_max_B</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="p">(</span>
            <span class="n">linear_unique_indices</span><span class="p">,</span>
            <span class="n">linear_unique_indices_length</span><span class="p">,</span>
            <span class="n">_</span><span class="p">,</span>
        <span class="p">)</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">fbgemm</span><span class="o">.</span><span class="n">get_unique_indices</span><span class="p">(</span>
            <span class="n">linear_cache_indices</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">total_cache_hash_size</span><span class="p">,</span>
            <span class="n">compute_count</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">fbgemm</span><span class="o">.</span><span class="n">lxu_cache_lookup</span><span class="p">(</span>
            <span class="n">linear_unique_indices</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">lxu_cache_state</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">total_cache_hash_size</span><span class="p">,</span>
            <span class="n">gather_cache_stats</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>  <span class="c1"># not collecting cache stats</span>
            <span class="n">num_uniq_cache_indices</span><span class="o">=</span><span class="n">linear_unique_indices_length</span><span class="p">,</span>
            <span class="n">lxu_cache_locations_output</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">lxu_cache_locations</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_init_uvm_cache_counter</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">cache_sets</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">persistent</span><span class="p">:</span> <span class="nb">bool</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">prefetch_pipeline</span> <span class="ow">and</span> <span class="n">persistent</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span>
                <span class="s2">&quot;lxu_cache_locking_counter&quot;</span><span class="p">,</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span>
                    <span class="n">cache_sets</span><span class="p">,</span>
                    <span class="n">DEFAULT_ASSOC</span><span class="p">,</span>
                    <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">current_device</span><span class="p">,</span>
                    <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span>
                <span class="p">),</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span>
                <span class="s2">&quot;lxu_cache_locking_counter&quot;</span><span class="p">,</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">current_device</span><span class="p">),</span>
                <span class="n">persistent</span><span class="o">=</span><span class="n">persistent</span><span class="p">,</span>
            <span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_init_uvm_cache_stats</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">gather_uvm_cache_stats</span><span class="p">:</span>
            <span class="c1"># If uvm_cache_stats is not enabled, register stub entries via buffer to state_dict for TorchScript to JIT properly.</span>
            <span class="c1"># Since we&#39;re not using these variables, we can choose minimize tensor size to keep state_dict size small.</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span>
                <span class="s2">&quot;uvm_cache_stats&quot;</span><span class="p">,</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span>
                    <span class="mi">1</span><span class="p">,</span>
                    <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">current_device</span><span class="p">,</span>
                    <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int64</span><span class="p">,</span>
                <span class="p">),</span>
                <span class="n">persistent</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span>
                <span class="s2">&quot;local_uvm_cache_stats&quot;</span><span class="p">,</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span>
                    <span class="mi">1</span><span class="p">,</span>
                    <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">current_device</span><span class="p">,</span>
                    <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span>
                <span class="p">),</span>
                <span class="n">persistent</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span>
                <span class="s2">&quot;uvm_cache_stats&quot;</span><span class="p">,</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span>
                    <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">uvm_cache_stats_size</span><span class="p">,),</span>
                    <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">current_device</span><span class="p">,</span>
                    <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int64</span><span class="p">,</span>
                <span class="p">),</span>
            <span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span>
                <span class="s2">&quot;local_uvm_cache_stats&quot;</span><span class="p">,</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span>
                    <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">uvm_cache_stats_size</span><span class="p">,),</span>
                    <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">current_device</span><span class="p">,</span>
                    <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span>
                <span class="p">),</span>
            <span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">reset_uvm_cache_stats</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">last_uvm_cache_print_state</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">uvm_cache_stats</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">reset_cache_states</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="c1"># pyre-fixme[29]: `Union[(self: TensorBase) -&gt; int, Module, Tensor]` is not</span>
        <span class="c1">#  a function.</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">lxu_cache_weights</span><span class="o">.</span><span class="n">numel</span><span class="p">():</span>
            <span class="k">return</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lxu_cache_state</span><span class="o">.</span><span class="n">fill_</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lxu_state</span><span class="o">.</span><span class="n">fill_</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">timestep</span> <span class="o">=</span> <span class="mi">1</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">reset_embedding_weight_momentum</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">pruned_indices</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">pruned_indices_offsets</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">logical_table_ids</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">buffer_ids</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span> <span class="o">==</span> <span class="n">OptimType</span><span class="o">.</span><span class="n">NONE</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Resetting embedding weight momentum is not supported for </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="p">)</span>
        <span class="n">total_cache_hash_size</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">total_cache_hash_size</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">):</span>
            <span class="n">total_cache_hash_size</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">total_cache_hash_size</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">total_cache_hash_size</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">total_cache_hash_size</span>

        <span class="n">rowwise</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span> <span class="ow">in</span> <span class="p">[</span>
            <span class="n">OptimType</span><span class="o">.</span><span class="n">EXACT_ROWWISE_ADAGRAD</span><span class="p">,</span>
        <span class="p">]</span>
        <span class="k">if</span> <span class="n">rowwise</span><span class="p">:</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">fbgemm</span><span class="o">.</span><span class="n">reset_weight_momentum</span><span class="p">(</span>
                <span class="n">dev_weights</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">weights_dev</span><span class="p">,</span>
                <span class="n">uvm_weights</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">weights_uvm</span><span class="p">,</span>
                <span class="n">lxu_cache_weights</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">lxu_cache_weights</span><span class="p">,</span>
                <span class="n">weights_placements</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">weights_placements</span><span class="p">,</span>
                <span class="n">weights_offsets</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">weights_offsets</span><span class="p">,</span>
                <span class="n">momentum1_dev</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">momentum1_dev</span><span class="p">,</span>
                <span class="n">momentum1_uvm</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">momentum1_uvm</span><span class="p">,</span>
                <span class="n">momentum1_placements</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">momentum1_placements</span><span class="p">,</span>
                <span class="n">momentum1_offsets</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">momentum1_offsets</span><span class="p">,</span>
                <span class="n">D_offsets</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">D_offsets</span><span class="p">,</span>
                <span class="n">pruned_indices</span><span class="o">=</span><span class="n">pruned_indices</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">current_device</span><span class="p">),</span>
                <span class="n">pruned_indices_offsets</span><span class="o">=</span><span class="n">pruned_indices_offsets</span><span class="o">.</span><span class="n">to</span><span class="p">(</span>
                    <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">current_device</span>
                <span class="p">),</span>
                <span class="n">logical_table_ids</span><span class="o">=</span><span class="n">logical_table_ids</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">current_device</span><span class="p">),</span>
                <span class="n">buffer_ids</span><span class="o">=</span><span class="n">buffer_ids</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">current_device</span><span class="p">),</span>
                <span class="n">cache_hash_size_cumsum</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">cache_hash_size_cumsum</span><span class="p">,</span>
                <span class="n">lxu_cache_state</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">lxu_cache_state</span><span class="p">,</span>
                <span class="n">total_cache_hash_size</span><span class="o">=</span><span class="n">total_cache_hash_size</span><span class="p">,</span>
            <span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">prepare_inputs</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">indices</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">offsets</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">per_sample_weights</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">batch_size_per_feature_per_rank</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">force_cast_input_types</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">prefetch_pipeline</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">],</span> <span class="n">invokers</span><span class="o">.</span><span class="n">lookup_args</span><span class="o">.</span><span class="n">VBEMetadata</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Prepare TBE inputs as follows:</span>

<span class="sd">        (1) Create VBE metadata</span>
<span class="sd">        (2) Convert input types if `force_cast_input_types=True`</span>
<span class="sd">        (3) Run `bounds_check_indices` if `bounds_check_mode` is not</span>
<span class="sd">            BoundsCheckMode.NONE</span>

<span class="sd">        Args:</span>
<span class="sd">            indices (Tensor): Input indices</span>
<span class="sd">            offsets (Tensor): Input offsets</span>
<span class="sd">            per_sample_weights (Optional[Tensor]): Input per sample</span>
<span class="sd">                weights</span>
<span class="sd">            batch_size_per_feature_per_rank</span>
<span class="sd">                (Optional[List[List[int]]]): A 2D tensor of batch size</span>
<span class="sd">                for each rank and feature. Shape = (number of</span>
<span class="sd">                features, number of ranks)</span>
<span class="sd">            force_cast_input_types (bool): A flag to force convert</span>
<span class="sd">                input types if set to True</span>

<span class="sd">        Returns:</span>
<span class="sd">            A tuple of indices, offsets, per_sample_weights, and VBE</span>
<span class="sd">            metadata</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="c1"># Generate VBE metadata</span>
        <span class="n">vbe_metadata</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_generate_vbe_metadata</span><span class="p">(</span>
            <span class="n">offsets</span><span class="p">,</span> <span class="n">batch_size_per_feature_per_rank</span>
        <span class="p">)</span>

        <span class="n">vbe</span> <span class="o">=</span> <span class="n">vbe_metadata</span><span class="o">.</span><span class="n">B_offsets</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
        <span class="c1"># Note this check has already been done in C++ side</span>
        <span class="c1"># TODO:  max_B &lt;= self.info_B_mask in python</span>
        <span class="c1"># We cannot use assert as it breaks pt2 compile for dynamic shape</span>
        <span class="c1"># and need to use torch._check for dynamic shape and cannot construct fstring, use constant string.</span>
        <span class="c1"># torch._check(</span>
        <span class="c1">#     max_B &lt;= self.info_B_mask,</span>
        <span class="c1">#     &quot;Not enough infos bits to accommodate T and B.&quot;,</span>
        <span class="c1"># )</span>
        <span class="c1"># We cannot use lambda as it fails jit script.</span>
        <span class="c1"># torch._check is also not supported in jitscript</span>

        <span class="c1"># TODO: remove this and add an assert after updating</span>
        <span class="c1"># bounds_check_indices to support different indices type and offset</span>
        <span class="c1"># type</span>
        <span class="n">force_cast_input_types</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">indices</span><span class="o">.</span><span class="n">dtype</span> <span class="o">!=</span> <span class="n">offsets</span><span class="o">.</span><span class="n">dtype</span> <span class="ow">or</span> <span class="n">force_cast_input_types</span>
        <span class="p">)</span>

        <span class="k">if</span> <span class="n">force_cast_input_types</span><span class="p">:</span>
            <span class="c1"># NOTE: Force offsets to have the same dtype as indices since the</span>
            <span class="c1"># kernels assume same dtype.  We might need to revisit the assumption</span>
            <span class="c1"># of same dtypes in the future.</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">embedding_table_index_type</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">int32</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">log</span><span class="p">(</span>
                    <span class="s2">&quot;Casting indices to int32 based on embedding_table_index_type input.&quot;</span>
                <span class="p">)</span>
                <span class="n">indices</span> <span class="o">=</span> <span class="n">indices</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">embedding_table_index_type</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">embedding_table_offset_type</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">log</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;Force casting offsets to </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">embedding_table_index_type</span><span class="si">}</span><span class="s2"> so that it is the same as the indices type.&quot;</span>
                <span class="p">)</span>
            <span class="n">offsets</span> <span class="o">=</span> <span class="n">offsets</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">indices</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

            <span class="c1"># Force casting per_sample_weights to float</span>
            <span class="k">if</span> <span class="n">per_sample_weights</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">per_sample_weights</span> <span class="o">=</span> <span class="n">per_sample_weights</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">bounds_check_mode_int</span> <span class="o">!=</span> <span class="n">BoundsCheckMode</span><span class="o">.</span><span class="n">NONE</span><span class="o">.</span><span class="n">value</span><span class="p">:</span>
            <span class="c1"># Override the bounds check version based on prefetch_pipeline</span>
            <span class="n">use_bounds_check_v2</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bounds_check_version</span> <span class="o">==</span> <span class="mi">2</span> <span class="ow">or</span> <span class="n">prefetch_pipeline</span>
            <span class="n">bounds_check_version</span> <span class="o">=</span> <span class="p">(</span>
                <span class="mi">2</span> <span class="k">if</span> <span class="n">use_bounds_check_v2</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">bounds_check_version</span>
            <span class="p">)</span>

            <span class="n">vbe</span> <span class="o">=</span> <span class="n">vbe_metadata</span><span class="o">.</span><span class="n">B_offsets</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>

            <span class="c1"># Compute B info and VBE metadata for bounds_check_indices only if</span>
            <span class="c1"># VBE and bounds check indices v2 are used</span>
            <span class="k">if</span> <span class="n">vbe</span> <span class="ow">and</span> <span class="n">use_bounds_check_v2</span><span class="p">:</span>
                <span class="n">B_offsets</span> <span class="o">=</span> <span class="n">vbe_metadata</span><span class="o">.</span><span class="n">B_offsets</span>
                <span class="n">B_offsets_rank_per_feature</span> <span class="o">=</span> <span class="n">vbe_metadata</span><span class="o">.</span><span class="n">B_offsets_rank_per_feature</span>
                <span class="n">output_offsets_feature_rank</span> <span class="o">=</span> <span class="n">vbe_metadata</span><span class="o">.</span><span class="n">output_offsets_feature_rank</span>
                <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">B_offsets</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">),</span> <span class="s2">&quot;B_offsets must be tensor&quot;</span>
                <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span>
                    <span class="n">B_offsets_rank_per_feature</span><span class="p">,</span> <span class="n">Tensor</span>
                <span class="p">),</span> <span class="s2">&quot;B_offsets_rank_per_feature must be tensor&quot;</span>
                <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span>
                    <span class="n">output_offsets_feature_rank</span><span class="p">,</span> <span class="n">Tensor</span>
                <span class="p">),</span> <span class="s2">&quot;output_offsets_feature_rank must be tensor&quot;</span>

                <span class="n">row_output_offsets</span><span class="p">,</span> <span class="n">b_t_map</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">fbgemm</span><span class="o">.</span><span class="n">generate_vbe_metadata</span><span class="p">(</span>
                    <span class="n">B_offsets</span><span class="p">,</span>
                    <span class="n">B_offsets_rank_per_feature</span><span class="p">,</span>
                    <span class="n">output_offsets_feature_rank</span><span class="p">,</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">D_offsets</span><span class="p">,</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">max_D</span><span class="p">,</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">is_nobag</span><span class="p">,</span>
                    <span class="n">vbe_metadata</span><span class="o">.</span><span class="n">max_B_feature_rank</span><span class="p">,</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">info_B_num_bits</span><span class="p">,</span>
                    <span class="n">offsets</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span>  <span class="c1"># total_B</span>
                <span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">b_t_map</span> <span class="o">=</span> <span class="kc">None</span>

            <span class="n">torch</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">fbgemm</span><span class="o">.</span><span class="n">bounds_check_indices</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">rows_per_table</span><span class="p">,</span>
                <span class="n">indices</span><span class="p">,</span>
                <span class="n">offsets</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">bounds_check_mode_int</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">bounds_check_warning</span><span class="p">,</span>
                <span class="n">per_sample_weights</span><span class="p">,</span>
                <span class="n">B_offsets</span><span class="o">=</span><span class="n">vbe_metadata</span><span class="o">.</span><span class="n">B_offsets</span><span class="p">,</span>
                <span class="n">max_B</span><span class="o">=</span><span class="n">vbe_metadata</span><span class="o">.</span><span class="n">max_B</span><span class="p">,</span>
                <span class="n">b_t_map</span><span class="o">=</span><span class="n">b_t_map</span><span class="p">,</span>
                <span class="n">info_B_num_bits</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">info_B_num_bits</span><span class="p">,</span>
                <span class="n">info_B_mask</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">info_B_mask</span><span class="p">,</span>
                <span class="n">bounds_check_version</span><span class="o">=</span><span class="n">bounds_check_version</span><span class="p">,</span>
                <span class="n">prefetch_pipeline</span><span class="o">=</span><span class="n">prefetch_pipeline</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="k">return</span> <span class="n">indices</span><span class="p">,</span> <span class="n">offsets</span><span class="p">,</span> <span class="n">per_sample_weights</span><span class="p">,</span> <span class="n">vbe_metadata</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_debug_print_input_stats_factory</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Callable</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="kc">None</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        If the environment variable FBGEMM_DEBUG_PRINT_INPUT_STATS=1,</span>
<span class="sd">        return a function pointer of a function that prints input</span>
<span class="sd">        stats including weighted/unweighted, number of features,</span>
<span class="sd">        batch size, average pooling factor, total number of indices,</span>
<span class="sd">        number of unique indices, and number of indices that goes</span>
<span class="sd">        through the different backward functions. Otherwise, return</span>
<span class="sd">        a dummy function pointer.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="nd">@torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">ignore</span>
        <span class="k">def</span><span class="w"> </span><span class="nf">_debug_print_input_stats_factory_impl</span><span class="p">(</span>
            <span class="n">indices</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
            <span class="n">offsets</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
            <span class="n">per_sample_weights</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">            </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">            Print input stats (for debugging purpose only)</span>

<span class="sd">            Args:</span>
<span class="sd">                indices (Tensor): Input indices</span>
<span class="sd">                offsets (Tensor): Input offsets</span>
<span class="sd">                per_sample_weights (Optional[Tensor]): Input per</span>
<span class="sd">                    sample weights</span>
<span class="sd">            &quot;&quot;&quot;</span>
            <span class="c1"># pyre-fixme[29]: `Union[(self: TensorBase, other: Union[bool, complex,</span>
            <span class="c1">#  float, int, Tensor]) -&gt; Tensor, Module, Tensor]` is not a function.</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">debug_step</span> <span class="o">%</span> <span class="mi">100</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="c1"># Get number of features (T) and batch size (B)</span>
                <span class="n">T</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">feature_table_map</span><span class="p">)</span>
                <span class="n">B</span> <span class="o">=</span> <span class="p">(</span><span class="n">offsets</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">//</span> <span class="n">T</span>

                <span class="c1"># Transfer hash_size_cumsum, indices and offsets to CPU</span>
                <span class="n">hash_size_cumsum_cpu</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">hash_size_cumsum</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span>
                <span class="n">indices_cpu</span> <span class="o">=</span> <span class="n">indices</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span>
                <span class="n">offsets_cpu</span> <span class="o">=</span> <span class="n">offsets</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span>

                <span class="c1"># Compute linear indices</span>
                <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">T</span><span class="p">):</span>
                    <span class="n">start</span> <span class="o">=</span> <span class="n">offsets_cpu</span><span class="p">[</span><span class="n">B</span> <span class="o">*</span> <span class="n">t</span><span class="p">]</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
                    <span class="n">end</span> <span class="o">=</span> <span class="n">offsets_cpu</span><span class="p">[</span><span class="n">B</span> <span class="o">*</span> <span class="p">(</span><span class="n">t</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)]</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
                    <span class="n">indices_cpu</span><span class="p">[</span><span class="n">start</span><span class="p">:</span><span class="n">end</span><span class="p">]</span> <span class="o">+=</span> <span class="n">hash_size_cumsum_cpu</span><span class="p">[</span><span class="n">t</span><span class="p">]</span>

                <span class="c1"># Compute unique indices</span>
                <span class="n">uniq_indices_cpu</span><span class="p">,</span> <span class="n">counts</span> <span class="o">=</span> <span class="n">indices_cpu</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">return_counts</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

                <span class="c1"># Compute num unique indices</span>
                <span class="n">num_uniq_indices</span> <span class="o">=</span> <span class="n">uniq_indices_cpu</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span>

                <span class="c1"># The warp_per_row kernel handles indices that their</span>
                <span class="c1"># segment lengths &lt;= 32</span>
                <span class="c1">#</span>
                <span class="c1"># The cta_per_row kernel handles indices that their</span>
                <span class="c1"># segment lengths &gt; 32. A single thread block is used</span>
                <span class="c1"># if segment lengths &lt;= 1024. Otherwise, multiple</span>
                <span class="c1"># thread blocks are used.</span>
                <span class="c1">#</span>
                <span class="c1"># Counts of indices that segment lengths &lt;= 32</span>
                <span class="n">counts_warp_per_row</span> <span class="o">=</span> <span class="n">counts</span><span class="p">[</span><span class="n">counts</span> <span class="o">&lt;=</span> <span class="mi">32</span><span class="p">]</span>
                <span class="n">counts_cta_per_row</span> <span class="o">=</span> <span class="n">counts</span><span class="p">[</span><span class="n">counts</span> <span class="o">&gt;</span> <span class="mi">32</span><span class="p">]</span>
                <span class="c1"># Counts of indices that segment lengths &gt; 32 and &lt;= 1024</span>
                <span class="n">counts_cta_per_row_sth</span> <span class="o">=</span> <span class="n">counts_cta_per_row</span><span class="p">[</span><span class="n">counts_cta_per_row</span> <span class="o">&lt;=</span> <span class="mi">1024</span><span class="p">]</span>
                <span class="c1"># Counts of indices that segment lengths &gt; 1024</span>
                <span class="n">counts_cta_per_row_mth</span> <span class="o">=</span> <span class="n">counts_cta_per_row</span><span class="p">[</span><span class="n">counts_cta_per_row</span> <span class="o">&gt;</span> <span class="mi">1024</span><span class="p">]</span>

                <span class="k">def</span><span class="w"> </span><span class="nf">compute_numel_and_avg</span><span class="p">(</span><span class="n">counts</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">float</span><span class="p">]:</span>
                    <span class="n">numel</span> <span class="o">=</span> <span class="n">counts</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span>
                    <span class="n">avg</span> <span class="o">=</span> <span class="p">(</span><span class="n">counts</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="o">/</span> <span class="n">numel</span><span class="p">)</span> <span class="k">if</span> <span class="n">numel</span> <span class="o">!=</span> <span class="mi">0</span> <span class="k">else</span> <span class="o">-</span><span class="mf">1.0</span>
                    <span class="k">return</span> <span class="n">numel</span><span class="p">,</span> <span class="n">avg</span>

                <span class="c1"># warp_per_row stats</span>
                <span class="n">num_warp_per_row</span><span class="p">,</span> <span class="n">avg_seglen_warp_per_row</span> <span class="o">=</span> <span class="n">compute_numel_and_avg</span><span class="p">(</span>
                    <span class="n">counts_warp_per_row</span>
                <span class="p">)</span>
                <span class="c1"># cta_per_row using a single thread block stats</span>
                <span class="n">num_cta_per_row_sth</span><span class="p">,</span> <span class="n">avg_seglen_cta_per_row_sth</span> <span class="o">=</span> <span class="n">compute_numel_and_avg</span><span class="p">(</span>
                    <span class="n">counts_cta_per_row_sth</span>
                <span class="p">)</span>
                <span class="c1"># cta_per_row using multiple thread block stats</span>
                <span class="n">num_cta_per_row_mth</span><span class="p">,</span> <span class="n">avg_seglen_cta_per_row_mth</span> <span class="o">=</span> <span class="n">compute_numel_and_avg</span><span class="p">(</span>
                    <span class="n">counts_cta_per_row_mth</span>
                <span class="p">)</span>

                <span class="k">assert</span> <span class="n">num_uniq_indices</span> <span class="o">==</span> <span class="p">(</span>
                    <span class="n">num_warp_per_row</span> <span class="o">+</span> <span class="n">num_cta_per_row_sth</span> <span class="o">+</span> <span class="n">num_cta_per_row_mth</span>
                <span class="p">)</span>

                <span class="bp">self</span><span class="o">.</span><span class="n">log</span><span class="p">(</span>
                    <span class="s2">&quot;TBE_DEBUG: &quot;</span>
                    <span class="s2">&quot;weighted </span><span class="si">{}</span><span class="s2"> &quot;</span>
                    <span class="s2">&quot;num features </span><span class="si">{}</span><span class="s2"> &quot;</span>
                    <span class="s2">&quot;batch size </span><span class="si">{}</span><span class="s2"> &quot;</span>
                    <span class="s2">&quot;avg pooling factor </span><span class="si">{:.2f}</span><span class="s2"> &quot;</span>
                    <span class="s2">&quot;total num indices </span><span class="si">{}</span><span class="s2"> &quot;</span>
                    <span class="s2">&quot;num unique indices </span><span class="si">{}</span><span class="s2"> &quot;</span>
                    <span class="s2">&quot;num warp_per_row </span><span class="si">{}</span><span class="s2"> (avg segment length </span><span class="si">{:.2f}</span><span class="s2">) &quot;</span>
                    <span class="s2">&quot;num cta_per_row single thread block (avg segment length) </span><span class="si">{}</span><span class="s2"> (</span><span class="si">{:.2f}</span><span class="s2">) &quot;</span>
                    <span class="s2">&quot;num cta_per_row multiple thread blocks (avg segment length) </span><span class="si">{}</span><span class="s2"> (</span><span class="si">{:.2f}</span><span class="s2">)&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                        <span class="n">per_sample_weights</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">,</span>
                        <span class="n">T</span><span class="p">,</span>
                        <span class="n">B</span><span class="p">,</span>
                        <span class="n">indices</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span> <span class="o">/</span> <span class="p">(</span><span class="n">B</span> <span class="o">*</span> <span class="n">T</span><span class="p">),</span>
                        <span class="n">indices</span><span class="o">.</span><span class="n">numel</span><span class="p">(),</span>
                        <span class="n">num_uniq_indices</span><span class="p">,</span>
                        <span class="n">num_warp_per_row</span><span class="p">,</span>
                        <span class="n">avg_seglen_warp_per_row</span><span class="p">,</span>
                        <span class="n">num_cta_per_row_sth</span><span class="p">,</span>
                        <span class="n">avg_seglen_cta_per_row_sth</span><span class="p">,</span>
                        <span class="n">num_cta_per_row_mth</span><span class="p">,</span>
                        <span class="n">avg_seglen_cta_per_row_mth</span><span class="p">,</span>
                    <span class="p">)</span>
                <span class="p">)</span>
            <span class="c1"># pyre-fixme[16]: `SplitTableBatchedEmbeddingBagsCodegen` has no</span>
            <span class="c1">#  attribute `debug_step`.</span>
            <span class="c1"># pyre-fixme[29]: `Union[(self: TensorBase, other: Union[bool, complex,</span>
            <span class="c1">#  float, int, Tensor]) -&gt; Tensor, Module, Tensor]` is not a function.</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">debug_step</span> <span class="o">+=</span> <span class="mi">1</span>

        <span class="nd">@torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">ignore</span>
        <span class="k">def</span><span class="w"> </span><span class="nf">_debug_print_input_stats_factory_null</span><span class="p">(</span>
            <span class="n">indices</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
            <span class="n">offsets</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
            <span class="n">per_sample_weights</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">pass</span>

        <span class="k">if</span> <span class="nb">int</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;FBGEMM_DEBUG_PRINT_INPUT_STATS&quot;</span><span class="p">,</span> <span class="s2">&quot;0&quot;</span><span class="p">))</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="c1"># pyre-fixme[16]: `SplitTableBatchedEmbeddingBagsCodegen` has no</span>
            <span class="c1">#  attribute `debug_step`.</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">debug_step</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="k">return</span> <span class="n">_debug_print_input_stats_factory_impl</span>
        <span class="k">return</span> <span class="n">_debug_print_input_stats_factory_null</span></div>


<span class="k">class</span><span class="w"> </span><span class="nc">DenseTableBatchedEmbeddingBagsCodegen</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Table-batched version of nn.EmbeddingBag(sparse=False)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">weights</span><span class="p">:</span> <span class="n">Tensor</span>
    <span class="n">weights_offsets</span><span class="p">:</span> <span class="n">Tensor</span>
    <span class="n">D_offsets</span><span class="p">:</span> <span class="n">Tensor</span>
    <span class="n">total_D</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">max_D</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">hash_size_cumsum</span><span class="p">:</span> <span class="n">Tensor</span>
    <span class="n">total_hash_size_bits</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">embedding_specs</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">]]</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">embedding_specs</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">]],</span>  <span class="c1"># tuple of (rows, dims)</span>
        <span class="n">feature_table_map</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>  <span class="c1"># [T]</span>
        <span class="n">weights_precision</span><span class="p">:</span> <span class="n">SparseType</span> <span class="o">=</span> <span class="n">SparseType</span><span class="o">.</span><span class="n">FP32</span><span class="p">,</span>
        <span class="n">pooling_mode</span><span class="p">:</span> <span class="n">PoolingMode</span> <span class="o">=</span> <span class="n">PoolingMode</span><span class="o">.</span><span class="n">SUM</span><span class="p">,</span>
        <span class="n">use_cpu</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">output_dtype</span><span class="p">:</span> <span class="n">SparseType</span> <span class="o">=</span> <span class="n">SparseType</span><span class="o">.</span><span class="n">FP32</span><span class="p">,</span>
        <span class="n">use_mtia</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>  <span class="c1"># noqa C901  # tuple of (rows, dims,)</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">DenseTableBatchedEmbeddingBagsCodegen</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">uuid</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="n">uuid</span><span class="o">.</span><span class="n">uuid4</span><span class="p">())</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">log</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;Feature Gates: </span><span class="si">{</span><span class="p">[(</span><span class="n">feature</span><span class="o">.</span><span class="n">name</span><span class="p">,</span><span class="w"> </span><span class="n">feature</span><span class="o">.</span><span class="n">is_enabled</span><span class="p">())</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">feature</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">FeatureGateName</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">pooling_mode</span> <span class="o">=</span> <span class="n">pooling_mode</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weights_precision</span> <span class="o">=</span> <span class="n">weights_precision</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">output_dtype</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="n">output_dtype</span><span class="o">.</span><span class="n">as_int</span><span class="p">()</span>
        <span class="n">table_embedding_dtype</span> <span class="o">=</span> <span class="n">weights_precision</span><span class="o">.</span><span class="n">as_dtype</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">use_cpu</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="n">use_cpu</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">use_mtia</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="n">use_mtia</span>

        <span class="k">assert</span> <span class="ow">not</span> <span class="p">(</span><span class="n">use_cpu</span> <span class="ow">and</span> <span class="n">use_mtia</span><span class="p">),</span> <span class="s2">&quot;Cannot use CPU and MTIA at the same time&quot;</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_cpu</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">pooling_mode</span> <span class="o">==</span> <span class="n">PoolingMode</span><span class="o">.</span><span class="n">NONE</span><span class="p">:</span>
            <span class="k">assert</span> <span class="n">output_dtype</span> <span class="ow">in</span> <span class="p">[</span>
                <span class="n">SparseType</span><span class="o">.</span><span class="n">FP32</span><span class="p">,</span>
                <span class="n">SparseType</span><span class="o">.</span><span class="n">FP16</span><span class="p">,</span>
                <span class="n">SparseType</span><span class="o">.</span><span class="n">BF16</span><span class="p">,</span>
            <span class="p">],</span> <span class="s2">&quot;Fused pooled embedding quantization only supported for cuda.&quot;</span>

        <span class="c1"># pyre-fixme[8]: Attribute has type `device`; used as `Union[int, device]`.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">current_device</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cpu&quot;</span><span class="p">)</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_cpu</span>
            <span class="k">else</span> <span class="p">(</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;mtia:</span><span class="si">{</span><span class="n">torch</span><span class="o">.</span><span class="n">mtia</span><span class="o">.</span><span class="n">current_device</span><span class="p">()</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_mtia</span>
                <span class="k">else</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">current_device</span><span class="p">()</span>
            <span class="p">)</span>
        <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">embedding_specs</span> <span class="o">=</span> <span class="n">embedding_specs</span>
        <span class="p">(</span><span class="n">rows</span><span class="p">,</span> <span class="n">dims</span><span class="p">)</span> <span class="o">=</span> <span class="nb">zip</span><span class="p">(</span><span class="o">*</span><span class="n">embedding_specs</span><span class="p">)</span>
        <span class="n">T_</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">embedding_specs</span><span class="p">)</span>
        <span class="k">assert</span> <span class="n">T_</span> <span class="o">&gt;</span> <span class="mi">0</span>

        <span class="n">feature_table_map</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">feature_table_map</span> <span class="k">if</span> <span class="n">feature_table_map</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">T_</span><span class="p">))</span>
        <span class="p">)</span>
        <span class="n">T</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">feature_table_map</span><span class="p">)</span>
        <span class="k">assert</span> <span class="n">T_</span> <span class="o">&lt;=</span> <span class="n">T</span>

        <span class="n">feature_dims</span> <span class="o">=</span> <span class="p">[</span><span class="n">dims</span><span class="p">[</span><span class="n">t</span><span class="p">]</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">feature_table_map</span><span class="p">]</span>
        <span class="n">D_offsets</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="nb">list</span><span class="p">(</span><span class="n">accumulate</span><span class="p">(</span><span class="n">feature_dims</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">total_D</span> <span class="o">=</span> <span class="n">D_offsets</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_D</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">dims</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span>
            <span class="s2">&quot;D_offsets&quot;</span><span class="p">,</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">D_offsets</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">current_device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int32</span><span class="p">),</span>
        <span class="p">)</span>
        <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">D_offsets</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span> <span class="o">==</span> <span class="n">T</span> <span class="o">+</span> <span class="mi">1</span>

        <span class="c1"># Required for VBE</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span>
            <span class="s2">&quot;feature_dims&quot;</span><span class="p">,</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">feature_dims</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s2">&quot;cpu&quot;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int64</span><span class="p">),</span>
        <span class="p">)</span>

        <span class="n">hash_size_cumsum</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="nb">list</span><span class="p">(</span><span class="n">accumulate</span><span class="p">(</span><span class="n">rows</span><span class="p">))</span>
        <span class="k">if</span> <span class="n">hash_size_cumsum</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">total_hash_size_bits</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">total_hash_size_bits</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">log2</span><span class="p">(</span><span class="nb">float</span><span class="p">(</span><span class="n">hash_size_cumsum</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]))</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
        <span class="c1"># The last element is to easily access # of rows of each table by</span>
        <span class="c1"># hash_size_cumsum[t + 1] - hash_size_cumsum[t]</span>
        <span class="n">hash_size_cumsum</span> <span class="o">=</span> <span class="p">[</span><span class="n">hash_size_cumsum</span><span class="p">[</span><span class="n">t</span><span class="p">]</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">feature_table_map</span><span class="p">]</span> <span class="o">+</span> <span class="p">[</span>
            <span class="n">hash_size_cumsum</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span>
            <span class="s2">&quot;hash_size_cumsum&quot;</span><span class="p">,</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span>
                <span class="n">hash_size_cumsum</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">current_device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int64</span>
            <span class="p">),</span>
        <span class="p">)</span>
        <span class="n">weights_offsets</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="nb">list</span><span class="p">(</span>
            <span class="n">accumulate</span><span class="p">([</span><span class="n">row</span> <span class="o">*</span> <span class="n">dim</span> <span class="k">for</span> <span class="p">(</span><span class="n">row</span><span class="p">,</span> <span class="n">dim</span><span class="p">)</span> <span class="ow">in</span> <span class="n">embedding_specs</span><span class="p">])</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weights</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span>
                <span class="n">weights_offsets</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span>
                <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">current_device</span><span class="p">,</span>
                <span class="n">dtype</span><span class="o">=</span><span class="n">table_embedding_dtype</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="p">)</span>
        <span class="k">for</span> <span class="n">feature</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">T</span><span class="p">):</span>
            <span class="n">t</span> <span class="o">=</span> <span class="n">feature_table_map</span><span class="p">[</span><span class="n">feature</span><span class="p">]</span>
            <span class="n">row</span><span class="p">,</span> <span class="n">dim</span> <span class="o">=</span> <span class="n">embedding_specs</span><span class="p">[</span><span class="n">t</span><span class="p">]</span>
            <span class="k">if</span> <span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">[</span><span class="n">weights_offsets</span><span class="p">[</span><span class="n">t</span><span class="p">]</span> <span class="p">:</span> <span class="n">weights_offsets</span><span class="p">[</span><span class="n">t</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]]</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span>
                <span class="o">!=</span> <span class="n">row</span> <span class="o">*</span> <span class="n">dim</span>
            <span class="p">):</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">log</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;row </span><span class="si">{</span><span class="n">row</span><span class="si">}</span><span class="s2"> dim </span><span class="si">{</span><span class="n">dim</span><span class="si">}</span><span class="s2"> feature </span><span class="si">{</span><span class="n">feature</span><span class="si">}</span><span class="s2"> t </span><span class="si">{</span><span class="n">t</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">[</span><span class="n">weights_offsets</span><span class="p">[</span><span class="n">t</span><span class="p">]</span><span class="w"> </span><span class="p">:</span><span class="w"> </span><span class="n">weights_offsets</span><span class="p">[</span><span class="n">t</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">1</span><span class="p">]]</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span><span class="si">}</span><span class="s2">&quot;</span>
                <span class="p">)</span>
            <span class="k">assert</span> <span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">[</span><span class="n">weights_offsets</span><span class="p">[</span><span class="n">t</span><span class="p">]</span> <span class="p">:</span> <span class="n">weights_offsets</span><span class="p">[</span><span class="n">t</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]]</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span>
                <span class="o">==</span> <span class="n">row</span> <span class="o">*</span> <span class="n">dim</span>
            <span class="p">)</span>
            <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">hash_size_cumsum</span><span class="p">[</span><span class="n">feature</span><span class="p">]</span> <span class="o">==</span> <span class="nb">sum</span><span class="p">(</span>
                <span class="n">row</span> <span class="k">for</span> <span class="p">(</span><span class="n">row</span><span class="p">,</span> <span class="n">_</span><span class="p">)</span> <span class="ow">in</span> <span class="n">embedding_specs</span><span class="p">[:</span><span class="n">t</span><span class="p">]</span>
            <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">weights_physical_offsets</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="n">weights_offsets</span>
        <span class="n">weights_offsets</span> <span class="o">=</span> <span class="p">[</span><span class="n">weights_offsets</span><span class="p">[</span><span class="n">t</span><span class="p">]</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">feature_table_map</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span>
            <span class="s2">&quot;weights_offsets&quot;</span><span class="p">,</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span>
                <span class="n">weights_offsets</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">current_device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int64</span>
            <span class="p">),</span>
        <span class="p">)</span>

    <span class="nd">@torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">ignore</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">log</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">msg</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Log with TBE id prefix to distinguish between multiple TBE instances</span>
<span class="sd">        per process</span>

<span class="sd">        Args:</span>
<span class="sd">            msg (str): The message to print</span>

<span class="sd">        Returns:</span>
<span class="sd">            None</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;[TBE=</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">uuid</span><span class="si">}</span><span class="s2">] </span><span class="si">{</span><span class="n">msg</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="nd">@torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">ignore</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">_generate_vbe_metadata</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">offsets</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">batch_size_per_feature_per_rank</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]]],</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">invokers</span><span class="o">.</span><span class="n">lookup_args</span><span class="o">.</span><span class="n">VBEMetadata</span><span class="p">:</span>
        <span class="c1"># Blocking D2H copy, but only runs at first call</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">feature_dims</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">feature_dims</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">generate_vbe_metadata</span><span class="p">(</span>
            <span class="n">offsets</span><span class="p">,</span>
            <span class="n">batch_size_per_feature_per_rank</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">pooling_mode</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">feature_dims</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">current_device</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">indices</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">offsets</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
        <span class="n">per_sample_weights</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">feature_requires_grad</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">batch_size_per_feature_per_rank</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="c1"># Generate VBE metadata</span>
        <span class="n">vbe_metadata</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_generate_vbe_metadata</span><span class="p">(</span>
            <span class="n">offsets</span><span class="p">,</span> <span class="n">batch_size_per_feature_per_rank</span>
        <span class="p">)</span>

        <span class="c1"># NOTE: Force offsets to have the same dtype as indices since the</span>
        <span class="c1"># kernels assume same dtype.  We might need to revisit the assumption</span>
        <span class="c1"># of same dtypes in the future.</span>
        <span class="n">offsets</span> <span class="o">=</span> <span class="n">offsets</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">indices</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

        <span class="c1"># Force casting per_sample_weights to float</span>
        <span class="k">if</span> <span class="n">per_sample_weights</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">per_sample_weights</span> <span class="o">=</span> <span class="n">per_sample_weights</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>

        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">fbgemm</span><span class="o">.</span><span class="n">dense_embedding_codegen_lookup_function</span><span class="p">(</span>
            <span class="n">dev_weights</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">,</span>
            <span class="n">weights_offsets</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">weights_offsets</span><span class="p">,</span>
            <span class="n">D_offsets</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">D_offsets</span><span class="p">,</span>
            <span class="n">total_D</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">total_D</span><span class="p">,</span>
            <span class="n">max_D</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">max_D</span><span class="p">,</span>
            <span class="n">hash_size_cumsum</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">hash_size_cumsum</span><span class="p">,</span>
            <span class="n">total_hash_size_bits</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">total_hash_size_bits</span><span class="p">,</span>
            <span class="n">indices</span><span class="o">=</span><span class="n">indices</span><span class="p">,</span>
            <span class="n">offsets</span><span class="o">=</span><span class="n">offsets</span><span class="p">,</span>
            <span class="n">pooling_mode</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">pooling_mode</span><span class="p">,</span>
            <span class="n">indice_weights</span><span class="o">=</span><span class="n">per_sample_weights</span><span class="p">,</span>
            <span class="n">feature_requires_grad</span><span class="o">=</span><span class="n">feature_requires_grad</span><span class="p">,</span>
            <span class="n">output_dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">output_dtype</span><span class="p">,</span>
            <span class="n">B_offsets</span><span class="o">=</span><span class="n">vbe_metadata</span><span class="o">.</span><span class="n">B_offsets</span><span class="p">,</span>
            <span class="n">vbe_output_offsets_feature_rank</span><span class="o">=</span><span class="n">vbe_metadata</span><span class="o">.</span><span class="n">output_offsets_feature_rank</span><span class="p">,</span>
            <span class="n">vbe_B_offsets_rank_per_feature</span><span class="o">=</span><span class="n">vbe_metadata</span><span class="o">.</span><span class="n">B_offsets_rank_per_feature</span><span class="p">,</span>
            <span class="n">max_B</span><span class="o">=</span><span class="n">vbe_metadata</span><span class="o">.</span><span class="n">max_B</span><span class="p">,</span>
            <span class="n">max_B_feature_rank</span><span class="o">=</span><span class="n">vbe_metadata</span><span class="o">.</span><span class="n">max_B_feature_rank</span><span class="p">,</span>
            <span class="n">vbe_output_size</span><span class="o">=</span><span class="n">vbe_metadata</span><span class="o">.</span><span class="n">output_size</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="nd">@torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">export</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">split_embedding_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns a list of weights, split by table</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">splits</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">t</span><span class="p">,</span> <span class="p">(</span><span class="n">rows</span><span class="p">,</span> <span class="n">dim</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">embedding_specs</span><span class="p">):</span>
            <span class="n">offset</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights_physical_offsets</span><span class="p">[</span><span class="n">t</span><span class="p">]</span>
            <span class="n">splits</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="o">.</span><span class="n">detach</span><span class="p">()[</span><span class="n">offset</span> <span class="p">:</span> <span class="n">offset</span> <span class="o">+</span> <span class="n">rows</span> <span class="o">*</span> <span class="n">dim</span><span class="p">]</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">rows</span><span class="p">,</span> <span class="n">dim</span><span class="p">)</span>
            <span class="p">)</span>
        <span class="k">return</span> <span class="n">splits</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">init_embedding_weights_uniform</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">min_val</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span> <span class="n">max_val</span><span class="p">:</span> <span class="nb">float</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">splits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">split_embedding_weights</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">splits</span><span class="p">:</span>
            <span class="n">param</span><span class="o">.</span><span class="n">uniform_</span><span class="p">(</span><span class="n">min_val</span><span class="p">,</span> <span class="n">max_val</span><span class="p">)</span>
</pre></div>

             </article>
             
            </div>
            <footer>
  

  

    <hr>

  

  <div role="contentinfo">
    <p>
        &copy; Copyright 2020 - 2025, FBGEMM Team.

    </p>
  </div>
    
      <div>
        Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
      </div>
     

</footer>

          </div>
        </div>

        <div class="pytorch-content-right" id="pytorch-content-right">
          <div class="pytorch-right-menu" id="pytorch-right-menu">
            <div class="pytorch-side-scroll" id="pytorch-side-scroll-right">
              
            </div>
          </div>
        </div>
      </section>
    </div>

  


  

     
       <script type="text/javascript" id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
         <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
         <script src="../../_static/doctools.js"></script>
         <script src="../../_static/sphinx_highlight.js"></script>
     

  

  <script type="text/javascript" src="../../_static/js/vendor/popper.min.js"></script>
  <script type="text/javascript" src="../../_static/js/vendor/bootstrap.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/1.5.0/list.min.js"></script>
  <script type="text/javascript" src="../../_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

  <!-- Begin Footer -->

  <div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
    <div class="container">
      <div class="row">
        <div class="col-md-4 text-center">
          <h2>Docs</h2>
          <p>Access comprehensive developer documentation for PyTorch</p>
          <a class="with-right-arrow" href="https://pytorch.org/docs/stable/index.html">View Docs</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Tutorials</h2>
          <p>Get in-depth tutorials for beginners and advanced developers</p>
          <a class="with-right-arrow" href="https://pytorch.org/tutorials">View Tutorials</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Resources</h2>
          <p>Find development resources and get your questions answered</p>
          <a class="with-right-arrow" href="https://pytorch.org/resources">View Resources</a>
        </div>
      </div>
    </div>
  </div>

  <footer class="site-footer">
    <div class="container footer-container">
      <div class="footer-logo-wrapper">
        <a href="https://pytorch.org/" class="footer-logo"></a>
      </div>

      <div class="footer-links-wrapper">
        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/">PyTorch</a></li>
            <li><a href="https://pytorch.org/get-started">Get Started</a></li>
            <li><a href="https://pytorch.org/features">Features</a></li>
            <li><a href="https://pytorch.org/ecosystem">Ecosystem</a></li>
            <li><a href="https://pytorch.org/blog/">Blog</a></li>
            <li><a href="https://github.com/pytorch/pytorch/blob/master/CONTRIBUTING.md">Contributing</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/resources">Resources</a></li>
            <li><a href="https://pytorch.org/tutorials">Tutorials</a></li>
            <li><a href="https://pytorch.org/docs/stable/index.html">Docs</a></li>
            <li><a href="https://discuss.pytorch.org" target="_blank">Discuss</a></li>
            <li><a href="https://github.com/pytorch/pytorch/issues" target="_blank">Github Issues</a></li>
            <li><a href="https://pytorch.org/assets/brand-guidelines/PyTorch-Brand-Guidelines.pdf" target="_blank">Brand Guidelines</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title">Stay up to date</li>
            <li><a href="https://www.facebook.com/pytorch" target="_blank">Facebook</a></li>
            <li><a href="https://twitter.com/pytorch" target="_blank">Twitter</a></li>
            <li><a href="https://www.youtube.com/pytorch" target="_blank">YouTube</a></li>
            <li><a href="https://www.linkedin.com/company/pytorch" target="_blank">LinkedIn</a></li>
          </ul>  
          </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title">PyTorch Podcasts</li>
            <li><a href="https://open.spotify.com/show/6UzHKeiy368jKfQMKKvJY5" target="_blank">Spotify</a></li>
            <li><a href="https://podcasts.apple.com/us/podcast/pytorch-developer-podcast/id1566080008" target="_blank">Apple</a></li>
            <li><a href="https://www.google.com/podcasts?feed=aHR0cHM6Ly9mZWVkcy5zaW1wbGVjYXN0LmNvbS9PQjVGa0lsOA%3D%3D" target="_blank">Google</a></li>
            <li><a href="https://music.amazon.com/podcasts/7a4e6f0e-26c2-49e9-a478-41bd244197d0/PyTorch-Developer-Podcast?" target="_blank">Amazon</a></li>
          </ul>
         </div>
        </div>
        
        <div class="privacy-policy">
          <ul>
            <li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/terms/" target="_blank">Terms</a></li>
            <li class="privacy-policy-links">|</li>
            <li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/privacy-policy/" target="_blank">Privacy</a></li>
          </ul>
        </div>
        <div class="copyright">
        <p>© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation.
          For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see
          <a href="https://www.linuxfoundation.org/policies/">www.linuxfoundation.org/policies/</a>. The PyTorch Foundation supports the PyTorch open source
          project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC,
          please see <a href="https://www.lfprojects.org/policies/">www.lfprojects.org/policies/</a>.</p>
      </div>
     </div>

  </footer>

  <div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="../../_static/images/pytorch-x.svg">
  </div>
</div>

  <!-- End Footer -->

  <!-- Begin Mobile Menu -->

  <div class="mobile-main-menu">
    <div class="container-fluid">
      <div class="container">
        <div class="mobile-main-menu-header-container">
          <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>
          <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
        </div>
      </div>
    </div>

    <div class="mobile-main-menu-links-container">
      <div class="main-menu">
        <ul>
           <li class="resources-mobile-menu-title">
             <a>Learn</a>
           </li>
           <ul class="resources-mobile-menu-items">
             <li>
               <a href="https://pytorch.org/get-started">Get Started</a>
             </li>
             <li>
               <a href="https://pytorch.org/tutorials">Tutorials</a>
             </li>
             <li>
               <a href="https://pytorch.org/tutorials/beginner/basics/intro.html">Learn the Basics</a>
             </li>
             <li>
               <a href="https://pytorch.org/tutorials/recipes/recipes_index.html">PyTorch Recipes</a>
             </li>
             <li>
               <a href="https://pytorch.org/tutorials/beginner/introyt.html">Introduction to PyTorch - YouTube Series</a>
             </li>
           </ul>
           <li class="resources-mobile-menu-title">
             <a>Ecosystem</a>
           </li>
           <ul class="resources-mobile-menu-items">
             <li>
               <a href="https://pytorch.org/ecosystem">Tools</a>
             </li>
             <li>
               <a href="https://pytorch.org/#community-module">Community</a>
             </li>
             <li>
               <a href="https://discuss.pytorch.org/">Forums</a>
             </li>
             <li>
               <a href="https://pytorch.org/resources">Developer Resources</a>
             </li>
             <li>
               <a href="https://pytorch.org/ecosystem/contributor-awards-2023">Contributor Awards - 2024</a>
             </li>
           </ul>

           <li class="resources-mobile-menu-title">
             <a>Edge</a>
           </li>

           <ul class="resources-mobile-menu-items">
             <li>
               <a href="https://pytorch.org/edge">About PyTorch Edge</a>
             </li>
             
             <li>
               <a href="https://pytorch.org/executorch-overview">ExecuTorch</a>
             </li>
             <li>
               <a href="https://pytorch.org/executorch/stable/index.html">ExecuTorch Documentation</a>
             </li>
           </ul>

           <li class="resources-mobile-menu-title">
             <a>Docs</a>
           </li>

           <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://pytorch.org/docs/stable/index.html">PyTorch</a>
            </li>

            <li>
              <a href="https://pytorch.org/pytorch-domains">PyTorch Domains</a>
            </li>
          </ul>

          <li class="resources-mobile-menu-title">
            <a>Blog & News</a>
          </li>
            
           <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://pytorch.org/blog/">PyTorch Blog</a>
            </li>
            <li>
              <a href="https://pytorch.org/community-blog">Community Blog</a>
            </li>

            <li>
              <a href="https://pytorch.org/videos">Videos</a>
            </li>

            <li>
              <a href="https://pytorch.org/community-stories">Community Stories</a>
            </li>
            <li>
              <a href="https://pytorch.org/events">Events</a>
            </li>
            <li>
               <a href="https://pytorch.org/newsletter">Newsletter</a>
             </li>
          </ul>
          
          <li class="resources-mobile-menu-title">
            <a>About</a>
          </li>

          <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://pytorch.org/foundation">PyTorch Foundation</a>
            </li>
            <li>
              <a href="https://pytorch.org/governing-board">Governing Board</a>
            </li>
            <li>
               <a href="https://pytorch.org/credits">Cloud Credit Program</a>
            </li>
            <li>
               <a href="https://pytorch.org/tac">Technical Advisory Council</a>
            </li>
            <li>
               <a href="https://pytorch.org/staff">Staff</a>
            </li>
            <li>
               <a href="https://pytorch.org/contact-us">Contact Us</a>
            </li>
          </ul>
        </ul>
      </div>
    </div>
  </div>

  <!-- End Mobile Menu -->

  <script type="text/javascript" src="../../_static/js/vendor/anchor.min.js"></script>

  <script type="text/javascript">
    $(document).ready(function() {
      mobileMenu.bind();
      mobileTOC.bind();
      pytorchAnchors.bind();
      sideMenus.bind();
      scrollToAnchor.bind();
      highlightNavigation.bind();
      mainMenuDropdown.bind();
      filterTags.bind();

      // Add class to links that have code blocks, since we cannot create links in code blocks
      $("article.pytorch-article a span.pre").each(function(e) {
        $(this).closest("a").addClass("has-code");
      });
    })
  </script>
</body>
</html>